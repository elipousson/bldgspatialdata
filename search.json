[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website was created by Eli Pousson for the Fall 2022 course Building Spatial Datasets at the University of Maryland Baltimore County, Department of Geography & Environmental Systems (GES). This website. The site includes the course syllabus, a schedule of readings, assignments, examples, and resources."
  },
  {
    "objectID": "about.html#credits",
    "href": "about.html#credits",
    "title": "About",
    "section": "Credits",
    "text": "Credits\nThis course website is based on the Program Evaluation for Public Service course website (repo) by Andrew Heiss at Georgia State University (licensed under a MIT License)."
  },
  {
    "objectID": "assignments/00_weekly_logs.html",
    "href": "assignments/00_weekly_logs.html",
    "title": "Write a weekly log",
    "section": "",
    "text": "Each week students are expected to write a 1-2 paragraph reflection the assigned readings and one to three questions related to the technical materials.\nEach weekly log entry should be committed to the log folder as a Markdown (.md) file with a file name corresponding to the session date (e.g. 2022-08-31_log.md).\nWeekly logs that are unrelated to the readings or do not include a question will be considered incomplete but may be revised and resubmitted. You are allowed to skip or miss up to four weekly log entries during the course—so each entry submitted beyond 10 entries is a bonus point."
  },
  {
    "objectID": "assignments/00_weekly_logs.html#submitting-a-weekly-log-entry-with-github",
    "href": "assignments/00_weekly_logs.html#submitting-a-weekly-log-entry-with-github",
    "title": "Write a weekly log",
    "section": "Submitting a weekly log entry with GitHub",
    "text": "Submitting a weekly log entry with GitHub\nWeekly log entries are submitted using your personal GitHub repository in the course GitHub organization. I recommend installed GitHub Desktop to use during this course.\nAfter you initially set up your GitHub account, use GitHub Desktop to clone your personal course repository to your own computer (see documentation for more information).\nHere is a typical process for submitting a log entry using RStudio and GitHub:\n\nOpen the logs.Rproj file in the logs folder in your repository with RStudio.\nLocate the template file (2022-MM-DD_log.md) using the files pane of RStudio (you can customize the pane layout using the options dialog for the RStudio IDE).\nSelect the template file, make a copy (using the More > Copy drop-down menu option), and name the new file substituting the month and date of the related session for MM-DD in the file name. For example, a log due on September 21, 2022 should be named 2022-09-21_log.md.\nAdd your own reflection and questions to the new Markdown document. Both a reflection and a question are expected for a complete log entry. Please keep the headings from the template in place. Use the RStudio visual editor to add formatting and links as needed.\nUse the Git tab to commit and push the new log file to your repository:\n\nClick the Git tab\nCheck the “Staged” check box next to the new file\nClick Commit\nType a message in Commit message, e.g. “Add weekly log for September 7”\nClick Commit to confirm the addition\nClick Push to sync the change with the remote repository on Github.com\n\n\nYou can also commit and push changes using GitHub Desktop. Review the documentation on Committing and reviewing changes to your project for more information.\nFeatured image: PHCS Ron Bayles, USN (1989)"
  },
  {
    "objectID": "assignments/01_find-data.html",
    "href": "assignments/01_find-data.html",
    "title": "Find spatial data",
    "section": "",
    "text": "Learn how to…\n\nFrame a search strategy for spatial data based on your research question and anticipated data needs.\nPerform a search for geospatial data in the BTAA Geoportal using the text, map, and facet searching features of that system.\nDetermine how to access relevant geospatial datasets using the BTAA Geoportal based on the results of your search process.\nThe BTAA Geoportal includes the Government Open Geospatial Data Collection which contains links to open geospatial datasets, web services, interactive maps, and data portals distributed by a federal, state, county, municipal, or regional public entity. The collection includes data from Maryland iMap, Open Baltimore, the Baltimore County Open Data Portal, and other county-level open data portals."
  },
  {
    "objectID": "assignments/01_find-data.html#step-1-framing-your-search",
    "href": "assignments/01_find-data.html#step-1-framing-your-search",
    "title": "Find spatial data",
    "section": "Step 1: Framing your search",
    "text": "Step 1: Framing your search\nWrite: Take a few minutes, and write down some notes related to the questions below to frame your search strategy for spatial data.\n\n\n\n\n\n\nThe What, Where, and When questions will help you determine the initial scope of your data needs. The Who questions will help you brainstorm some potential sources for beginning your search.\n\n\n\nWhat - Ask yourself questions like:\n\nWhat is my research question, and what types of analysis am I planning to carry out?\nWhat data will I need to address this question or carry out this analysis?\nWhat types of data am I seeking (e.g., vector, raster, tabular)?\n\nWhere - Ask yourself questions like:\n\nWhat geographic area(s) am I studying?\nWhat spatial scale/resolution am I planning to use for my analysis, and is this realistic?\n\nWhen - Ask yourself questions like:\n\nWhat time period(s) am I studying?\nIs my analysis based on a single point in time, or am I planning to compare data across multiple time periods?\nWhat is my desired periodicity for data collection (e.g., monthly, annually, every decade), and is this realistic?\n\nWho - Ask yourself questions like:\n\nWho is likely to care about this research topic and associated datasets?\nWho has published on this topic, and what data did they use?\nWhat government agencies, educational institutions, non-profits, research centers, commercial entities, or other organizations might collect and distribute these data?"
  },
  {
    "objectID": "assignments/01_find-data.html#step-2-searching-for-data-in-the-btaa-geoportal",
    "href": "assignments/01_find-data.html#step-2-searching-for-data-in-the-btaa-geoportal",
    "title": "Find spatial data",
    "section": "Step 2: Searching for data in the BTAA Geoportal",
    "text": "Step 2: Searching for data in the BTAA Geoportal\nWrite: In 1-2 sentences, briefly describe your research question or topic. You are welcome but not required to stick with this question or topic beyond this assignment.\nAccess the Big Ten Academic Alliance Geoportal at https://geo.btaa.org.\n\nComplete a keyword search\n\nBased on your articulated research question or topic, start by performing a text-based search (i.e., keywords).\nWrite: What search terms did you use?\nExplore your initial results. If your search returns relevant results, open the item page for a few of those records to examine in more detail.\nReflect and write: Did your search return relevant results? If so, record the title(s) for the relevant dataset(s).\n\nWhat did you learn from examining the item page? Describe the ways these data are accessible (e.g., web services, direct download).\nIf not, what other search terms might return more relevant results?\n\nObserve and write: Try a second keyword search using these new terms, and explore these new results. Record your new search terms and the title(s) for any relevant dataset(s).\n\n\n\nComplete a map-based search and use facets to filter results\n\nReturn to the BTAA Geoportal homepage, and perform a map-based search (using option 1).\nObserve and write: Examine your initial results. If your search returns relevant results, open the item page for a few of those records to examine in more detail. Record the title(s) for the relevant dataset(s).\nReturn to your search results page. Use one or more of the facets available on the left side of the page to refine your search results.\nReflect and write: What facet(s) did you use to refine your search? Was this approach helpful in narrowing down your results to more relevant datasets? Record the title(s) for any relevant datasets.\nObserve and write: Examine the item page for one of these datasets. Describe the ways these data are accessible (e.g., web services, direct download).\n\n\n\nPlanning next steps in your search\nTo wrap up, briefly describe your next steps.\nIf you located relevant datasets for your project:\n\nAre there additional searches that you plan to perform in the BTAA Geoportal to try to find other relevant datasets? What types of searches are you planning?\nFor datasets you’ve already located, how will you go about accessing them and evaluating them for use in your project?\n\nIf you did not locate relevant datasets for your project:\n\nAre there other searches that you might try in the BTAA Geoportal to find more relevant results? What types of searches?\nIf the BTAA Geoportal is unlikely to match your data needs for this project, what other options might be available to you? What other sources do you plan to explore next for finding relevant datasets?"
  },
  {
    "objectID": "assignments/02_make-map-r.html",
    "href": "assignments/02_make-map-r.html",
    "title": "Make a map with R and ggplot2",
    "section": "",
    "text": "Learn how to…\n\nOpen and save project files using RStudio\nRead vector data files (e.g. shapefile or GeoJSON) using the sf package\nFilter data by attribute using the {dplyr} package\nCreate a map with the {ggplot2} package using an attribute to define the aesthetics of the map\nLabel and export a map using the {ggplot2} package"
  },
  {
    "objectID": "assignments/02_make-map-r.html#step-by-step-instructions",
    "href": "assignments/02_make-map-r.html#step-by-step-instructions",
    "title": "Make a map with R and ggplot2",
    "section": "Step-by-step instructions",
    "text": "Step-by-step instructions\nComplete all of the following steps using the linked function documentation for reference:\n\nOpen the 02_map-R.RProj project file (see using RStudio Projects) and open the RMarkdown template found within the project folder.\nRead any additional data using sf::read_sf() or sf::st_read()\nFilter data by attribute using dplyr::filter()\nCreate a plot with ggplot2::ggplot() and a geom with ggplot2::geom_sf()\nSpecify one or more aesthetic attributes using ggplot2::aes()\nAdd an appropriate scale for the attribute using ggplot2::scale_fill_brewer(), ggplot2::scale_color_brewer(), or another ggplot2 scale function\nSet a theme using ggplot2::theme_minimal() or another theme function\nCustomize the map title, caption, and legend using ggplot2::labs(). Depending on the attribute type, you may want to use a labelling function from the {scales} package, e.g. label_dolar() for currency values or label_percent() for percentages. These functions are passed to the labels parameter of your scale function.\nExport the map as a PDF file using ggplot2::ggsave()"
  },
  {
    "objectID": "assignments/02_make-map-r.html#optional-bonus-activities",
    "href": "assignments/02_make-map-r.html#optional-bonus-activities",
    "title": "Make a map with R and ggplot2",
    "section": "Optional bonus activities",
    "text": "Optional bonus activities\nCreate one or more additional maps using any of the following approaches for an extra bonus points:\n\nCreate a multi-panel map using a faceting function. Typically, multi-panel maps are used for categorical variables or binned continuous variables. Consider using the dplyr::mutate() function to re-code a categorical variable or using a binned scale to convert a continuous variable into a discrete variable.\nCreate a bar chart showing the distribution of the mapped attribute by location or a scatter plot to compare the mapped attribute data to another attribute in your data. Ch. 3 Data visualization in Wickham and Grolemund (2022) has examples of both types of data visualizations or you can explore the documentation for ggplot2::geom_bar() or ggplot2::geom_point().\n\n\n\n\n\n\n\nSubmitting the completed assignment\n\n\n\nAdd the PDF map and a RMarkdown file with a brief description of your process and the map to the assignment folder in your course GitHub repository. If your map uses data that is not included in the Natural Earth Quick Start file, you must include links to sources where the data can be downloaded to allow the project file to be opened for review and feedback. If data files are less than 50 MB, please add the files to the GitHub repository in the same folder as the project file."
  },
  {
    "objectID": "assignments/03_make-map-qgis.html",
    "href": "assignments/03_make-map-qgis.html",
    "title": "Read data and make a map with QGIS",
    "section": "",
    "text": "Learn how to…\n\nOpen and save project files\nRead vector data files (e.g. shapefile or GeoJSON) using the QGIS Data Source Manager\nExplore the attributes of data using the QGIS Attribute Table\nCustomize the color, fill, or symbols of a layer based on an attribute using the QGIS Style Manager\nCompose, customize, and export a map using the QGIS Print Layout feature\nFor this assignment, you are encouraged to use the Natural Earth Quickstart Kit (a small sample of Natural Earth themes styled in in a QGIS document) as a data source or basemap:"
  },
  {
    "objectID": "assignments/03_make-map-qgis.html#step-by-step-instructions",
    "href": "assignments/03_make-map-qgis.html#step-by-step-instructions",
    "title": "Read data and make a map with QGIS",
    "section": "Step-by-step instructions",
    "text": "Step-by-step instructions\nComplete all of the following steps using the linked documentation for reference:\n\nOpen the project file Natural_Earth_quick_start_for_QGIS.qgs.\nLoad one or more additional data identified in the Finding data assignment using the Data Source Manager (see Opening Data)\nFilter data from the Natural Earth data or the additional data by attributes using the Attribute Table (see Working with the Attribute Table)\nVisualize a select attribute with a color ramp using the Style Manager (see Style Manager)\nCreate a map using the Print Layout option (see Overview of the Print Layout)\nCustomize the map title, caption, and legend of the Print Layout (see Legend Item)\nExport the map to PDF (see Creating an Output) with the file name 03_map-qgis_layout.pdf.\nSave an renamed project file with the file name 03_map-qgis_project-file.qgs.\nWrite a brief description of your process for filtering and visualizing the data as well as composing the map and save the description as a Markdown file with the filename 03_map-qgis_project-description.md."
  },
  {
    "objectID": "assignments/03_make-map-qgis.html#optional-bonus-activities",
    "href": "assignments/03_make-map-qgis.html#optional-bonus-activities",
    "title": "Read data and make a map with QGIS",
    "section": "Optional bonus activities",
    "text": "Optional bonus activities\nCreate one or more additional maps using any of the following approaches for an extra bonus points:\n\nJoin a new data set without spatial data to a mapped data set using attribute data (see Performing Table Joins (QGIS3) from QGIS Tutorials and Tips for a walk through on using the “Join attributes by field value” feature in the Processing Toolbox). For example, if you have a table or CSV file with county-level data, your could use the county name or FIPS number to join the data to the “Admin 2 – Counties” data from Natural Earth (included in the Quick Start data).\nDraw a new feature and add it to your map. The new feature could be a region based on the boundaries of the existing features on the map (see Lesson 13 from the QGIS Uncovered video series for an example).\n\n\n\n\n\n\n\nSubmitting the completed assignment\n\n\n\nAdd the PDF map, QGS project file, and the Markdown file with your description to the assignment folder in your course GitHub repository. If your maps uses data that is not included in the Natural Earth Quick Start project, you must include links to sources in the Markdown file where the data can be downloaded to allow the project file to be opened for review and feedback. If data files are less than 50 MB, please add the files to the GitHub repository in the same folder as the project file."
  },
  {
    "objectID": "assignments/03_make-map-qgis.html#related-resources",
    "href": "assignments/03_make-map-qgis.html#related-resources",
    "title": "Read data and make a map with QGIS",
    "section": "Related Resources",
    "text": "Related Resources\n\nMaking a Map (QGIS3) from QGIS Tutorials and Tips\n5.3 Sample Session: Loading raster and vector layers from QGIS Desktop User Guide/Manual (3.22)"
  },
  {
    "objectID": "assignments/04_final-project.html",
    "href": "assignments/04_final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "The final project is an opportunity for you to practice the spatial data skills we’ve been working on in this course while also exploring your interests and the potential for spatial data to support civic goals.\nThis project has three parts:\n\nA project proposal (due November 9)\nAn in-class presentation about your project (due December 7)\nA project repository including code, data, output files, and a README (due December 16)\n\n\n\nIn framing your project, look for opportunities to apply one or more of the six models of local practice described in Loukissas (2019):\n\nLook at the data setting, not just the data set\nMake place part of data presentation\nTake a comparative approach to data analysis\nCreate counterdata to challenge normative algorithms\nCreate interfaces that cause friction\nUse data to build relationships\n\nIs your project designed around what Loukissas calls the common “ambitions” for working data—orientation, access, analysis, andoroptimization? Or, are you trying to promote critical reflection on the local conditions of data using strategies such as place making, restraint, reflexivity, or contestation?\nYour goals may change between your initial proposal and the completion of your project but your final presentation should include both an explanation of your goals and how your goals do or do not engage with critical approaches to spatial data.\n\n\n\nYour project should either:\n\nUse one or more existing spatial datasets\nCreate a spatial dataset based on existing material (e.g. a written report or a georeferenced map)\nCollect a new dataset with a spatial component\n\nIf you are creating or collecting data, your scope for other aspects of the project may need to be more limited in order to allow adequate time to design your data collection tools.\nIn all cases, your selection\n\n\n\nYou are welcome to explore a wide range of possible formats for your final project:\n\nAn interactive web map that enables users to explore how local conditions may vary over time or place\nA short exploratory data analysis using tables, plots, and maps to illustrate what a data set can (or can’t) tell us about a place\nAn R package for publishing and documenting a spatial data set (and how the data was created)\nA PDF fact sheet using spatial data to summarize the local impact of a national challenge and what local organizations are working to address the issue\nOr something else entirely.\n\nThink about the public you may want to engage with your project."
  },
  {
    "objectID": "assignments/04_final-project.html#project-proposal",
    "href": "assignments/04_final-project.html#project-proposal",
    "title": "Final Project",
    "section": "Project proposal",
    "text": "Project proposal\nYour project proposal needs to answer three main questions:\n\nWhat are your goals for the project? Your goals could include answering a research question, making the case for a public policy change, or building an interface to help people better understand an issue in their community. Your goals could also include developing your own ability to analyze a specific type of data or exploring an academic interest. Reflect on who might benefit from your proposed project and how your project can avoid causing people harm.\nWhat data can you use to support your goal? Your data could include any public spatial data or data that has a spatial attribute. You can even create data from scratch or collect your own data. Reflect on the “setting” for the data, whose local knowledge it represents, and what communities participate in collecting or maintaining the data.\nWhat is your approach to using data to support your goal? Your approach could include mapping, exploratory analysis, documentation, visualization, or a combination of multiple tools and methods. You don’t need to reinvent the wheel. You can adapt an existing approach or propose a few options you hope to try. Reflect on any expected challenges and if your proposed approach is feasible in the time you have available during the semester.\n\n\n\n\n\n\n\nI will schedule short one-on-one meetings in advance of the due date for the final project proposal to help you in refining your ideas, identifying possible data sources, and any existing R packages or frameworks you can adapt or use for your project.\n\n\n\nThere are few more requirements to keep in mind:\n\nAnswer each question with a brief but considered response. If it helps to have a total word count to work towards, try to answer all three questions in something between 500 and 1000 words.\nFormat your proposal as an RMarkdown or Quarto document. Your document should include links to any published data or related resources and reproducible code for any preliminary data analysis you completed to support your proposal.\nCite your sources! While an extensive literature review is unnecessary, reviewing how other researching and practitioners have used the same or similar spatial data may give you ideas for your own project. Citing sources in RStudio is a little different than Microsoft Word so I strongly recommend using the Zotero citation manager in combination with the Better Bibtex extension. If a single R package is a big part of your proposed approach, make sure to also include a citation for the package. Read How to Cite R and R Packages by Steffi LaZerte for more background on how and why you should cite R packages.\n\n\n\n\n\n\n\nYou are strongly encouraged to use R as part of your project but it isn’t required. If you prefer to use QGIS or another tool as the main part of your approach, you are still expected to use Markdown or RMarkdown to share your proposal and your proposal should make the case for your preferred approach.\n\n\n\nIf you have already identified a data source for your project, make sure:\n\nyou know the data and goals are related (e.g. you have a question that can be answered using the data),\nyou have permission to use the data (e.g. the data is published under an open license),\nyou know the data is in an accessible format (e.g. CSV, ArcGIS Feature Server, GeoPackage file),\nand you know there are no major data quality issues (e.g. location accuracy, completeness) that you can’t address.\n\nIf you are creating data by based on existing unstructured sources (e.g. digitizing features from a georeferenced historic map) or collecting data based on phenomena or features in your community (e.g. using a smartphone app to document noise levels), make sure:\n\nyou have a clear reason for creating or collecting data instead of using an existing source,\nyou identify what resources you need to complete your project including the time to collect the data,\nand you know your proposal is not subject to review by the UMBC Institutional Review board (review What is NOT Human Subjects Research? for more information)."
  },
  {
    "objectID": "assignments/04_final-project.html#project-presentation",
    "href": "assignments/04_final-project.html#project-presentation",
    "title": "Final Project",
    "section": "Project presentation",
    "text": "Project presentation\nYour in-class presentation should be around ten minutes and address these key questions:\n\nWhat were your initial goals for the project? How did they change or develop as you worked on your project?\nWhat data sources did you use? How, why, and where were they created?\nWhat packages, templates, or other resources did you use in creating your final project?\nWhat challenges did you encounter in making use of these resources and this data?\nWhat do you think your project does well?\n\n\n\n\n\n\n\nConsider using Quarto to create an HTML presentation using reveal.js. Taking this approach could allow you to easily incorprorate data visualizations or other materials from your project into your presentation."
  },
  {
    "objectID": "assignments/04_final-project.html#final-project-repository",
    "href": "assignments/04_final-project.html#final-project-repository",
    "title": "Final Project",
    "section": "Final project repository",
    "text": "Final project repository\nYour final project should be submitted as a GitHub repository. A private repository can be provided to you as part of the course organization or you can set up your own repository on your personal GitHub account.\nThe repository must include:\n\nproject data: including the source files or, if files exceed the 50MB maximum size allowed on GitHub, a script used for importing and processing the data before visualization or analysis\nproject code: including any R scripts, RMarkdown, or Quarto files used to read, tidy, transform, analyze, visualize or map the selected data.\noutput files: including any processed data files or rendered PDF or HTML documents.\nREADME: a public-facing summary of the project explaining your process for processing the data and any relevant information another person may need to work with the data or your code.\nadditional materials: including any data collection materials (e.g. survey forms), reference data used by the project code, or other related materials."
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Final Project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind spatial data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMake a map with R and ggplot2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRead data and make a map with QGIS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite a weekly log\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "examples/01_read-data-r.html",
    "href": "examples/01_read-data-r.html",
    "title": "Read and filter spatial data with R",
    "section": "",
    "text": "This example shows how to read spatial data into R using the sf package and explains the basic structure and attributes simple feature sf objects. This example uses two datasets from Natural Earth:\nYou are required to use this same data in completing the introductory assignments on reading and mapping data with QGIS and with R so this example."
  },
  {
    "objectID": "examples/01_read-data-r.html#read-spatial-data",
    "href": "examples/01_read-data-r.html#read-spatial-data",
    "title": "Read and filter spatial data with R",
    "section": "Read spatial data",
    "text": "Read spatial data\n\n\n\n\n\n\nTip\n\n\n\n\n\nReview the sf vignette on Reading, Writing and Converting Simple Features for more detailed explanation on how to read spatial data into R.\n\n\n\nReading spatial data into your local R environment using the sf package is straight forward. For example, you can read a local GeoPackage file to an sf object by setting the dsn (short for data source name) to the file path:\n\ndsn <- here::here(\"files/data\", \"ne_50m_populated_places_simple.gpkg\")\n\npopulated_places <- st_read(dsn = dsn)\n\nReading layer `ne_50m_populated_places_simple' from data source \n  `/Users/elipousson/Documents/GitHub/bldgspatialdata/files/data/ne_50m_populated_places_simple.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1251 features and 31 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -175.2206 ymin: -90 xmax: 179.2166 ymax: 78.22097\nGeodetic CRS:  WGS 84\n\n\nst_read read the GeoPackage file into a simple feature (sf) object and output details on the layer, data source, driver, and attributes of the new object.\nYou can also use read_sf() which is the same as st_read() but it returns a tibble object and uses quiet = TRUE by default.\n\ndsn <- here::here(\"files/data\", \"ne_10m_admin_0_countries.gpkg\")\n\ncountries <- read_sf(dsn = dsn)\n\nThe dsn argument can also be a URL for a spatial data file that is stored online (instead of locally on your computer) or a database endpoint. The data source is also not limited to a GeoPackage files. Supported filetypes include GeoJSON files, shapefiles, KML files, and any other file type with a GDAL (Geospatial Data Abstraction Library) driver. You can review list available drivers using the sf::st_drivers() function.\n\n\nSupported vector drivers\nSupported raster drivers"
  },
  {
    "objectID": "examples/01_read-data-r.html#structure-and-attributes-of-sf-objects",
    "href": "examples/01_read-data-r.html#structure-and-attributes-of-sf-objects",
    "title": "Read and filter spatial data with R",
    "section": "Structure and attributes of sf objects",
    "text": "Structure and attributes of sf objects\n\n\n\n\n\n\nTip\n\n\n\n\n\nRead the sf vignette on Simple Features for R for more detailed explanation of geometry types, dimensions, coordinate reference systems, and more. For more on data frames, read the R Manual: An Introduction to R on data frames or the chapter on Tibbles from R for Data Science.\n\n\n\nA simple feature (sf) object is always a data frame where each row is a feature. Like any other data frame, these columns can hold numeric (double or integer), character, factor, or logical values. These columns may also include list columns or nested data frame columns. The dplyr::glimpse() function provides a convenient way to get a quick overview of the column types and values for “populated_places” and “countries”:\n\nglimpse(populated_places)\n\nRows: 1,251\nColumns: 32\n$ scalerank  <int> 10, 10, 10, 10, 10, 10, 10, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7…\n$ natscale   <int> 1, 1, 1, 1, 1, 1, 1, 10, 10, 10, 10, 10, 20, 20, 20, 20, 20…\n$ labelrank  <int> 5, 5, 3, 3, 3, 8, 0, 3, 3, 3, 3, 0, 6, 5, 5, 5, 5, 5, 5, 6,…\n$ featurecla <chr> \"Admin-1 region capital\", \"Admin-1 region capital\", \"Admin-…\n$ name       <chr> \"Bombo\", \"Fort Portal\", \"Potenza\", \"Campobasso\", \"Aosta\", \"…\n$ namepar    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Uruguay\", NA, …\n$ namealt    <chr> NA, NA, NA, NA, NA, NA, NA, NA, \"Poitiers\", NA, NA, NA, NA,…\n$ nameascii  <chr> \"Bombo\", \"Fort Portal\", \"Potenza\", \"Campobasso\", \"Aosta\", \"…\n$ adm0cap    <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ capalt     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ capin      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ worldcity  <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ megacity   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sov0name   <chr> \"Uganda\", \"Uganda\", \"Italy\", \"Italy\", \"Italy\", \"Finland\", \"…\n$ sov_a3     <chr> \"UGA\", \"UGA\", \"ITA\", \"ITA\", \"ITA\", \"ALD\", \"IS1\", \"VAT\", \"FR…\n$ adm0name   <chr> \"Uganda\", \"Uganda\", \"Italy\", \"Italy\", \"Italy\", \"Aland\", \"Pa…\n$ adm0_a3    <chr> \"UGA\", \"UGA\", \"ITA\", \"ITA\", \"ITA\", \"ALD\", \"PSX\", \"VAT\", \"FR…\n$ adm1name   <chr> \"Bamunanika\", \"Kabarole\", \"Basilicata\", \"Molise\", \"Valle d'…\n$ iso_a2     <chr> \"UG\", \"UG\", \"IT\", \"IT\", \"IT\", \"AX\", \"PS\", \"VA\", \"FR\", \"FR\",…\n$ note       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ latitude   <dbl> 0.583299, 0.671004, 40.642002, 41.562999, 45.737001, 60.096…\n$ longitude  <dbl> 32.533300, 30.275002, 15.798997, 14.655997, 7.315003, 19.94…\n$ pop_max    <dbl> 75000, 42670, 69060, 50762, 34062, 10682, 24599, 832, 85960…\n$ pop_min    <dbl> 21000, 42670, 69060, 50762, 34062, 10682, 24599, 832, 84807…\n$ pop_other  <dbl> 0, 0, 0, 0, 0, 0, 0, 562430, 80866, 223592, 117010, 0, 7894…\n$ rank_max   <int> 8, 7, 8, 8, 7, 6, 7, 2, 8, 10, 9, 1, 8, 10, 10, 10, 2, 8, 8…\n$ rank_min   <int> 7, 7, 8, 8, 7, 6, 7, 2, 8, 9, 9, 1, 8, 8, 8, 8, 2, 8, 8, 7,…\n$ meganame   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ls_name    <chr> NA, NA, NA, NA, NA, NA, NA, \"Vatican City\", \"Poitier\", \"Cle…\n$ min_zoom   <dbl> 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 9.0, 7.0, 6.7, 6.0, 7.0, 7.0,…\n$ ne_id      <dbl> 1159113923, 1159113959, 1159117259, 1159117283, 1159117361,…\n$ geom       <POINT [°]> POINT (32.5333 0.5832991), POINT (30.275 0.6710041), …\n\nglimpse(countries)\n\nRows: 258\nColumns: 169\n$ featurecla <chr> \"Admin-0 country\", \"Admin-0 country\", \"Admin-0 country\", \"A…\n$ scalerank  <int> 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ LABELRANK  <int> 2, 3, 2, 3, 2, 2, 3, 5, 2, 2, 4, 5, 5, 2, 3, 6, 2, 6, 3, 3,…\n$ SOVEREIGNT <chr> \"Indonesia\", \"Malaysia\", \"Chile\", \"Bolivia\", \"Peru\", \"Argen…\n$ SOV_A3     <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"GB1\", \"CYP\", \"IN…\n$ ADM0_DIF   <int> 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ LEVEL      <int> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ TYPE       <chr> \"Sovereign country\", \"Sovereign country\", \"Sovereign countr…\n$ TLC        <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\",…\n$ ADMIN      <chr> \"Indonesia\", \"Malaysia\", \"Chile\", \"Bolivia\", \"Peru\", \"Argen…\n$ ADM0_A3    <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ GEOU_DIF   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ GEOUNIT    <chr> \"Indonesia\", \"Malaysia\", \"Chile\", \"Bolivia\", \"Peru\", \"Argen…\n$ GU_A3      <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ SU_DIF     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ SUBUNIT    <chr> \"Indonesia\", \"Malaysia\", \"Chile\", \"Bolivia\", \"Peru\", \"Argen…\n$ SU_A3      <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ BRK_DIFF   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ NAME       <chr> \"Indonesia\", \"Malaysia\", \"Chile\", \"Bolivia\", \"Peru\", \"Argen…\n$ NAME_LONG  <chr> \"Indonesia\", \"Malaysia\", \"Chile\", \"Bolivia\", \"Peru\", \"Argen…\n$ BRK_A3     <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ BRK_NAME   <chr> \"Indonesia\", \"Malaysia\", \"Chile\", \"Bolivia\", \"Peru\", \"Argen…\n$ BRK_GROUP  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ABBREV     <chr> \"Indo.\", \"Malay.\", \"Chile\", \"Bolivia\", \"Peru\", \"Arg.\", \"Dhe…\n$ POSTAL     <chr> \"INDO\", \"MY\", \"CL\", \"BO\", \"PE\", \"AR\", \"DH\", \"CY\", \"IND\", \"C…\n$ FORMAL_EN  <chr> \"Republic of Indonesia\", \"Malaysia\", \"Republic of Chile\", \"…\n$ FORMAL_FR  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ NAME_CIAWF <chr> \"Indonesia\", \"Malaysia\", \"Chile\", \"Bolivia\", \"Peru\", \"Argen…\n$ NOTE_ADM0  <chr> NA, NA, NA, NA, NA, NA, \"U.K.\", NA, NA, NA, NA, NA, NA, NA,…\n$ NOTE_BRK   <chr> NA, NA, NA, NA, NA, NA, \"U.K. Base\", NA, NA, NA, NA, \"Parti…\n$ NAME_SORT  <chr> \"Indonesia\", \"Malaysia\", \"Chile\", \"Bolivia\", \"Peru\", \"Argen…\n$ NAME_ALT   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ MAPCOLOR7  <int> 6, 2, 5, 1, 4, 3, 6, 1, 1, 4, 3, 3, 4, 4, 1, 2, 5, 1, 3, 2,…\n$ MAPCOLOR8  <int> 6, 4, 1, 5, 4, 1, 6, 2, 3, 4, 2, 2, 4, 4, 3, 8, 2, 3, 6, 6,…\n$ MAPCOLOR9  <int> 6, 3, 5, 2, 4, 3, 6, 3, 2, 4, 5, 5, 4, 1, 3, 6, 7, 4, 2, 2,…\n$ MAPCOLOR13 <int> 11, 6, 9, 3, 11, 13, 3, 7, 2, 3, 9, 8, 12, 13, 5, 7, 3, 5, …\n$ POP_EST    <dbl> 270625568, 31949777, 18952038, 11513100, 32510453, 44938712…\n$ POP_RANK   <int> 17, 15, 14, 14, 15, 15, 5, 12, 18, 18, 13, 12, 13, 17, 14, …\n$ POP_YEAR   <int> 2019, 2019, 2019, 2019, 2019, 2019, 2013, 2019, 2019, 2019,…\n$ GDP_MD     <int> 1119190, 364681, 282318, 40895, 226848, 445445, 314, 24948,…\n$ GDP_YEAR   <int> 2019, 2019, 2019, 2019, 2019, 2019, 2013, 2019, 2019, 2019,…\n$ ECONOMY    <chr> \"4. Emerging region: MIKT\", \"6. Developing region\", \"5. Eme…\n$ INCOME_GRP <chr> \"4. Lower middle income\", \"3. Upper middle income\", \"3. Upp…\n$ FIPS_10    <chr> \"ID\", \"MY\", \"CI\", \"BL\", \"PE\", \"AR\", \"-99\", \"CY\", \"IN\", \"CH\"…\n$ ISO_A2     <chr> \"ID\", \"MY\", \"CL\", \"BO\", \"PE\", \"AR\", \"-99\", \"CY\", \"IN\", \"CN\"…\n$ ISO_A2_EH  <chr> \"ID\", \"MY\", \"CL\", \"BO\", \"PE\", \"AR\", \"-99\", \"CY\", \"IN\", \"CN\"…\n$ ISO_A3     <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"-99\", \"CYP\", \"IN…\n$ ISO_A3_EH  <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"-99\", \"CYP\", \"IN…\n$ ISO_N3     <chr> \"360\", \"458\", \"152\", \"068\", \"604\", \"032\", \"-99\", \"196\", \"35…\n$ ISO_N3_EH  <chr> \"360\", \"458\", \"152\", \"068\", \"604\", \"032\", \"-99\", \"196\", \"35…\n$ UN_A3      <chr> \"360\", \"458\", \"152\", \"068\", \"604\", \"032\", \"-099\", \"196\", \"3…\n$ WB_A2      <chr> \"ID\", \"MY\", \"CL\", \"BO\", \"PE\", \"AR\", \"-99\", \"CY\", \"IN\", \"CN\"…\n$ WB_A3      <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"-99\", \"CYP\", \"IN…\n$ WOE_ID     <int> 23424846, 23424901, 23424782, 23424762, 23424919, 23424747,…\n$ WOE_ID_EH  <int> 23424846, 23424901, 23424782, 23424762, 23424919, 23424747,…\n$ WOE_NOTE   <chr> \"Exact WOE match as country\", \"Exact WOE match as country\",…\n$ ADM0_ISO   <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"-99\", \"CYP\", \"IN…\n$ ADM0_DIFF  <chr> NA, NA, NA, NA, NA, NA, \"1\", NA, NA, NA, NA, NA, NA, NA, \"1…\n$ ADM0_TLC   <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_US <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_FR <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_RU <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_ES <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_CN <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_TW <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_IN <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_NP <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_PK <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_DE <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_GB <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_BR <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_IL <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_PS <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_SA <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_EG <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_MA <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_PT <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_AR <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_JP <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_KO <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_VN <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_TR <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_ID <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_PL <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_GR <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_IT <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_NL <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_SE <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_BD <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_UA <chr> \"IDN\", \"MYS\", \"CHL\", \"BOL\", \"PER\", \"ARG\", \"ESB\", \"CYP\", \"IN…\n$ ADM0_A3_UN <int> -99, -99, -99, -99, -99, -99, -99, -99, -99, -99, -99, -99,…\n$ ADM0_A3_WB <int> -99, -99, -99, -99, -99, -99, -99, -99, -99, -99, -99, -99,…\n$ CONTINENT  <chr> \"Asia\", \"Asia\", \"South America\", \"South America\", \"South Am…\n$ REGION_UN  <chr> \"Asia\", \"Asia\", \"Americas\", \"Americas\", \"Americas\", \"Americ…\n$ SUBREGION  <chr> \"South-Eastern Asia\", \"South-Eastern Asia\", \"South America\"…\n$ REGION_WB  <chr> \"East Asia & Pacific\", \"East Asia & Pacific\", \"Latin Americ…\n$ NAME_LEN   <int> 9, 8, 5, 7, 4, 9, 8, 6, 5, 5, 6, 9, 7, 8, 8, 7, 5, 6, 8, 5,…\n$ LONG_LEN   <int> 9, 8, 5, 7, 4, 9, 8, 6, 5, 5, 6, 9, 7, 8, 11, 7, 5, 6, 8, 5…\n$ ABBREV_LEN <int> 5, 6, 5, 7, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 7, 4, 4, 4, 5, 5,…\n$ TINY       <int> -99, -99, -99, -99, -99, -99, 3, -99, -99, -99, -99, -99, 4…\n$ HOMEPART   <int> 1, 1, 1, 1, 1, 1, -99, 1, 1, 1, 1, -99, 1, 1, 1, 1, 1, 1, 1…\n$ MIN_ZOOM   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ MIN_LABEL  <dbl> 1.7, 3.0, 1.7, 3.0, 2.0, 2.0, 6.5, 4.5, 1.7, 1.7, 3.0, 4.5,…\n$ MAX_LABEL  <dbl> 6.7, 8.0, 6.7, 7.5, 7.0, 7.0, 11.0, 9.5, 6.7, 5.7, 8.0, 9.5…\n$ LABEL_X    <dbl> 101.892949, 113.837080, -72.318871, -64.593433, -72.900160,…\n$ LABEL_Y    <dbl> -0.954404, 2.528667, -38.151771, -16.666015, -12.976679, -3…\n$ NE_ID      <dbl> 1159320845, 1159321083, 1159320493, 1159320439, 1159321163,…\n$ WIKIDATAID <chr> \"Q252\", \"Q833\", \"Q298\", \"Q750\", \"Q419\", \"Q414\", \"Q9206745\",…\n$ NAME_AR    <chr> \"إندونيسيا\", \"ماليزيا\", \"تشيلي\", \"بوليفيا\", \"بيرو\", \"الأرجن…\n$ NAME_BN    <chr> \"ইন্দোনেশিয়া\", \"মালয়েশিয়া\", \"চিলি\", \"বলিভিয়া\", \"পেরু\", \"আর্জেন্…\n$ NAME_DE    <chr> \"Indonesien\", \"Malaysia\", \"Chile\", \"Bolivien\", \"Peru\", \"Arg…\n$ NAME_EN    <chr> \"Indonesia\", \"Malaysia\", \"Chile\", \"Bolivia\", \"Peru\", \"Argen…\n$ NAME_ES    <chr> \"Indonesia\", \"Malasia\", \"Chile\", \"Bolivia\", \"Perú\", \"Argent…\n$ NAME_FA    <chr> \"اندونزی\", \"مالزی\", \"شیلی\", \"بولیوی\", \"پرو\", \"آرژانتین\", \"د…\n$ NAME_FR    <chr> \"Indonésie\", \"Malaisie\", \"Chili\", \"Bolivie\", \"Pérou\", \"Arge…\n$ NAME_EL    <chr> \"Ινδονησία\", \"Μαλαισία\", \"Χιλή\", \"Βολιβία\", \"Περού\", \"Αργεν…\n$ NAME_HE    <chr> \"אינדונזיה\", \"מלזיה\", \"צ'ילה\", \"בוליביה\", \"פרו\", \"ארגנטינה\"…\n$ NAME_HI    <chr> \"इंडोनेशिया\", \"मलेशिया\", \"चिली\", \"बोलिविया\", \"पेरू\", \"अर्जेण्टीना\",…\n$ NAME_HU    <chr> \"Indonézia\", \"Malajzia\", \"Chile\", \"Bolívia\", \"Peru\", \"Argen…\n$ NAME_ID    <chr> \"Indonesia\", \"Malaysia\", \"Chili\", \"Bolivia\", \"Peru\", \"Argen…\n$ NAME_IT    <chr> \"Indonesia\", \"Malaysia\", \"Cile\", \"Bolivia\", \"Perù\", \"Argent…\n$ NAME_JA    <chr> \"インドネシア\", \"マレーシア\", \"チリ\", \"ボリビア\", \"ペルー\",…\n$ NAME_KO    <chr> \"인도네시아\", \"말레이시아\", \"칠레\", \"볼리비아\", \"페루\", \"아…\n$ NAME_NL    <chr> \"Indonesië\", \"Maleisië\", \"Chili\", \"Bolivia\", \"Peru\", \"Argen…\n$ NAME_PL    <chr> \"Indonezja\", \"Malezja\", \"Chile\", \"Boliwia\", \"Peru\", \"Argent…\n$ NAME_PT    <chr> \"Indonésia\", \"Malásia\", \"Chile\", \"Bolívia\", \"Peru\", \"Argent…\n$ NAME_RU    <chr> \"Индонезия\", \"Малайзия\", \"Чили\", \"Боливия\", \"Перу\", \"Аргент…\n$ NAME_SV    <chr> \"Indonesien\", \"Malaysia\", \"Chile\", \"Bolivia\", \"Peru\", \"Arge…\n$ NAME_TR    <chr> \"Endonezya\", \"Malezya\", \"Şili\", \"Bolivya\", \"Peru\", \"Arjanti…\n$ NAME_UK    <chr> \"Індонезія\", \"Малайзія\", \"Чилі\", \"Болівія\", \"Перу\", \"Аргент…\n$ NAME_UR    <chr> \"انڈونیشیا\", \"ملائیشیا\", \"چلی\", \"بولیویا\", \"پیرو\", \"ارجنٹائ…\n$ NAME_VI    <chr> \"Indonesia\", \"Malaysia\", \"Chile\", \"Bolivia\", \"Peru\", \"Argen…\n$ NAME_ZH    <chr> \"印度尼西亚\", \"马来西亚\", \"智利\", \"玻利维亚\", \"秘鲁\", \"阿根…\n$ NAME_ZHT   <chr> \"印度尼西亞\", \"馬來西亞\", \"智利\", \"玻利維亞\", \"秘魯\", \"阿根…\n$ FCLASS_ISO <chr> \"Admin-0 country\", \"Admin-0 country\", \"Admin-0 country\", \"A…\n$ TLC_DIFF   <chr> NA, NA, NA, NA, NA, NA, \"1\", NA, NA, NA, NA, NA, NA, NA, NA…\n$ FCLASS_TLC <chr> \"Admin-0 country\", \"Admin-0 country\", \"Admin-0 country\", \"A…\n$ FCLASS_US  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ FCLASS_FR  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ FCLASS_RU  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FCLASS_ES  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ FCLASS_CN  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FCLASS_TW  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Unrecognized\", NA, NA,…\n$ FCLASS_IN  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FCLASS_NP  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FCLASS_PK  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Unrecognized\", \"Ad…\n$ FCLASS_DE  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ FCLASS_GB  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ FCLASS_BR  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FCLASS_IL  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FCLASS_PS  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Admin-0 countr…\n$ FCLASS_SA  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Unrecognized\", \"Ad…\n$ FCLASS_EG  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FCLASS_MA  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FCLASS_PT  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ FCLASS_AR  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FCLASS_JP  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FCLASS_KO  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ FCLASS_VN  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FCLASS_TR  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ FCLASS_ID  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FCLASS_PL  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ FCLASS_GR  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ FCLASS_IT  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ FCLASS_NL  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ FCLASS_SE  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ FCLASS_BD  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Unrecognized\", \"Ad…\n$ FCLASS_UA  <chr> NA, NA, NA, NA, NA, NA, \"Admin-0 dependency\", NA, NA, NA, N…\n$ geom       <MULTIPOLYGON [°]> MULTIPOLYGON (((117.7036 4...., MULTIPOLYGON (…\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nReview the tibble vignette on Column types for more detailed explanation of geometry types, dimensions, coordinate reference systems, and more.\n\n\n\nA sf object can have any number of rows but it always has at least one special list column with the geometry for each feature. This column is usually named “geometry” or “geom” but it can be named anything. You can extract, rename, or replace the geometry column using the sf::st_geometry function.\n\nsf::st_geometry(countries)\n\nGeometry set for 258 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -90 xmax: 180 ymax: 83.6341\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((117.7036 4.163415, 117.7036 4.1...\n\n\nMULTIPOLYGON (((117.7036 4.163415, 117.6971 4.1...\n\n\nMULTIPOLYGON (((-69.51009 -17.50659, -69.50611 ...\n\n\nMULTIPOLYGON (((-69.51009 -17.50659, -69.51009 ...\n\n\nMULTIPOLYGON (((-69.51009 -17.50659, -69.63832 ...\n\n\nIn addition to a geometry column, sf object also four special attributes that make it different than other dataframes. These attributes are:\n\nGeometry type\nDimensions\nBounding box\nCoordinate reference system\n\nSimple feature collections (sfc) objects share all these same attributes. Bounding box (bbox) objects have a crs attribute but none of the other attributes.\nGeometry types\nYou can use sf::st_geometry_type() to list the geometry types for any sf object. All of the features in countries use MULTIPOLYGON geometry. Features in populated_places use POINT geometry.\n\nst_geometry_type(countries, by_geometry = FALSE)\n\n[1] MULTIPOLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\nst_geometry_type(populated_places, by_geometry = FALSE)\n\n[1] POINT\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\n\nWhile GeoPackage and shapefiles only support a single geometry type for each layer, sf objects do support mixed types. To show how this works, we can filter the populated places and country boundaries data to a single country then combine both objects into a single object using dplyr::bind_rows().\n\n\n\n\n\n\nTip\n\n\n\n\n\nReview the Data transformation chapter from R for Data Science for more on how to use dplyr package to filter and arrange rows, select columns, and add new variables to a data frame.\n\n\n\n\nusa_name <- \"United States of America\"\n\nusa_places <- filter(populated_places, adm0name == usa_name)\n\nusa_boundaries <- filter(countries, SOVEREIGNT == usa_name)\n\nusa <-\n  bind_rows(\n    usa_places,\n    usa_boundaries\n  )\n\nst_geometry_type(usa, by_geometry = TRUE)\n\n  [1] POINT        POINT        POINT        POINT        POINT       \n  [6] POINT        POINT        POINT        POINT        POINT       \n [11] POINT        POINT        POINT        POINT        POINT       \n [16] POINT        POINT        POINT        POINT        POINT       \n [21] POINT        POINT        POINT        POINT        POINT       \n [26] POINT        POINT        POINT        POINT        POINT       \n [31] POINT        POINT        POINT        POINT        POINT       \n [36] POINT        POINT        POINT        POINT        POINT       \n [41] POINT        POINT        POINT        POINT        POINT       \n [46] POINT        POINT        POINT        POINT        POINT       \n [51] POINT        POINT        POINT        POINT        POINT       \n [56] POINT        POINT        POINT        POINT        POINT       \n [61] POINT        POINT        POINT        POINT        POINT       \n [66] POINT        POINT        POINT        POINT        POINT       \n [71] POINT        POINT        POINT        POINT        POINT       \n [76] POINT        POINT        POINT        POINT        POINT       \n [81] POINT        POINT        POINT        POINT        POINT       \n [86] POINT        POINT        POINT        POINT        POINT       \n [91] POINT        POINT        POINT        POINT        POINT       \n [96] POINT        POINT        POINT        POINT        POINT       \n[101] POINT        POINT        POINT        POINT        POINT       \n[106] POINT        POINT        POINT        POINT        POINT       \n[111] POINT        MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n[116] MULTIPOLYGON MULTIPOLYGON MULTIPOLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\n\nYou can also use the sf::st_is() function to test if an object matches a specific geometry type. You can combine this function with all or any to check features as a whole.\n\nst_is(usa_boundaries, type = \"MULTIPOLYGON\")\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\nany(st_is(usa_places, type = \"MULTIPOLYGON\"))\n\n[1] FALSE\n\nall(st_is(usa_places, type = \"POINT\"))\n\n[1] TRUE\n\nany(st_is(usa, type = \"MULTIPOLYGON\"))\n\n[1] TRUE\n\n\nDimension\nsf objects must have at least two dimensions: X and Y. All geometries (such as polygons or linestrings) are made up of points so two dimensions are required to locate a point within a coordinate reference system. You may also see people refer to X and Y as easting and northing or longitude and latitude. sf objects can optionally include a Z dimension (for altitude) or a M coordinate (for a measure associated with an individual point). The M coordinate is rarely used but can be\nM coordinate (rarely used), denoting some measure that is associated with the point, rather than with the feature as a whole (in which case it would be a feature attribute) (such as the time of measurement or measurement error of the coordinates)\nIf you do not need the Z dimension in your data, you can drop it using the sf::st_zm() function.\nAll geometries are composed of points. Points are coordinates in a 2-, 3- or 4-dimensional space. All points in a geometry have the same dimensionality. In addition to X and Y coordinates, there are two optional additional dimensions:\na Z coordinate, denoting altitude\nan \nThe four possible cases then are:\ntwo-dimensional points refer to x and y, , we refer to them as XY\nthree-dimensional points as XYZ\nthree-dimensional points as XYM\nfour-dimensional points as XYZM (the third axis is Z, fourth M)\nBounding box\nYou can get the bounding box for any sf or sfc object using sf::st_bbox\n\nusa_bbox <- st_bbox(usa)\n\nusa_bbox\n\n      xmin       ymin       xmax       ymax \n-179.14350  -14.53289  179.78094   71.41250 \n\n\nA bounding box is a named numeric vector with a crs attribute. You can convert it to a numeric vector using as.numeric() or get the coordinate reference system with sf::st_crs.\n\nas.numeric(usa_bbox)\n\n[1] -179.14350  -14.53289  179.78094   71.41250\n\nst_crs(usa_bbox)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nYou can convert a numeric vector back into a bounding box object with sf::st_bbox.\n\nst_bbox(\n  c(\n    \"xmin\" = -179.14350,\n    \"ymin\" = -14.53289,\n    \"xmax\" = 179.78094,\n    \"ymax\" = 71.41250\n  )\n)\n\n      xmin       ymin       xmax       ymax \n-179.14350  -14.53289  179.78094   71.41250 \n\n\nIf we want to plot the bounding box on a map, we can convert it into a sfc object with sf::st_as_sfc() and convert the sfc object into an sf object using sf::st_as_sf().\n\nusa_bbox_sfc <- st_as_sfc(usa_bbox)\n\nusa_bbox_sf <- st_as_sf(usa_bbox_sfc)\n\nFinally, we can use tmap::tmap_leaflet() to compare the bounding box to the country boundary and places objects created in the previous section on geometry types.\n\ntmap_leaflet(\n  tm_shape(usa_bbox_sf) +\n    tm_borders() +\n    tm_shape(usa_boundaries) +\n    tm_polygons(col = \"NAME\", alpha = 0.2) +\n    tm_shape(usa_places) +\n    tm_sf(id = \"name\", alpha = 0.8)\n)\n\n\n\n\n\n\nCoordinate reference systems\nYou can get a coordinate reference system with sf::st_crs. This returns a crs object which has the crs for the user input object as a character string and the well-known text (wkt) for the coordinate reference system.\n\nusa_crs <- st_crs(usa)\n\nusa_crs\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nA crs object also has a method for returning the spatial reference identifier (or SRID). The SRID is a unique identifier for a specific coordinate system, tolerance, and resolution.\n\n\n\n\n\n\nFind coordinate reference systems\n\n\n\n\n\nYou can explore a database of over 6000 coordinate reference system with the corresponding EPSG and ESRI SRID codes at EPSG.io. You can also use the crsuggest package which uses data from the EPSG Registry (a product of the International Association of Oil & Gas Producers). The Web Mercator projection or “EPSG:3857” is convenient option that works well for many use cases.\n\n\n\n\nst_crs(usa)$srid\n\n[1] \"EPSG:4326\"\n\n\nYou can change the coordinate reference system using the sf::st_transform function:\n\nusa_3857 <- st_transform(usa, 3857)\n\nst_crs(usa_3857)\n\nCoordinate Reference System:\n  User input: EPSG:3857 \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\n\nYou can also check if a object is using a geographic (also known as geodetic or simply lon/lat) or projected coordinate reference system using the sf::st_is_longlat function:\n\nst_is_longlat(usa)\n\n[1] TRUE\n\nst_is_longlat(usa_3857)\n\n[1] FALSE\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nReview the sf vignette Spherical geometry in sf using s2geometry for a technical explanation of how sf uses the S2 Geometry Library when manipulating data with a geographic coordinate reference system.\n\n\n\nBoth ggplot2 and tmap can convert the crs of input objects before mapping. For example, tm_shape supports an optional “projection” argument such as “EPSG:3035” (the Lambert azimuthal equal-area projection):\n\ntm_shape(countries, projection = \"EPSG:3035\") +\n  tm_polygons(\"grey85\", border.col = \"grey30\") +\n  tm_layout(earth.boundary = TRUE, frame = FALSE)\n\n\n\n\nThe ggplot2::geom_sf function uses the coordinate reference system of the first sf object provided and re-projects additional objects to match.\n\nlibrary(ggplot2)\n\ncountries_3857 <- st_transform(countries, 3857)\n\nggplot() +\n  geom_sf(data = populated_places, alpha = 0.2) +\n  geom_sf(data = countries_3857, fill = NA) +\n  labs(title = st_crs(populated_places)$srid) +\n  theme_minimal()"
  },
  {
    "objectID": "examples/02_read-tabular-data-r.html#read-tabular-data-with-coordinates-and-convert-to-sf",
    "href": "examples/02_read-tabular-data-r.html#read-tabular-data-with-coordinates-and-convert-to-sf",
    "title": "Read tabular data and convert to spatial data with R",
    "section": "Read tabular data with coordinates and convert to sf",
    "text": "Read tabular data with coordinates and convert to sf\nThe readr::read_csv() function from the readr package is the preferred method for importing a comma separated variable (csv) data file. The package also supports less common tabular data files such as tab separated variable (tsv). You could also use the read.csv from the utils package (included with base R).\nFor example, you can read a csv file with the names and locations of Baltimore City Public School System programs.\n\nfile <- here::here(\"files/data\", \"bcps_programs_SY2122.csv\")\n\nbcps_programs_df <- read_csv(file = file)\n\nRows: 162 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): program_name_short, type, category\ndbl (3): program_number, lon, lat\nlgl (1): swing_space\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBy default, read_csv prints the number of rows and columns, the selected delimiter and the automatic specification of column types. In general, you should always examine the data after reading it in to make sure the column types are appropriate or if manual specification is needed.\n\nbcps_programs_df\n\n# A tibble: 162 × 7\n   program_name_short program_number type            categ…¹ swing…²   lon   lat\n   <chr>                       <dbl> <chr>           <chr>   <lgl>   <dbl> <dbl>\n 1 Cecil E                         7 Traditional     E       FALSE   -76.6  39.3\n 2 Steuart Hill E                  4 Traditional     E       FALSE   -76.6  39.3\n 3 Lakeland EM                    12 Traditional     EM      FALSE   -76.6  39.3\n 4 Eutaw-Marshburn E              11 Traditional     E       FALSE   -76.6  39.3\n 5 City Springs EM                 8 Charter-Conver… EM      FALSE   -76.6  39.3\n 6 James McHenry EM               10 Traditional     EM      FALSE   -76.6  39.3\n 7 Tench Tilghman EM              13 Traditional     EM      FALSE   -76.6  39.3\n 8 Stadium School M               15 Traditional     M       FALSE   -76.6  39.3\n 9 Hilton E                       21 Traditional     E       FALSE   -76.7  39.3\n10 Johnston Sq E                  16 Traditional     E       FALSE   -76.6  39.3\n# … with 152 more rows, and abbreviated variable names ¹​category, ²​swing_space\n\n\nOne way to address issues with column specification is to read all columns as character types and convert columns to other types as needed.\n\nbcps_programs_df <- read_csv(file = file, col_types = \"ccccccc\")\n\nbcps_programs_df <- bcps_programs_df %>%\n  mutate(\n    lat = as.numeric(lat),\n    lon = as.numeric(lon)\n  )\n\n\n\n\n\n\n\nValidating CSV files\n\n\n\n\n\nIn some cases, you may have a CSV file that does not work the way you expect or a CSV file that you want to validate using an established schema. CSVLint or Data Curator (from the Queensland Cyber Infrastructure Foundation) are two tools developed to support this type of validation.\n\n\n\nYou can then use st_as_sf() function from the sf package to convert the data frame read into a simple feature object. You must specify the name of the coordinate columns and the expected coordinate reference system. By default, the st_as_sf returns a tibble data frame.\n\nbcps_programs_sf <-\n  st_as_sf(\n    x = bcps_programs_df,\n    coords = c(\"lon\", \"lat\"),\n    crs = 4326\n  )\n\nclass(bcps_programs_sf)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\nggplot() +\n  geom_sf(data = md_counties[3, ]) +\n  geom_sf(data = bcps_programs_sf)\n\n\n\n\nThis function can be tricky for novice users. Missing values in coordinate columns results in an error by default (set na.fail = FALSE to change this).\n\nbcps_programs_df_error <- bcps_programs_df\nbcps_programs_df_error[1, ]$lon <- NA\n\nst_as_sf(\n  x = bcps_programs_df_error,\n  coords = c(\"lon\", \"lat\"),\n  crs = 4326\n)\n\nError in st_as_sf.data.frame(x = bcps_programs_df_error, coords = c(\"lon\", : missing values in coordinates not allowed\n\n\nIf you reverse the order of the latitude and longitude columns, the function returns an sf object—but with the points at incorrect or nonexistent locations.\n\nbcps_programs_coord_rev <-\n  st_as_sf(\n    x = bcps_programs_df,\n    coords = c(\"lat\", \"lon\"),\n    crs = 4326\n  )\n\nggplot() +\n  geom_sf(data = bcps_programs_sf, color = \"green\") +\n  geom_sf(data = bcps_programs_coord_rev, color = \"red\")\n\n\n\n\n\n\n\n\n\n\nLongitude and latitude or latitude and longitude?\n\n\n\n\n\nTom MacWright (creator geojson.io and the OpenStreetMap iD editor) documented the inconsistent order of latitude and longitude across a range of file formats, APIs, and service specifications. If you’re wondering, which order is right, MacWright explains: “Neither. This is an opinion with no right answer. Geographical tradition favors lat, lon. Math and software prefer lon, lat.”\n\n\n\nWorking with other types of tabular data follows a similar work flow.\n\nDate from Excel files (xls or xslx) can be imported using readxl::read_excel. Both readr and readxl work with local file paths or with remote files if you provide a url.\nData from Google Sheets can be accessed with googlesheets4::read_sheet. Google Sheets can be particularly useful when collaborating on a data project with people who can’t use a GIS application or programming language.\nNested data from a JSON file can be read with jsonlite::read_json and converted to a data frame by setting simplifyVector = TRUE.\n\n\n\n\n\n\n\nBeware auto-formatting in Excel\n\n\n\n\n\nThe convenience of automatic formatting in Microsoft Excel can cause with automatic rounding of numeric coordinates and automatic formatting of non-date values as date.\n\n\n\nThe sf package also supports reading Excel and CSV files directly but it returns a tibble (not an sf object) when trying to read data where the geometry is contained in numeric coordinate columns.\n\nbcps_programs_st_read <-\n  st_read(\n    dsn = file,\n    coords = c(\"lon\", \"lat\"),\n    crs = 4326\n  )\n\nReading layer `bcps_programs_SY2122' from data source \n  `/Users/elipousson/Documents/GitHub/bldgspatialdata/files/data/bcps_programs_SY2122.csv' \n  using driver `CSV'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\n\nHowever, st_read does work with a CSV file where the geometry has been encoded as well-known text such as the sample “baltimore_city_wkt.csv” file.\n\ndsn <- here::here(\"files/data\", \"baltimore_city_wkt.csv\")\n\nbaltimore_city <-\n  st_read(\n    dsn = dsn,\n    crs = 4326\n  )\n\nReading layer `baltimore_city_wkt' from data source \n  `/Users/elipousson/Documents/GitHub/bldgspatialdata/files/data/baltimore_city_wkt.csv' \n  using driver `CSV'\nSimple feature collection with 1 feature and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -76.71152 ymin: 39.19721 xmax: -76.52946 ymax: 39.37221\nGeodetic CRS:  WGS 84\n\nggplot() +\n  geom_sf(data = baltimore_city, fill = NA) +\n  theme_minimal()"
  },
  {
    "objectID": "examples/02_read-tabular-data-r.html#read-and-geocode-tabular-data-with-addresses",
    "href": "examples/02_read-tabular-data-r.html#read-and-geocode-tabular-data-with-addresses",
    "title": "Read tabular data and convert to spatial data with R",
    "section": "Read and geocode tabular data with addresses",
    "text": "Read and geocode tabular data with addresses\nWhat if you do not have coordinates? Geocoding address data is a common task and there are many ways to do this using R and a variety of other tools. The easiest package in most cases is the tidygeocoder package. This package supports over a dozen different geocoding services including Nominatim and the US Census Burearu geocoder (which are both free to use without an API key).\nUnfortunately, accurately geocoding address data can be challenging. This toppic is addresed in more detail in the example on cleaning data."
  },
  {
    "objectID": "examples/02_read-tabular-data-r.html#read-tabular-data-and-join-geometry-based-on-a-named-area-or-location",
    "href": "examples/02_read-tabular-data-r.html#read-tabular-data-and-join-geometry-based-on-a-named-area-or-location",
    "title": "Read tabular data and convert to spatial data with R",
    "section": "Read tabular data and join geometry based on a named area or location",
    "text": "Read tabular data and join geometry based on a named area or location\nOne last common way to add geometry to tabular data is to join the table to geometry based on a named area (typically an administrative boundary) or a named location.\n\nfile_url <- \"https://opendata.maryland.gov/api/views/ftsr-vapt/rows.csv\"\n\nmd_foreclosure <- file_url %>%\n  read_csv(show_col_types = FALSE) %>%\n  tidyr::pivot_longer(\n    cols = !contains(\"Zip\"),\n    names_to = \"date\",\n    values_to = \"count\"\n  )\n\nFor data organized by zipcode can be matched to geometry for Zip Code Tabulation Areas (or ZCTAs) downloaded from the U.S. Census Bureau API with the tigris::zcta function and saved to a GeoPackage file “md_zctas.gpkg”. The “Zip” column should be equivalent to the “ZCTA5CE20” column from md_zctas.\n\nmd_zctas <- read_sf(here::here(\"files/data/md_zctas.gpkg\"))\n\nmd_foreclosure <-\n  left_join(\n    md_foreclosure,\n    md_zctas,\n    by = c(\"Zip\" = \"ZCTA5CE20\")\n  )\n\nmd_foreclosure_sf <- st_as_sf(md_foreclosure)\n\nmd_foreclosure_202207 <- filter(md_foreclosure_sf, date == \"July 2022\")\n\nggplot() +\n  geom_sf(data = md_foreclosure_202207, aes(fill = count), color = NA) +\n  geom_sf(data = md_counties, fill = NA, color = \"white\", size = 0.2) +\n  scale_fill_distiller(palette = \"PuRd\") +\n  theme_void() +\n  labs(\n    title = \"Notice of intent to foreclosure by zipcode in Maryland, July 2022\",\n    fill = \"# of notices\",\n    caption = \"Data courtesy Open Maryland\"\n  )"
  },
  {
    "objectID": "examples/06_spatial-operations-r-crashapi.html",
    "href": "examples/06_spatial-operations-r-crashapi.html",
    "title": "Spatial operations with the sf, tigris, and crashapi packages",
    "section": "",
    "text": "This exercise is designed to be used as an interactive activity to better understand how to work with spatial transformation functions from the sf package. A few suggestions:"
  },
  {
    "objectID": "examples/06_spatial-operations-r-crashapi.html#getting-started",
    "href": "examples/06_spatial-operations-r-crashapi.html#getting-started",
    "title": "Spatial operations with the sf, tigris, and crashapi packages",
    "section": "Getting started",
    "text": "Getting started\nLoading packages\nFirst we need to load a few packages:\n\nlibrary(tmap)\nlibrary(sf)\nlibrary(dplyr)\n\nDownloading data\nThen, we can download the counties with tigris::counties() and then filter to those counties in the Baltimore–Columbia–Towson Metropolitan Statistical Area.\n\n# Set the state abbreviation\nmd <- \"MD\"\n\n# Get the counties with tigris::counties()\nmd_counties <- tigris::counties(state = md, cb = TRUE)\n\n# FIPS codes for the counties in the Baltimore MSA\nmsa_fips <- c(\"510\", \"005\", \"013\", \"003\", \"027\", \"035\", \"025\")\n\n# Filter to the counties in the Baltimore MSA\nmsa_counties <-\n  filter(\n    md_counties,\n    COUNTYFP %in% msa_fips\n  )\n\nI also want data on U.S. highways. We won’t use this data right away but we’ll need it later on in this exercise.\n\nus_highways <- tigris::primary_roads(year = 2020)\n\nNow, I can use the county FIPS code column from msa (named COUNTYFP) to get crash data using the {crashapi} package and the National Highway Traffic Safety Administration (NHTSA) Fatality Analysis Reporting System (FARS) API.\nBelow, I am using purrr::map_dfr() allows us to combine results for each county into a single data frame. When geometry = TRUE, crashapi::get_fars_crashes() is using sf::st_as_sf() to convert a data frame into a sf object based on the longitude and latitude columns.\n\nmd_crashes <-\n  purrr::map_dfr(\n    md_counties$COUNTYFP,\n    ~ crashapi::get_fars_crashes(\n      year = 2020,\n      state = md,\n      county = .x,\n      geometry = TRUE\n    )\n  )\n\nmsa_crashes <-\n  filter(\n    md_crashes,\n    county %in% as.integer(msa_fips)\n  )\n\n\n\n\n\n\n\nIf you’re interested in learning more using API packages, check out Ch. 8 Geographic data I/O in Lovelace, Nowosad, and Muenchow (2022). For more on the advanced topic of developing API packages, check out this vignette from the {httr2} package (which is what {crashapi} is using to download data from NHTSA).\n\n\n\nMapping data\nFor this exercise, we are using the {tmap} package to take a look at the data. Visual checks are often essential to exploring and understanding what the spatial and geometric transformation functions are doing to your data. Similar to ggplot2 plots, you can put the results of a map into an object that you can use as a base map.\n\nShow the code | Map Baltimore MSA boundariesmd_borders <-\n  tm_shape(md_counties) +\n  tm_borders(col = \"gray60\")\n\nmsa_borders <-\n  tm_shape(msa_counties) +\n  tm_borders(col = \"gray60\")\n\nmsa_borders +\n  tm_shape(msa_crashes) +\n  tm_dots(col = \"red\", alpha = 0.4)\n## Warning in sf::st_is_longlat(shp2): bounding box has potentially an invalid\n## value range for longlat data\n\n\n\n\nWait a moment: did you see spot the warning up there? 👆🏼 It said: “Warning in sf::st_is_longlat(shp2): bounding box has potentially an invalid value range for longlat data.”\nWhere is that coming from? Let’s take a quick look at the data using the base summary() function:\n\nsummary(msa_crashes)\n##      city             cityname            county           countyname       \n##  Length:229         Length:229         Length:229         Length:229        \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n##    case_year        fatals         latitude         longitud      \n##  Min.   :2020   Min.   :1.000   Min.   : 38.73   Min.   : -77.17  \n##  1st Qu.:2020   1st Qu.:1.000   1st Qu.: 39.20   1st Qu.: -76.75  \n##  Median :2020   Median :1.000   Median : 39.30   Median : -76.63  \n##  Mean   :2020   Mean   :1.061   Mean   : 39.73   Mean   : -68.20  \n##  3rd Qu.:2020   3rd Qu.:1.000   3rd Qu.: 39.37   3rd Qu.: -76.53  \n##  Max.   :2020   Max.   :3.000   Max.   :100.00   Max.   :1000.00  \n##     state            statename           st_case          totalvehicles  \n##  Length:229         Length:229         Length:229         Min.   :1.000  \n##  Class :character   Class :character   Class :character   1st Qu.:1.000  \n##  Mode  :character   Mode  :character   Mode  :character   Median :1.000  \n##                                                           Mean   :1.629  \n##                                                           3rd Qu.:2.000  \n##                                                           Max.   :8.000  \n##    tway_id            tway_id2            ve_forms             geometry  \n##  Length:229         Length:229         Min.   :1.00   POINT        :229  \n##  Class :character   Class :character   1st Qu.:1.00   epsg:4326    :  0  \n##  Mode  :character   Mode  :character   Median :1.00   +proj=long...:  0  \n##                                        Mean   :1.52                      \n##                                        3rd Qu.:2.00                      \n##                                        Max.   :5.00\n\n☝️ Hint: look at the range of the latitude and longitude columns.\nDid you spot the issue? I can use filter to track down the problem:\n\nfilter(\n  msa_crashes,\n  longitud > 180 | latitude > 90 | longitud < -180 | latitude < -90\n)\n## Warning in st_is_longlat(x): bounding box has potentially an invalid value range\n## for longlat data\n## # A tibble: 2 × 16\n##   city  cityname  county countyname case_…¹ fatals latit…² longi…³ state state…⁴\n##   <chr> <chr>     <chr>  <chr>        <int>  <int>   <dbl>   <dbl> <chr> <chr>  \n## 1 50    BALTIMORE 510    BALTIMORE…    2020      1   100.    1000. 24    Maryla…\n## 2 50    BALTIMORE 510    BALTIMORE…    2020      1    77.8    778. 24    Maryla…\n## # … with 6 more variables: st_case <chr>, totalvehicles <int>, tway_id <chr>,\n## #   tway_id2 <chr>, ve_forms <int>, geometry <POINT [°]>, and abbreviated\n## #   variable names ¹​case_year, ²​latitude, ³​longitud, ⁴​statename\n\nTo fix this issue, we need to crop the data to exclude outlying invalid coordinates. Cropping is one of a few geometry operations supported by the sf package. Let’s dig in."
  },
  {
    "objectID": "examples/06_spatial-operations-r-crashapi.html#geometry-operations",
    "href": "examples/06_spatial-operations-r-crashapi.html#geometry-operations",
    "title": "Spatial operations with the sf, tigris, and crashapi packages",
    "section": "Geometry operations",
    "text": "Geometry operations\nCropping and transforming data\nHow can I use st_crop() to exclude those incorrectly located crashes? st_crop() takes two parameters:\n\n\nx: A sf or sfc object that you want to crop\n\n\ny: A sf, sfc, or bbox object (or numeric x and y min/max values) that you want to crop to\n\n\nThis pattern of an x and y parameter is common across all the geometry operations although, for at least some, the second parameter is optional.\n🤔 What parameters do you need to crop msa_crashes?\n\n# msa_crashes <- st_crop()\n\nOops. Did you get an error?\nHere is the catch: both objects need to use the same coordinate reference system for spatial transformation functions to work. To do this we need to know what coordinate reference system msa_crashes is using.\nYou can use st_crs() to check:\n\nst_crs(msa_crashes)\n## Coordinate Reference System:\n##   User input: EPSG:4326 \n##   wkt:\n## GEOGCRS[\"WGS 84\",\n##     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n##         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n##         MEMBER[\"World Geodetic System 1984 (G730)\"],\n##         MEMBER[\"World Geodetic System 1984 (G873)\"],\n##         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n##         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n##         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n##         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n##         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n##             LENGTHUNIT[\"metre\",1]],\n##         ENSEMBLEACCURACY[2.0]],\n##     PRIMEM[\"Greenwich\",0,\n##         ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     CS[ellipsoidal,2],\n##         AXIS[\"geodetic latitude (Lat)\",north,\n##             ORDER[1],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         AXIS[\"geodetic longitude (Lon)\",east,\n##             ORDER[2],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     USAGE[\n##         SCOPE[\"Horizontal component of 3D system.\"],\n##         AREA[\"World.\"],\n##         BBOX[-90,-180,90,180]],\n##     ID[\"EPSG\",4326]]\n\n# You can also use st_crs() to get a Spatial Reference identifier (srid) for a coordinate reference system\n# st_crs(msa_crashes)$srid\n\nThen we need to use the st_transform() function to change to coordinate reference system of msa_counties. The first parameter of st_transform() is always the sf or sfc object to transform but the second parameter can be a number, a character, or the output from the st_crs() function. For example, both of these calls transform msa_counties to the Pseudo-Mercator coordinate reference system used by Google Maps, OpenStreetMap, and other web maps:\n\nst_transform(msa_counties, 3857)\n\nst_transform(msa_counties, \"EPSG:3857\")\n\n🤔 Now it is your turn to transform msa_counties. Fill in the missing parameters:\n\n# msa_counties <- st_transform()\n\nFinally, you can crop msa_crashes the way you wanted:\n\n# Overwrite msa_crashes w/ cropped data\nmsa_crashes <- st_crop(msa_crashes, msa_counties)\n\nOK. Let’s map the cropped msa_crashes data:\n\nShow the code | Map crashes in the Baltimore metro areamsa_borders +\n  tm_shape(msa_crashes) +\n  tm_dots(col = \"red\")\n## Warning in sf::st_is_longlat(shp2): bounding box has potentially an invalid\n## value range for longlat data\n\n\n\n\nTake a moment and compare this map with a map of md_crashes (which also needs to be cropped). Remember, tmap uses the bounding box of the first sf object passed to tm_shape() to set the plot area.\n\nmd_counties <- st_transform(md_counties, st_crs(md_crashes))\n# Crop md_crashes also\nmd_crashes <- st_crop(md_crashes, md_counties)\n\nmsa_borders +\n  tm_shape(md_crashes) +\n  tm_dots(col = \"red\")\n\n\n\n\nDo you notice the difference between the two maps?\nImagine we didn’t have couldn’t use a county attribute to filter msa_crashes from md_crashes. How could we get data that is more like the first map?\nGetting the intersection or difference between pairs of simple features\nLet’s try out st_intersection(). Like st_crop(), this function takes two parameters but works a more like a cookie cutter—trimming the features of the first parameter to the boundary of the second parameter.\nFor example, I can trim md_crashes to the boundary of Baltimore City.\n\nmd_counties <- st_transform(md_counties, st_crs(md_crashes))\nmsa_counties <- st_transform(msa_counties, st_crs(md_crashes))\n\nbaltimore_city <- msa_counties[7, ]\n\nbaltimore_crashes <- st_intersection(md_crashes, baltimore_city)\n## Warning: attribute variables are assumed to be spatially constant throughout all\n## geometries\n\nThe function did a little more than just exclude the crashes outside Baltimore City. The names and attributes for any intersecting features from the second parameter have been joined to the features from the first parameter. You can actually get the same result using the st_join() function when left = FALSE:\n\nbaltimore_crashes_join <- st_join(md_crashes, baltimore_city, left = FALSE, join = st_intersects)\n\n# all.equal tests if two objects are (nearly) equal\nall.equal(baltimore_crashes, baltimore_crashes_join)\n## [1] TRUE\n\nst_difference() works more like an eraser—selectively removing a specific area.\n\ncrashes_outside_baltimore <- st_difference(md_crashes, baltimore_city)\n## Warning: attribute variables are assumed to be spatially constant throughout all\n## geometries\n\nI can map the data to take a look at the results:\n\nShow the code | Map crashes in and outside of Baltimore citymsa_borders +\n  tm_shape(baltimore_crashes) +\n  tm_dots(col = \"orange\") +\n  tm_shape(crashes_outside_baltimore) +\n  tm_dots(col = \"red\")\n\n\n\n\n🤔 Now, your turn. Can you use st_intersection() to trim the crash data to Anne Arundel County?\n\n# anne_arundel_crashes <-\n\n🤔 Can you map the Anne Arundel County crashes over top of a crash map for the full state?\n\n# md_borders +\n#   tm_shape(anne_arundel_crashes) +\n#   tm_dots()\n\n🤔 Bonus question: can you figure out how many fatalities occurred in Anne Arundel County? Remember, the number of fatalities may not be the same as fatal crashes. Try using the nrow() and sum() functions to determine these numbers.\n\n\n\nIf you the intersection worked as expected, you should find 50 crashes and 50 fatalities."
  },
  {
    "objectID": "examples/06_spatial-operations-r-crashapi.html#geometric-operations",
    "href": "examples/06_spatial-operations-r-crashapi.html#geometric-operations",
    "title": "Spatial operations with the sf, tigris, and crashapi packages",
    "section": "Geometric operations",
    "text": "Geometric operations\nOK. Let’s try something different. Remember those highways? The data covers the entire U.S. but we just need the highways for Maryland.\nThis time, I’ll use the st_filter() function with the default st_intersects() predicate (we will come back to predicate functions at the end):\n\nus_highways <- st_transform(us_highways, st_crs(md_crashes))\n\nmd_highways <- st_filter(us_highways, md_counties, .predicate = st_intersects)\n\nAs usual, a map is helpful to make sure this worked the way I expected:\n\nShow the code | Map Maryland Highwaysmd_borders +\n  tm_shape(md_highways) +\n  tm_lines(col = \"darkgreen\")\n\n\n\n\nNow that we have highways, could we use st_filter() to get highway crashes? Give it a try:\n\nst_filter(md_crashes, md_highways)\n## # A tibble: 0 × 16\n## # … with 16 variables: city <chr>, cityname <chr>, county <chr>,\n## #   countyname <chr>, case_year <int>, fatals <int>, latitude <dbl>,\n## #   longitud <dbl>, state <chr>, statename <chr>, st_case <chr>,\n## #   totalvehicles <int>, tway_id <chr>, tway_id2 <chr>, ve_forms <int>,\n## #   geometry <GEOMETRY [°]>\n\nWell, that didn’t work. The highways are a LINESTRING feature so it isn’t the most effective spatial filter. Using st_buffer() we can add a little space around each highway and try again.\nHow does st_buffer() work? Using the dist (short dor distance) parameter you can add space around a point, linestring, or polygon.\n\n\n\n\n\n\nThis can get a little complicated depending on your coordinate reference system. When using the GEOS library, the units for dist are degrees for geometric coordinate reference systems (e.g. EPSG:4326) or the system’s native units (most often feet or meters) for projected coordinate reference systems. But, when you are using the s2geometry library, dist is passed to the s2::s2_buffer_cells() function where the units are determined by the units used by the radius parameter (which defaults to meters). It can be helpful to use the units::set_units() function that makes it easier to set the buffer in whatever distance you like and then convert to the needed units.\n\n\n\nFor this exercise, let’s convert our data projected CRS so we can use the version of st_buffer() that works with GEOS.\n\nShow the code | Transform data to a projected CRSmd_crashes <- st_transform(md_crashes, 3857)\nmd_counties <- st_transform(md_counties, 3857)\nmsa_crashes <- st_transform(msa_crashes, 3857)\nmsa_counties <- st_transform(msa_counties, 3857)\nmd_highways <- st_transform(md_highways, 3857)\n\n\nNow, I can add a buffer:\n\nmd_highways_quartermi <-\n  st_buffer(\n    md_highways,\n    dist = 402.336 # quarter-mile in m\n  )\n\nmd_highways_halfmi <-\n  st_buffer(\n    md_highways,\n    dist = units::set_units(0.5, \"mi\") %>%\n      units::set_units(\"m\")\n  )\n\nI’ll map the data to show that it works (using Baltimore County to set the map bounding box so it easier to see the difference between the two buffers).\n\nShow the code | Map bufferred Maryland highwaystm_shape(msa_counties, bbox = msa_counties[7, ]) +\n  tm_borders(col = \"gray60\") +\n  tm_shape(md_highways_quartermi) +\n  tm_borders(col = \"blue\") +\n  tm_shape(md_highways_halfmi) +\n  tm_borders(col = \"purple\")\n\n\n\n\n🤔 A half mile buffer seems too large. Can you add a 250 meter buffer to the md_highways?\n\n# md_highways_250m <- st_buffer()\n\nThen, go back and filter crash data using md_highways_250m. How many fatal crashes took place within 250 meters of a highway in Maryland? How many fatalities?\n\n# st_filter()\n\nYou should find 96 crashes and 99 fatalities."
  },
  {
    "objectID": "examples/06_spatial-operations-r-crashapi.html#confirming-geometric-relationships",
    "href": "examples/06_spatial-operations-r-crashapi.html#confirming-geometric-relationships",
    "title": "Spatial operations with the sf, tigris, and crashapi packages",
    "section": "Confirming geometric relationships",
    "text": "Confirming geometric relationships\nThere is also a set of related functions called “predicates” or geometric confirmation functions that take pairs of simple feature geometry sets and return an index list or logical matrix based on the spatial relationship between the features. For example, take a look at st_intersects():\n\nst_intersects(msa_counties, md_counties, sparse = FALSE)\n##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11] [,12]\n## [1,]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n## [2,] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n## [3,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n## [4,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [5,] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n## [6,] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n## [7,] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n##      [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24]\n## [1,] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n## [2,] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE\n## [3,] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n## [4,] FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE\n## [5,] FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE\n## [6,] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE\n## [7,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n\nWhat is this matrix? The function is comparing each element of x (msa_counties) to each element of y (md_counties). We can simplify the function by using st_union() on the second argument:\n\nmd_counties_union <- st_union(md_counties)\n\nst_intersects(msa_counties, md_counties_union, sparse = FALSE)\n##      [,1]\n## [1,] TRUE\n## [2,] TRUE\n## [3,] TRUE\n## [4,] TRUE\n## [5,] TRUE\n## [6,] TRUE\n## [7,] TRUE"
  },
  {
    "objectID": "examples/07_tidy-data-r.html",
    "href": "examples/07_tidy-data-r.html",
    "title": "Tidying data in R",
    "section": "",
    "text": "Let’s start by with the question: what is tidy data?\nTo get data into a tidy format, you’ll need to know how to address these common problems (from Tidy data vignette):\nI’ve also highlighted several tips from the The Quartz guide to bad data whenever they are relevant in this exercise.\nNote: this particular example requires the {esri2sf} package (I recommend using my fork which incorporated the {httr2} package)."
  },
  {
    "objectID": "examples/07_tidy-data-r.html#pivoting-data-frames-between-wide-and-long-formats",
    "href": "examples/07_tidy-data-r.html#pivoting-data-frames-between-wide-and-long-formats",
    "title": "Tidying data in R",
    "section": "Pivoting data frames between wide and long formats",
    "text": "Pivoting data frames between wide and long formats\nWe are going to look at data on total new housing units authorized for construction in Maryland from 2010 to 2021:\n\nnew_units <-\n  readr::read_csv(\n    \"https://opendata.maryland.gov/api/views/c7z9-v9mr/rows.csv\",\n    show_col_types = FALSE\n  )\n\nTake a look at the column names:\n\ncolnames(new_units)\n\n [1] \"Date created\"           \"Year\"                   \"MARYLAND\"              \n [4] \"Allegany County\"        \"Anne Arundel County\"    \"Baltimore City\"        \n [7] \"Baltimore County\"       \"Calvert County\"         \"Caroline County\"       \n[10] \"Carroll County\"         \"Cecil County\"           \"Charles County\"        \n[13] \"Dorchester County\"      \"Frederick County\"       \"Garrett County\"        \n[16] \"Harford County\"         \"Howard County\"          \"Kent County\"           \n[19] \"Montgomery County\"      \"Prince George's County\" \"Queen Anne's County\"   \n[22] \"Somerset County\"        \"St. Mary's County\"      \"Talbot County\"         \n[25] \"Washington County\"      \"Wicomico County\"        \"Worcester County\"      \n\n\n🤔 What variable is being stored in the column names?\nYep. Each column after “year” is a location.\nThe locations aren’t all the same. They include two different scales: one column covering the whole state and the others covering a single county.\nFocusing on the counties, we can “pivot” these columns into a long format where each row has new housing units for just a single year and a single county. We do this with tidyr::pivot_longer() selecting the columns to pivot using the tidyselect::contains() function.\n\nnew_units_long <-\n  tidyr::pivot_longer(\n    data = new_units,\n    cols = contains(c(\"County\", \"city\")),\n    names_to = \"county\",\n    values_to = \"num_units\"\n  ) %>%\n  dplyr::select(-c(`Date created`, MARYLAND))\n\n\n\n\n\n\n\nRemember to avoid ambiguous field names. I’ve included the units of the value in the column name—making it clear that each number indicates the number of new housing units for that year and county.\n\n\n\nIn this example, the long format is helpful for a few reasons. First, this long format makes it easy to plot complex data with ggplot2:\n\nggplot(data = new_units_long) +\n  geom_col() +\n  aes(x = Year, y = num_units, fill = county) +\n  facet_wrap(~ county) +\n  guides(fill = \"none\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nAre these data too granular? Using counties as a grouping mean the data includes wide variation in population, size, and housing market conditions. We could consider combining counties into regions or metro area or classifying as urban or rural in order to make interpretation easier.\n\n\n\nFirst, the new location column makes it easy to join spatial data to the data frame with dplyr::left_join():\n\nmd_counties <- tigris::counties(state = \"MD\", cb = TRUE)\n\nRetrieving data for the year 2020\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nnew_units_counties <-\n  left_join(\n    x = new_units_long,\n    y = md_counties,\n    by = c(\"county\" = \"NAMELSAD\")\n  ) %>%\n  # Convert the dataframe back to sf\n  sf::st_as_sf()\n\nDid this join work the way we expected?\nIf a join didn’t find a match for any of the locations, any new columns will have NAs for the missing values. summary() only shows missing values for numeric values. skimr::skim() does show missing variables but there are a few packages that make these checks easier. {naniar} is specifically designed to work with missing data:\n\nnaniar::gg_miss_var(new_units_counties)\n\n\n\n\nWhoops. More than 10 unmatched rows. Which locations didn’t get a match? We can filter to look for NA values and then use distinct() on the join column.\n\nnew_units_counties %>%\n  filter(is.na(GEOID)) %>%\n  distinct(county)\n\n# A tibble: 1 × 1\n  county        \n  <chr>         \n1 Baltimore City\n\n\nOK. So the only columns that didn’t match are those for Baltimore City. Let’s take a look at the join column from md_counties to see what we can figure out.\n\nunique(new_units_long$county)\n\n [1] \"Allegany County\"        \"Anne Arundel County\"    \"Baltimore County\"      \n [4] \"Calvert County\"         \"Caroline County\"        \"Carroll County\"        \n [7] \"Cecil County\"           \"Charles County\"         \"Dorchester County\"     \n[10] \"Frederick County\"       \"Garrett County\"         \"Harford County\"        \n[13] \"Howard County\"          \"Kent County\"            \"Montgomery County\"     \n[16] \"Prince George's County\" \"Queen Anne's County\"    \"Somerset County\"       \n[19] \"St. Mary's County\"      \"Talbot County\"          \"Washington County\"     \n[22] \"Wicomico County\"        \"Worcester County\"       \"Baltimore City\"        \n\nunique(md_counties$NAMELSAD)\n\n [1] \"Queen Anne's County\"    \"Wicomico County\"        \"Garrett County\"        \n [4] \"Prince George's County\" \"Caroline County\"        \"St. Mary's County\"     \n [7] \"Anne Arundel County\"    \"Calvert County\"         \"Worcester County\"      \n[10] \"Harford County\"         \"Montgomery County\"      \"Talbot County\"         \n[13] \"Dorchester County\"      \"Cecil County\"           \"Carroll County\"        \n[16] \"Washington County\"      \"Frederick County\"       \"Howard County\"         \n[19] \"Kent County\"            \"Baltimore County\"       \"Allegany County\"       \n[22] \"Somerset County\"        \"Charles County\"         \"Baltimore city\"        \n\n\n🤔 Do you see the problem?\nHint: join columns are case sensitive. “Baltimore City” is not the same as “Baltimore city”.\nOne solution is to record the county variable using dplyr::mutate() and dplyr::case_when():\n\nnew_units_long <-\n  new_units_long %>%\n  mutate(\n    county = case_when(\n      county == \"Baltimore City\" ~ \"Baltimore city\",\n      TRUE ~ county\n    )\n  )\n\nnew_units_counties <-\n  left_join(\n    x = new_units_long,\n    y = md_counties,\n    by = c(\"county\" = \"NAMELSAD\")\n  ) %>%\n  # Convert the dataframe back to sf\n  sf::st_as_sf()\n\nThe wide format can also be helpful! For example, if we pivot wider and put the years into columns it makes it easy to compare one year to another with the mutate() or transmute() function:\n\nnew_units_wide <-\n  pivot_wider(\n    # Note: if we didn't drop the geometry we would get an error\n    sf::st_drop_geometry(new_units_counties),\n    id_cols = \"county\",\n    names_from = \"Year\",\n    values_from = \"num_units\"\n  )\n\nnew_units_wide <- new_units_wide %>%\n  mutate(\n    diff_num_units_10_15 = .data[[\"2020\"]] - .data[[\"2015\"]],\n    diff_num_units_10_20 = .data[[\"2020\"]] - .data[[\"2010\"]]\n  )\n\nnew_units_wide %>% \n  mutate(\n    county = stringr::str_remove(county, \"County$\")\n  ) %>% \nggplot() +\n  geom_bar(aes(x = county, weight = diff_num_units_10_20)) +\n  coord_flip() +\n  scale_color_gradient2(low = \"red\", mid = \"white\", high = \"blue\") +\n  theme_minimal() +\n  labs(\n    title = \"Difference in permitted new housing units, 2010 and 2020\",\n    caption = \"Data: Open Baltimore/Maryland Department of Housing and Community Development\"\n  )\n\n\n\n\nOK. Your turn. The open data on permitted multi-family units is also wide—placing the data for each county in adjoining columns.\n\nnew_multifamily_units <-\n  readr::read_csv(\n    \"https://opendata.maryland.gov/api/views/pz3y-chyn/rows.csv\",\n    show_col_types = FALSE\n  )\n\n🤔 Can you convert the data into a long format with pivot_longer()?\n\nnew_multifamily_units <-\n  pivot_longer(\n    new_multifamily_units,\n    cols = contains(c(\"County\", \"City\")),\n    names_to = \"county\",\n    values_to = \"num_units\"\n  )\n\nnew_multifamily_units\n\n# A tibble: 288 × 5\n   `Date created`       Year MARYLAND county              num_units\n   <dttm>              <dbl>    <dbl> <chr>                   <dbl>\n 1 2022-10-19 00:00:00  2010     3442 Allegany County            66\n 2 2022-10-19 00:00:00  2010     3442 Anne Arundel County       847\n 3 2022-10-19 00:00:00  2010     3442 Baltimore County          635\n 4 2022-10-19 00:00:00  2010     3442 Calvert County              2\n 5 2022-10-19 00:00:00  2010     3442 Caroline County             0\n 6 2022-10-19 00:00:00  2010     3442 Carroll County              0\n 7 2022-10-19 00:00:00  2010     3442 Cecil County               68\n 8 2022-10-19 00:00:00  2010     3442 Charles County              0\n 9 2022-10-19 00:00:00  2010     3442 Dorchester County          96\n10 2022-10-19 00:00:00  2010     3442 Frederick County           95\n# … with 278 more rows"
  },
  {
    "objectID": "examples/07_tidy-data-r.html#working-with-long-format-data",
    "href": "examples/07_tidy-data-r.html#working-with-long-format-data",
    "title": "Tidying data in R",
    "section": "Working with long format data",
    "text": "Working with long format data\nNow that we have both datasets in a long format we can put the two together.\n\n\n\nHow should we do that? One option is to use a join based the year and county name.\n\nmd_housing_units_wide <-\n  left_join(\n    new_units_long,\n    new_multifamily_units_long %>%\n      rename(\n        num_multifamily_units = num_units\n      ),\n    by = c(\"Year\", \"county\")\n  )\n\nUsing left_join() places the columns side-by-side in a wide format. What new variables that can we create by working with columns in this format?\n\n# md_housing_units_wide <-\n#   md_housing_units_wide %>%\n#   mutate(\n#\n#   )\n\nThis works for two datasets but you can also stack even more data frames into a long format. Using the .id parameter allows you to use the names assigned to each dataframe as a new column identifying the source of the data:\n\n\n\n\nmd_housing_units_long <-\n  bind_rows(\n    \"total\" = new_units_long,\n    \"multifamily\" = new_multifamily_units_long,\n    \"singlefamily\" = new_singlefamily_units_long,\n    .id = \"type\"\n  )\n\nIf we have year, permit type, and number of units in a “tidy” format, this creates new opportunities for both for visualization and for analysis using {tidyverse} packages.\nTo see how this works for visualization, try filtering the md_housing_units_long to a single county and using geom_col() to make a bar chart showing both the total and multifamily housing units by year for that county? I filled a few details already—you’ll want to set fill = type and position = \"dodge\" (note, position is an additional parameter outside the aesthetic mapping)—but you still need to filter the data and provide the required x and y aesthetics for geom_col().\n\n# md_housing_units_long %>%\n#   filter() %>%\n#   ggplot() +\n#   geom_col(aes(x = , y = , fill = type), position = \"dodge\")\n\nTo see how this works for analysis, consider how you can use group_by() and summarise() to create summary statistics by county and permit type.\n\nnew_multifamily_units %>% \n  group_by(county) %>% \n  summarise(\n    cum_sum_units = sum(num_units),\n    avg_units = mean(num_units),\n    min_units = min(num_units),\n    max_units = max(num_units)\n  )\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `county` is not found.\n\n\nUsing factors for categorical data\n\nmd_counties_xwalk <-\n  tibble::tribble(\n                      ~county,             ~region,                                               ~msa,\n             \"Garrett County\",  \"Western Maryland\",                                                 NA,\n            \"Allegany County\",  \"Western Maryland\",                            \"Cumberland, MD-WV MSA\",\n          \"Washington County\",  \"Western Maryland\",                \"Hagerstown–Martinsburg, MD–WV MSA\",\n           \"Frederick County\",    \"Capital Region\", \"Washington–Arlington–Alexandria, DC–VA–MD–WV MSA\",\n          \"Montgomery County\",    \"Capital Region\", \"Washington–Arlington–Alexandria, DC–VA–MD–WV MSA\",\n     \"Prince George's County\",    \"Capital Region\", \"Washington–Arlington–Alexandria, DC–VA–MD–WV MSA\",\n        \"Anne Arundel County\",  \"Central Maryland\",                    \"Baltimore–Columbia–Towson MSA\",\n             \"Baltimore city\",  \"Central Maryland\",                    \"Baltimore–Columbia–Towson MSA\",\n           \"Baltimore County\",  \"Central Maryland\",                    \"Baltimore–Columbia–Towson MSA\",\n             \"Carroll County\",  \"Central Maryland\",                    \"Baltimore–Columbia–Towson MSA\",\n             \"Harford County\",  \"Central Maryland\",                    \"Baltimore–Columbia–Towson MSA\",\n              \"Howard County\",  \"Central Maryland\",                    \"Baltimore–Columbia–Towson MSA\",\n             \"Calvert County\", \"Southern Maryland\", \"Washington–Arlington–Alexandria, DC–VA–MD–WV MSA\",\n             \"Charles County\", \"Southern Maryland\", \"Washington–Arlington–Alexandria, DC–VA–MD–WV MSA\",\n          \"St. Mary's County\", \"Southern Maryland\",                \"California-Lexington Park, MD MSA\",\n                \"Kent County\",     \"Eastern Shore\",                                                 NA,\n        \"Queen Anne's County\",     \"Eastern Shore\",                    \"Baltimore–Columbia–Towson MSA\",\n              \"Talbot County\",     \"Eastern Shore\",                                   \"Easton, MD μSA\",\n            \"Caroline County\",     \"Eastern Shore\",                                                 NA,\n          \"Dorchester County\",     \"Eastern Shore\",                                    \"Cambridge, MD\",\n            \"Wicomico County\",     \"Eastern Shore\",                             \"Salisbury, MD-DE MSA\",\n            \"Somerset County\",     \"Eastern Shore\",                             \"Salisbury, MD-DE MSA\",\n           \"Worcester County\",     \"Eastern Shore\",                             \"Salisbury, MD-DE MSA\",\n               \"Cecil County\",     \"Eastern Shore\",      \"Philadelphia-Camden-Wilmington, PA-NJ-DE-MD\"\n    )\n\n\nforcats::as_factor()\n\nError in UseMethod(\"as_factor\"): no applicable method for 'as_factor' applied to an object of class \"NULL\""
  },
  {
    "objectID": "examples/07_tidy-data-r.html#tidying-addresses-for-geocoding",
    "href": "examples/07_tidy-data-r.html#tidying-addresses-for-geocoding",
    "title": "Tidying data in R",
    "section": "Tidying addresses for geocoding",
    "text": "Tidying addresses for geocoding\n\nmultifamily_housing <-\n  readr::read_csv(\n    \"https://opendata.maryland.gov/api/views/cadm-spqd/rows.csv\",\n    name_repair = janitor::make_clean_names,\n    show_col_types = FALSE\n  )\n\nmultifamily_housing <-\n  dplyr::bind_cols(\n    multifamily_housing,\n    project_state = \"MD\"\n  )\n\nUsing regular expressions\n\nmultifamily_housing_geo_osm <-\n  tidygeocoder::geocode(\n    multifamily_housing[1:50, ],\n    street = \"project_address\",\n    city = \"project_city\",\n    county = \"project_county\",\n    state = \"project_state\",\n    method = \"osm\",\n    full_results = TRUE\n  )\n\nPassing 50 addresses to the Nominatim single address geocoder\n\n\nQuery completed in: 50.3 seconds\n\n\nDid that work? Well, when I check, I can see I am missing latitude and longitude for more than half of our addresses. 😬\n\nnaniar::gg_miss_var(multifamily_housing_geo_osm, show_pct = TRUE)\n\n\n\n\n\nfilter(multifamily_housing_geo_osm, is.na(long))$project_address\n\n [1] \"6652 Shelly Avenue\"                                        \n [2] \"8219 and 8221 Oakwood Road\"                                \n [3] \"8900 Manchester Road 8902 8904 9000 Manchester Rd\"         \n [4] \"341 E. 20th Street\"                                        \n [5] \"6349 Boston Street\"                                        \n [6] \"7667 N Maple Avenue\"                                       \n [7] \"1507 Ray Road\"                                             \n [8] \"111 Mitchell Street\"                                       \n [9] \"4300 Maple Shade Drive 4400-4412 Grape Vine Way; and 4401-\"\n[10] \"838 W. Fairmount Avenue\"                                   \n[11] \"18 W. Read Street\"                                         \n[12] \"45790  Military Lane\"                                      \n[13] \"1125 N. Patterson Park Avenue\"                             \n[14] \"301 W. Madison Street\"                                     \n[15] \"1500 W.  Fayette Street\"                                   \n[16] \"3202 Reed Street\"                                          \n[17] \"7513 Maple Avenue\"                                         \n[18] \"402 E. Second Street\"                                      \n[19] \"400 N. Athol Road\"                                         \n[20] \"1029 E. Baltimore Street\"                                  \n[21] \"93-108 Mt. Vernon Street\"                                  \n[22] \"30500 Riggin Street\"                                       \n[23] \"5511 Halpine Place\"                                        \n[24] \"4300 Cardwell Avenue\"                                      \n[25] \"1303 Greenmount Avenue\"                                    \n[26] \"4300 Frederick Avenue\"                                     \n\n\n\nmessy_counties <-\n  tibble::tribble(\n                  ~county,\n             \"Balt. City\",\n              \"Baltimore\",\n              \"Balt. Co.\",\n       \"BALTIMORE COUNTY\",\n               \"Bmore MD\",\n              \"Biltimore\",\n               \"Baltimor\",\n              \"Baltimore\",\n         \"Baltimore city\",\n       \"Baltimore County\",\n       \"Baltimore county\",\n    \"Anne Arundel County\",\n     \"Ann Arundel County\",\n                \"Arundel\",\n           \"Anne Arundel\"\n    )\n\nmessy_counties %>% \n  mutate(\n    address = str_replace(address, \"Shelley Road\", \"Shelley Avenue\")\n    county = case_when(\n      str_detect(county, \"Arundel|Arundle\") ~ \"Anne Arundel County\",\n      str_detect(county, \"city|City\") ~ \"Baltimore city\",\n      str_detect(county, \"Co.|County\") ~ \"Baltimore County\",\n      TRUE ~  \"Baltimore city\"\n      )\n  )\n\nError: <text>:24:5: unexpected symbol\n23:     address = str_replace(address, \"Shelley Road\", \"Shelley Avenue\")\n24:     county\n        ^\n\n\n\nmultifamily_housing_geo_census <-\n  tidygeocoder::geocode(\n    multifamily_housing[1:50, ],\n    street = \"project_address\",\n    city = \"project_city\",\n    state = \"project_state\",\n    method = \"census\",\n    full_results = TRUE\n  )\n\nPassing 50 addresses to the US Census batch geocoder\n\n\nQuery completed in: 1.6 seconds\n\n\n🤔 How did that go? What approach would you use to check from missing data?"
  },
  {
    "objectID": "examples/07_tidy-data-r.html#separating-variables-with-separate",
    "href": "examples/07_tidy-data-r.html#separating-variables-with-separate",
    "title": "Tidying data in R",
    "section": "Separating variables with separate()\n",
    "text": "Separating variables with separate()\n\n\nneighborhoods <- sf::read_sf(\n  dsn = here::here(\"files/data/neighborhoods.geojson\")\n)\n\npermits <-\n  esri2sf::esri2sf(\n  url = \"https://egisdata.baltimorecity.gov/egis/rest/services/Housing/DHCD_Open_Baltimore_Datasets/FeatureServer/3\",\n  bbox = neighborhoods[63,]\n)\n\n✔ Downloading \"Building Permits\" from\n  <]8;;https://egisdata.baltimorecity.gov/egis/rest/services/Housing/DHCD_Open_Baltimore_Datasets/FeatureServer/3https://egisdata.baltimorecity.gov/egis/rest/services/Housing/DHCD_Open_Baltimore_Datasets/FeatureServer/3]8;;>\nLayer type: \"Feature Layer\"\n\nGeometry type: \"esriGeometryPoint\"\n\nService CRS: \"EPSG:2248\"\n\nOutput CRS: \"EPSG:4326\"\n\n\n\nmultifamily_housing_sf <-\n  sf::st_as_sf(\n    multifamily_housing_geo,\n    coords = c(\"long\", \"lat\"),\n    crs = 4326,\n    remove = FALSE,\n    na.fail = FALSE\n  )\n\nError in sf::st_as_sf(multifamily_housing_geo, coords = c(\"long\", \"lat\"), : object 'multifamily_housing_geo' not found"
  },
  {
    "objectID": "examples/index.html",
    "href": "examples/index.html",
    "title": "Examples",
    "section": "",
    "text": "Read and filter spatial data with R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRead tabular data and convert to spatial data with R\n\n\nLearn to read tabular data with R, convert to spatial data with the sf package, and clean data with the dplyr and tidy packages.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial operations with the sf, tigris, and crashapi packages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidying data in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Building Spatial Datasets",
    "section": "",
    "text": "Building Spatial Datasets\n        \n        \n            Find, access, modify, and create spatial data in a wide range of formats to support research and practice.\n        \n        \n            GES 600 | Fall 2022Geography & Environmental SystemsUniversity of Maryland Baltimore County\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\nInstructor\n\n Eli Pousson\n eli.pousson@umbc.edu\n elipousson\n\n\n\nCourse details\n\n Wednesdays\n August 31 – December 7, 2022\n 6:00–8:30 PM\n Sondheim Hall 001 (Cartography Lab)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 21 and 28: Recap & Next Steps\n\n\n\n\n\n\n\nrecap\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2022\n\n\nEli Pousson\n\n\n\n\n\n\n\n\nSeptember 14: Recap & Next Steps\n\n\n\n\n\n\n\nrecap\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\nEli Pousson\n\n\n\n\n\n\n\n\nSeptember 7: Recap & Next Steps\n\n\n\n\n\n\n\nrecap\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2022\n\n\nEli Pousson\n\n\n\n\n\n\n\n\nAugust 31: Recap & Next Steps\n\n\n\n\n\n\n\nrecap\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2022\n\n\nEli Pousson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Materials",
    "section": "",
    "text": "Accessing open data with R\n\n\n\nslides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nCreating feature data with R and QGIS\n\n\n\nslides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nData sources\n\n\n\nresource\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do common tasks for the class with RStudio\n\n\n\nresource\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nRead and filter spatial data with R\n\n\n\nexample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nRead tabular data and convert to spatial data with R\n\n\n\nexample\n\n\n\nLearn to read tabular data with R, convert to spatial data with the sf package, and clean data with the dplyr and tidy packages.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nSpatial operations with the sf, tigris, and crashapi packages\n\n\n\nexample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nTidying data in R\n\n\n\nexample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nUsing QGIS\n\n\n\nresource\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nUsing R and RStudio Desktop\n\n\n\nresource\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nWorking with attributes in R (and QGIS)\n\n\n\nslides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nWorking with attributes in R (and QGIS)\n\n\n\nslides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-08-31_recap/index.html",
    "href": "posts/2022-08-31_recap/index.html",
    "title": "August 31: Recap & Next Steps",
    "section": "",
    "text": "Thanks to everyone for an exciting first class yesterday evening. I’ll try to send out a quick session recap and a review of next steps after each session so, if I miss anything, please let me know or just flag it in the Discord!\n\nSession 1 recap\n\nYou are all coming in with interesting and varied experiences with a mix of backgrounds in STATA, SAS, GIS desktop applications, Python, and R with interests including ecology, transit, reproducible data analysis, and disaster response. Please feel free to use your weekly reading responses to connect these interests to our readings and to find related R packages or QGIS methods you can use to expand your work in this area.\nWe reviewed the syllabus on the course website including the assignment structure and reading schedule. More details on the upcoming assignments and final project will be coming soon!\nI expect we will typically meet from 6:00 pm to 8:00 pm. A heads up if you need to miss a session is appreciated but we’ll do our best to catch you up if you are unable to attend.\nI shared an invitation to the course Discord channel and encouraged everyone to use Discord to share questions during the term. I will use Discord but make sure any essential communications are sent via Blackboard or email. It also turned out to be a helpful work around with the out-of-order projector this evening!\nWe confirmed that everyone has R, RStudio, and QGIS installed (or will be able to install soon). Let me know if you run into trouble with installing packages as we continue forward in the class.\nNote: In your RStudio settings, everyone should update the setting for “Save workspace to .RData on exit” to “Never” — a key step for ensuring reproducibility (endorsed by Hadley Wickham among others). This setting changes covered in the R Basics and workflows material included in the Setup section of our course schedule.\nWe concluded with a short demonstration of how to use the sf package to read data from a shapefile I downloaded from Natural Earth. We also looked at a few handy features and functions including:\nUsing the ? help operator to get to function documentation (see Getting Help with R for more info)\nUsing RMarkdown (or Quarto documents) to combine text with executable code (see this accessible guide from RStudio or Ch. 2 Basics from RMarkdown: The Definitive Guide for more info on RMarkdown).\nUsing functions to take a quick look at your data including View() and plot() (both base R functions), glimpse() (from the dplyr package), and mapview() (from the mapview package)\nI shared a copy of the sf cheatsheet (available here as a PDF) and you can find cheat sheets for the RStudio IDE, dplyr, and many other useful packages on the RStudio website or on GitHub.\n\n\n\nSession 1 next steps\n\nWe’re going to try out GitHub classroom for our GitHub fundamentals activity which you can access via this invitation link.\nIf you sent me your GitHub account name, I have added you to our GitHub organization and sent you an invite to a private repository matching your name. I’ll add a log folder to each repository and a template you can use for the written response for our first week assignment on finding spatial data.\n\n\n\nWhat to expect next week for Session 2\n\nNext week, we will kick off with a discussion covering both Chapter 1 and 2 from All Data Are Local.(Loukissas 2019) If you’re someone who enjoys taking in ideas via video or audio, you may enjoy this recorded one-hour talk by Loukissas (transcript here) where he summarizes the main ideas behind the book.\nWe will then continue our review of how the sf package works with spatial data (here is the example we started going through) and then review the same process of reading data with QGIS.\nAssignment 1 (setting up your GitHub account and completing the fundamentals training) and assignment 2 (finding spatial data) are both due next week.\n\n\n\n\n\n\nReferences\n\nLoukissas, Yanni Alexander. 2019. All Data Are Local: Thinking Critically in a Data-Driven Society. https://doi.org/10.7551/mitpress/11543.001.0001."
  },
  {
    "objectID": "posts/2022-09-07_recap/index.html",
    "href": "posts/2022-09-07_recap/index.html",
    "title": "September 7: Recap & Next Steps",
    "section": "",
    "text": "Thanks for your flexibility as we work out a balanced approach to the course this fall. You can see that I’ve added these recap and next steps updates to the course website so they are easy to find alongside the other course materials."
  },
  {
    "objectID": "posts/2022-09-07_recap/index.html#what-did-we-do-for-session-2",
    "href": "posts/2022-09-07_recap/index.html#what-did-we-do-for-session-2",
    "title": "September 7: Recap & Next Steps",
    "section": "What did we do for Session 2",
    "text": "What did we do for Session 2\n\nSchedule changes\nWe confirmed class schedule changes for October 5 and November 23. I’ll send out calendar invitations for the following sessions:\n\nMonday, October 3, 6:30 pm to 7:30 pm (participation is recommended but the session will be recorded for anyone who can’t attend)\nMonday, November 21, 6:00 pm to 7:30 pm (participation is optional - this session will be formatted as a drop-in discussion on final project proposals)\n\nWe also pushed our next few assignment due dates forward by one week:\n\nFind spatial data is due on September 14\nRead spatial data and make a map with R and with QGIS are both due September 28\nCompare spatial and geometry operations in QGIS and R is due on October 5\n\nI am keeping the other assignment due dates in place for now but I may make other adjustments if needed. The course schedule is updated to note these schedule changes.\n\n\nTopics and discussion\nWe reviewed the process of using GitHub Desktop and the basics of using GitHub to submit assignments. I’m still putting together a more detailed “how to” for this process and will add a link to that resource to this recap and the assignment pages when it is published.\nWe started with a quick review of last week’s introduction to R and RStudio including:\n\nInstalling packages from CRAN using install.packages() or from GitHub or other sources using pak::pkg_install()\nUsing RStudio project or .Rproj files (see Workflow: projects in Wickham and Grolemund (2022) for more details)\nCloning the course website repository to access the example Quarto document or .qmd files and related spatial data\n\nWe continued to review the example on the basics of working with simple feature data in R this week including:\n\nHow data frames include rows and columns with basic data types including numeric (including double and integer values) and character vectors\nUsing logical operators with the filter function (see Filter rows with filter() from Wickham and Grolemund (2022))\nHow sf objects use a geometry column (usually named geom or geometry) to store spatial data (and how to use the sf::st_geometry() function to access this column)\nHow spatial data is organized using an open data standard (officially named simple feature access) around points, lines, and polygons\nSummarizing data using the summary(), skimr::skim(), and mapview::mapview() functions\n\nI got a little off-topic and explained how using a “nerd” font offers a better display of logical operators. I recommended Fira Code but Hasklig is another popular alternative. See RStudio documentation on Appearance and Themes for more on customizing your Editor Font option.\nIn the second part of our class, we discussed the case study on the Arnold Arboretum from Loukissas (2019) and reflected on the parallels between Loukissas’ place-based approach to the analysis based on Patricia Hill Collins’ concept of the matrix of domination from D’Ignazio and Klein (2021).\nD’Ignazio and Klein also highlighted a few examples of counterdata projects that included:\n\nIndependent data projects like the Los feminicidios en México by María Salguero\nData journalism projects like How We Collected Nearly 5,000 Stories of Maternal Harm from ProPublica\nCollaborations between academic and community partners like Detroit Geographic Expedition and Institute\n\nIf you have your own examples of “counterdata”, please feel free to share them in the course Discord!\n\n\nNext steps\nEveryone has set up a GitHub user account which counts as full credit for the first assignment.\n\nSubmit the Finding data assignment\nThanks to everyone who already completed the finding data assignment. This assignment did not include any options for bonus points so if you submitted the assignment you should get full credit for completion on the second assignment. I’ll share feedback via GitHub but post resources related to your topics of interest to the Discord so others can also find them.\nIf you have not already completed the assignment, please use the 01_find-data_template.md file in the assignments folder of your personal course repository. Keep the headings in place but you can edit everything else in the document. Rename the file to 01_find-data.md and commit your changes when you are done with the assignment.\n\n\nSubmit a weekly log entry\nSince we extended the deadline for the weekly log, you can write the reflection based on any of the readings in the course so far. You are also welcome to incorporate outside materials into your reflection - the main goal of this assignment is to give you an opportunity to write about what you’re thinking about and learning during the class. Each log should also include a question. If you’re able to submit your log before our class session, we can use your questions to help guide our in-person work.\nMake sure to copy the template file 2022-MM-DD_log.md and save a new copy replacing MM with the two-digit month and DD with the two-digit date of the class session."
  },
  {
    "objectID": "posts/2022-09-07_recap/index.html#what-to-expect-for-session-3",
    "href": "posts/2022-09-07_recap/index.html#what-to-expect-for-session-3",
    "title": "September 7: Recap & Next Steps",
    "section": "What to expect for Session 3",
    "text": "What to expect for Session 3\nNext week, we’re going to continue our introduction to spatial data with R by covering:\n\nConverting tabular data into spatial data (covered in this example)\nSummarizing or aggregating attribute data\nBasics of visualizing attribute data with maps and plots\n\nWe will also spend some time on an introduction to QGIS with a focus on learning equivalent approaches for the activities we’ve covered in R so far.\nThis topic is covered in Learn QGIS by Andrew Cutts and Anita Graser but you are welcome to use alternate resources instead.\nThe QGIS User Guide covers similar material and there are a variety of helpful tutorial videos on YouTube including:\n\nQGIS Uncovered by Steven Bernard (a design editor with the Financial Times): These videos are a few years old but they are short and lessons 4, 9, and 10 cover the information you need for the QGIS map making assignment in an accessible format.\nAn Introductory QGIS Workshop for Beginners from Michele Tobias (UC Davis DataLab): This recorded workshop is over 3 hours long but you may find it helpful to check out the section on mapping vector data (starting at 1:24:00) and the section on working with the map layout tools (starting at 2:30:00). There is also a corresponding GitHub repository with more resources.\n\nI’m still in the process of adjusting the schedule to make sure we don’t have any more overloaded sessions so I’m holding off on the poll we discussed in class for now. As always, any feedback on your preferred approach to engaging with this material or questions about anything we have covered so far is welcome."
  },
  {
    "objectID": "posts/2022-09-14_recap/index.html",
    "href": "posts/2022-09-14_recap/index.html",
    "title": "September 14: Recap & Next Steps",
    "section": "",
    "text": "We took some a good bit of time at the start of class this past week to go over questions from the daily logs. Feel free to share your questions to the Discord during the week as well as including them in your log. Here are a few of the questions we tackled this week:\n\nHow do I submit an assignment using GitHub? I’ve added an assignment page for the weekly logs that explains the process for committing changes to your course repository using the Git tab in RStudio and links to step-by-step instructions on how to do the same using GitHub Studio.\nWhat is the difference between sf, sfc, sfg? In short, a sf (simple feature) object is a data frame with both geometry and attributes, a sfc (simple feature column) object is a list that is used as the geometry column for an sf object, and a sfg (simple feature geometry) object is a single point, polygon, or other feature (representing the geometry of a single feature when it is part of a complete sf object). The introductory vignette for the {sf} package includes a helpful explanation of the difference between sf, sfc, and sfg objects.\nHow do you create inline code within a RMarkdown document? And what is online code useful for? You can use backticks around text starting with a lower-case “r” to create inline code within a R Markdown or Quarto document. This approach is handy for integrating variables into the text of a rendered document. Check out The R Markdown Cookbook for more inline code and code chunks.\n\n\n\n\n\n\nWe spent the bulk of the session walking through the process of using base R and the dplyr package. The slides are available on the course website. You can also find them via the new Materials section I set up that includes slides, annotated examples, and resource pages. If you have any suggestions for changes or additions to the course material, please feel free to open an issue on the GitHub repository for the course site.\n\n\n\nWe briefly discussed the early history of QGIS (drawn from this brief interview with the founding QGIS developer, Gary Sherman) and jumped into a quick demonstration of the basic features including:\n\nImporting data by dragging local files on the Map pane\nImporting data using the Layer > Add Layer > Add Vector Layer and Add Delimited Text Layer options\nUsing the data browser pane to add local files to the Layers pane and reordering layers within the Layers pane\nInstalling the QuickMapServices plugin and using it to add Google Maps as a basemap.\nUsing the Layer > Save As menu option or Export > Save Features As… option from the right-click context menu to export a layer as a GeoPackage file\n\nWe also briefly looked at the the Natural Earth Quick Start Kit (available via the Downloads page) which offers a good demonstration of the varied layer styling and symbology options within QGiS.\n\n\n\n\n\nIf you’re interested in data visualization beyond mapping, I recommend Data Visualization: A practical introduction by Kieran Healy. The book has an accompanying support package packed full of examples.\nDuring the break we talked about {exiftoolr}: a package for accessing ExifTool (a Perl library for accessing EXIF metadata) including the coordinates attached to pictures you take with your phone. I recommended the read_sf_exif() function from my own {sfext} package (that extends the sf package with a variety of additional features) for working with this data.\nDuring the demonstration of mapping with {ggplot2} we talked briefly about color-blind safe palettes as one advantage in using a preset scale like the scale_fill_brewer() or scale_fill_distiller(). The palettes for those functions are both from ColorBrewer: a web tool for guidance in choosing choropleth map color schemes, based on the research of Dr. Cynthia Brewer. In a follow-up chat on Discord, I recommended the {cols4all} R package and noted that QGIS also has a feature allowing you to preview what a map looks like for a color-blind viewer.\n\n\n\n\nThe only assignment due this week is your weekly log. We’re taking a week off from All Data Are Local so you’d also be welcome to use your weekly log to reflect on any initial ideas for how you may be able to apply the approaches we are learning to your own research or practice.\nThe mapping with R and mapping with QGIS assignments are both due the following Wednesday, September 28. If there is interest, I can offer an “office hours” session late next week or early the week following to help trouble-shoot any challenges you may encounter."
  },
  {
    "objectID": "posts/2022-09-14_recap/index.html#what-to-expect-for-session-4",
    "href": "posts/2022-09-14_recap/index.html#what-to-expect-for-session-4",
    "title": "September 14: Recap & Next Steps",
    "section": "What to expect for Session 4",
    "text": "What to expect for Session 4\nNext week, we’re going to start with QGIS so I can demonstrate the basic features for working with attributes, explain how QGIS expressions work, and demonstrate the print layout tools you’ll need to use for the QGIS assignment.\nWe’ll then switch back to R and show off how to use the sf package for:\n\ngeometry operations for buffering or simplifying features\ngeometric operations like union, intersection, and difference\nand related functions including spatial joins and filters\n\nIn addition to the assigned chapter from Lovelace, Nowosad, and Muenchow (2022), some of these topics are also demonstrated in the {sf} vignette on manipulating simple feature geometries."
  },
  {
    "objectID": "posts/2022-09-28_recap/index.html",
    "href": "posts/2022-09-28_recap/index.html",
    "title": "September 21 and 28: Recap & Next Steps",
    "section": "",
    "text": "This is a two-week recap covering Session 4 and 5 along with the revisions to the course we discussed the last couple weeks. Looking forward to seeing you all online soon!"
  },
  {
    "objectID": "posts/2022-09-28_recap/index.html#what-did-we-do-for-session-4",
    "href": "posts/2022-09-28_recap/index.html#what-did-we-do-for-session-4",
    "title": "September 21 and 28: Recap & Next Steps",
    "section": "What did we do for Session 4",
    "text": "What did we do for Session 4\nIntroduction to filtering data with QGIS\nWe spent most of our time on September 21 reviewing how to work with QGIS including:\n\nHow to load data from a GeoPackage file or a CSV file with coordinates (also known as a delimited text tile)\nHow to filter data using the point-and-click GUI and using basic QGIS expressions\nHow to export filtered data to a new layer and a new file\n\nI demonstrated some of the most common ways mistakes (mostly inadvertently) so make sure to remember:\n\nWhen selecting features in the Map view, you must have the layer of interest selected in the Layer panel\nWhen building QGIS expressions, character literals must be wrapped in single quotes; double-quotes are reserved for field references\n\nWe primarily focused on how to use the “select by expression” interface but you can find options to use expressions all throughout QGIS including:\n\nCustom labels (e.g. concatenating multiple fields with the || operator)\nData-driven layer feature styles\nField calculator (for changing the value of an existing or new field)\nUsing variables in the Layout builder (reference variables using @ and eenclose expressions in [% and %] to evaluate)\n\nFor more information, you can review the documentation on QGIS expressions or explore a list of available functions, operators and variables. Many functions in QGIS expressions are also based on the conventions of SQL (short for Structured Query Language) and PostGIS so other relevant resources include the PostGIS reference documentation.\nAfter the QGIS demonstration, we had a constructive discussion about how the course is going so far and came up with a loose plan to extend the first section of the course so we can spend more time during class with hands-on activities. I also agreed to rework the remaining assignments to give everyone more opportunities to practice the skills described in Lovelace, Nowosad, and Muenchow (2022).\nWe closed the session with me attempting to demonstrate a workflow for summarizing data using my Baltimore Tax Sale data as an example. The code for that analysis turned out to be less immediately reproducible than I remembered so we’ll return to this topic with a more straightforward in a future class!"
  },
  {
    "objectID": "posts/2022-09-28_recap/index.html#what-did-we-do-for-session-5",
    "href": "posts/2022-09-28_recap/index.html#what-did-we-do-for-session-5",
    "title": "September 21 and 28: Recap & Next Steps",
    "section": "What did we do for Session 5",
    "text": "What did we do for Session 5\nWe dedicated most of last week’s session to a presentation and discussion with Reina Chano Murray, Geospatial Data Curator and Applications Administrator, Johns Hopkins Sheridan Libraries. Reina graciously shared her slides and a collection of related resources in a GitHub repository. Here Reina’s top tips for documenting your data:\n\nREADMEs are your best friend\nDocument your data along the way - saves you time at the end\nUse descriptive file names\nIf you’re using geospatial desktop software or web GIS, create your metadata in the platform/software you start in\n\nQuestions\nWe also caught up on a few questions from the last couple weeks of log entries:\nIs ggplot typically used more for exploratory work or is it also a good way to make “publication-worthy” maps and visualizations?\nIt is definitely a good way to make “publication-worthy” maps and plots! There are many ggplot2 extension packages. Among the 100+ packages are {ggpubr} (specifically designed to create publication ready plots) and {hrbrthemes} (a popular collection of typographic-centric themes). I also shared a simple example showing how a recent map from the Baltimore Banner could be effectively replicated by using system fonts and a carefully tweaked theme. Check out this post on Setting up and debugging custom fonts by June Choe for more details on using system fonts.\nIs an inner join the same as a right join?\nNope. dplyr::inner_join() and dplyr::right_join() are both examples of what Wickham and Grolemund (2022) describe as mutating joins. These functions typically have two parameters (x on the left and y on the right). A left join keeps all observations in x, a right join keeps all observations in y, and an inner join keeps only those observations found in both x and y.\nHow can you “merge” two different data sets when it doesn’t make sense to use dplyr::left_join()?\nYou can combine sf data frames by using dplyr::bind_rows() to stack one data frame on top of another. You can also use the base R rbind() function but only if the two dataframes have identical columns. dplyr::bind_rows() is more flexible and just adds new columns for any attribute that appears in the second data frame but not the first."
  },
  {
    "objectID": "posts/2022-09-28_recap/index.html#what-to-expect-for-session-6",
    "href": "posts/2022-09-28_recap/index.html#what-to-expect-for-session-6",
    "title": "September 21 and 28: Recap & Next Steps",
    "section": "What to expect for Session 6",
    "text": "What to expect for Session 6\nDue to the schedule conflict with Yom Kippur, our next course session is this Monday evening (tomorrow!). We’re diving back into R to review how to use sf for spatial operations (functions dealing with spatial relationships between features) and geometric operations (functions that modify the geometry of features) and, hopefully, start talking about how you can use these functions as part of an exploratory analysis."
  },
  {
    "objectID": "posts/2022-09-28_recap/index.html#updates-to-the-course-schedule",
    "href": "posts/2022-09-28_recap/index.html#updates-to-the-course-schedule",
    "title": "September 21 and 28: Recap & Next Steps",
    "section": "Updates to the course schedule",
    "text": "Updates to the course schedule\nRight now, you can compare the schedule from the start of the semester with this updated schedule. Once we finalize these changes, I’ll replace the schedule with the updated version. Here are the highlights of the changes:\n\nI updated the titles and descriptions for the last five weeks to more accurately reflect the materials we have been able to cover so far\nI added two new sessions to the first half of the course: Tidying and summarizing data in R and Summarizing and analyzing data with R (extending this first half by two weeks)\nI added a new session on Editing and creating geometry with R and QGIS to the start of the second half of the course and reworked the descriptions for Getting data from public web services and Working on collaborative data projects\nI temporarily removed the assignment due dates while I rework the remaining assignments to function more effectively as exercises.\n\nWe’ll briefly review these changes during our October 3 session and I’ll share an updated activity schedule and more details on a final project ASAP."
  },
  {
    "objectID": "resources/data.html",
    "href": "resources/data.html",
    "title": "Data sources",
    "section": "",
    "text": "Spatial data is available in a wide range of formats at the local, national, and international levels. Data is often published through larger data portals where you can find links to access data through a web service or options to download part of all of the data."
  },
  {
    "objectID": "resources/data.html#local-data-sources-for-baltimore-and-maryland",
    "href": "resources/data.html#local-data-sources-for-baltimore-and-maryland",
    "title": "Data sources",
    "section": "Local data sources for Baltimore and Maryland 🦀",
    "text": "Local data sources for Baltimore and Maryland 🦀\n\nBaltimore City Open Data Hub\nBaltimore County GIS Open Data Portal\nBMC Regional GIS Data Center - Baltimore Metropolitan Council\nChesapeake Bay Open Data Portal\nMaryland’s GIS Data Catalog\nMaryland Open Data\nVital Signs Open Data Portal\nMaryland Food Systems Open Data Portal"
  },
  {
    "objectID": "resources/data.html#national-data-sources-for-the-u.s.",
    "href": "resources/data.html#national-data-sources-for-the-u.s.",
    "title": "Data sources",
    "section": "National data sources for the U.S. 🇺🇸",
    "text": "National data sources for the U.S. 🇺🇸\n\nU.S. Department of Housing and Urban Development Geospatial Data Storefront\nGeospatial at BTS (Bureau of Transportation Statistics), U.S. Department of Transportation\nBTS Data Inventory (Bureau of Transportation Statistics), U.S. Department of Transportation\nEDGE Geodata (Education Demographic and Geographic Estimates), U.S. Department of Education, National Center for Education Statistics\nU.S. Geological Survey Science Data Catalog\nGeoPlatform.gov\nNational Ecological Observatory Network (NEON) Data Portal\nOpen Data Network (Socrata/Tyler Technologies)\nUrban Institute Data Catalog"
  },
  {
    "objectID": "resources/data.html#global-data-sources",
    "href": "resources/data.html#global-data-sources",
    "title": "Data sources",
    "section": "Global data sources 🌏",
    "text": "Global data sources 🌏\n\nNatural Earth\nOpenStreetMap (see the OSM Wiki for more on how data is organized on OSM)\nArcGIS Living Atlas of the World\nNASA Open Data Portal\nNOAA Data Discovery Portal\nWorld Bank Open Data\nHumanitarian Data Exchange\nData Portals (Open Knowledge Foundation)"
  },
  {
    "objectID": "resources/data.html#r-packages-for-data-access",
    "href": "resources/data.html#r-packages-for-data-access",
    "title": "Data sources",
    "section": "R packages for data access",
    "text": "R packages for data access\nGeneral data access packages include:\n\n{esri2sf} 📦 to access ArcGIS FeatureServer data\n{RSocrata} to access Socrata data portals\n{ckanr} to access CKAN data portals\n\nThere are many different data access R packages for interacting with APIs and web services. A few broadly useful recommendations include:\n\n{rnaturalearth}: Get Natural Earth data.\n{tigris}: Download and use Census TIGER/Line shapefiles.\n{tidycensus}: Get US Census boundary and attribute data.\n{geodata}: Get climate, elevation, soil, crop, species occurrence, and administrative boundary data.\n{dataRetrieval}: Get USGS or EPA water quality sample data, streamflow data, and metadata directly from web services.\n{osmdata}: Get OpenStreetMap data.\n\nI also maintain a few R packages for open data access in Baltimore and Maryland that students working on local projects may find helpful:\n\n{mapbaltimore}\n{mapmaryland}"
  },
  {
    "objectID": "resources/how-to-rstudio.html",
    "href": "resources/how-to-rstudio.html",
    "title": "How to do common tasks for the class with RStudio",
    "section": "",
    "text": "Go to File > New Project…\n\n\nFile menu options\n\n\nUsing the “New Project Wizard” select “New Directory” and then “Quarto Project” (or another type if I have recommended a different initial setup).\n\n\nNew Project Wizard in RStudio\n\n\nFill in the directory name with a short descriptive project name, e.g. “explorebaltimorebirds”.\n\n\n\n\n\n\nMake sure to avoid using any spaces in your project name. Your project name is also going to be the name used for your project repository.\n\n\n\n\n\nCreate Quarto Project in New Project Wizard\n\n\nYour new project should open in the RStudio IDE.\n\nInstall the usethis package then restart R. If you already have the use this package installed you can skip this step.\n\ninstall.packages(\"usethis\")\n\nLoad the usethis package.\n\nlibrary(usethis)\n\nUse the git_sitrep() function to check that you have GitHub set up correctly.\n\ngit_sitrep()\n\n\n\n\n\n\n\nIf you see any error messages, review the instructions from Happy Git and GitHub for the useR on getting started with Git or connecting your GitHub account to RStudio.\n\n\n\nMake your first commit to set up the project using the use_git() function.\n\nuse_git(message = \"Create new project\")\n\nIf this works, the results in your console should look something like this:\n\nConfirm in your console that is OK to commit the files.\nAdd your local Git repository to the class GitHub organization by filling in the organization parameter for use_github().\n\nuse_github(organisation = \"bldgspatialdata22\")\n\nIf this works, the results in your console should look something like this:\n\n\n\n\n\n\n\nIf you have multiple GitHub accounts, make sure you use the same GitHub account that belongs to the course organization. Make sure you accepted the invitation to join the course organization. If you are not able to set up a new project, message Eli on Discord to double-check your permissions and send a new invitation to join the course repository if needed.\n\n\n\nYou can now add your project proposal and other files to the repository. Make sure to commit any changes to the repository using the Git pane within RStudio or using the GitHub Desktop application.\n\nAdd a README to your project. Use the use_readme_rmd() function to create creating a README with RMarkdown.\n\nuse_readme_rmd()\n\n\n\n\n\n\n\nUpdating your README\n\n\n\nCreating a README with RMarkdown allows you to include executable code chunks in your README. You should render your README as a Markdown file using the devtools::build_readme() function any time you make changes.\n\n\nSelect a license and add it to your project using any of the license functions from usethis.\n\nuse_mit_license(\"Your Name Here\")\n\nuse_cc0_license()\n\nuse_gpl3_license()\n\nuse_ccby_license()\n\n\n\n\n\n\n\nSelecting a license\n\n\n\nThe MIT license is common license for many R packages and other open-source projects. Open data is often published using a Creative Commons Zero (CC0) or “No Rights Reserved” license. The GNU General Public License (GPL3), Creative Commons Attribution (CC-BY) license (or Creative Commons Attribution-ShareAlike (CC-BY-SA) are other options you could consider.\n\n\n\n\n\n\n\n\nUsing licensed works by others\n\n\n\nIf you are using or adapting code or data from other projects, make sure you credit those projects on your README and clarify the the project repository includes material available under a different license than the license you selected for your own work. In some cases you may be required to use a specific license, such as GPL or CC-BY-SA, if you are making substantial use of an existing work published under that license.\nSee the Licensing chapter from R Packages by Hadley Wickham, the choosealicense.com website from GitHub, or this guide on Considerations for licensors from Creative Commons more detailed guidance on this topic."
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Data sources\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do common tasks for the class with RStudio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nUsing QGIS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nUsing R and RStudio Desktop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resources/qgis.html",
    "href": "resources/qgis.html",
    "title": "Using QGIS",
    "section": "",
    "text": "Read the Desktop User Guide/Manual\nFollow the tutorials in the QGIS Training manual\nLearn GIS basics in A gentle introduction in GIS"
  },
  {
    "objectID": "resources/qgis.html#introductory-resources",
    "href": "resources/qgis.html#introductory-resources",
    "title": "Using QGIS",
    "section": "Introductory resources 🐣",
    "text": "Introductory resources 🐣\n\nExplore QGIS Tutorials and Tips from Ujaval Gandhi\nIntroduction to QGIS from Spatial Thoughts (Ujaval Gandhi)"
  },
  {
    "objectID": "resources/qgis.html#getting-help-with-qgis",
    "href": "resources/qgis.html#getting-help-with-qgis",
    "title": "Using QGIS",
    "section": "Getting help with QGIS 🆘",
    "text": "Getting help with QGIS 🆘\n\nAsk qgis Questions on StackOverflow"
  },
  {
    "objectID": "resources/qgis.html#recommended-qgis-plugins",
    "href": "resources/qgis.html#recommended-qgis-plugins",
    "title": "Using QGIS",
    "section": "Recommended QGIS plugins 🔌",
    "text": "Recommended QGIS plugins 🔌\nPlugins are optional for working with spatial data in QGIS but recommended for ease of use and convenience. Plugins are written and published by individual volunteers, government agencies, and private GIS consulting firms. The following table lists a selection of recommended plugins you may want to install.\n\n\n\nPlugin\nSource\n\n\n\n\nQuickMapServices by NextGIS\nrepo\n\n\nQGIS Resource Sharing by Akbar Gumbira, Håvard Tveite, and Julien Moura\nrepo\n\n\nqgis2web by Tom Chadwin\nrepo\n\n\nProcessing R Provider by North Road and Jan Caha\nrepo\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLearn more about installing QGIS plugins or writing your own QGIS Processing plugin (recommended for beginners) or QGIS Python plugin (recommended for advanced users)."
  },
  {
    "objectID": "resources/qgis.html#additional-resources",
    "href": "resources/qgis.html#additional-resources",
    "title": "Using QGIS",
    "section": "Additional resources",
    "text": "Additional resources\nThere are a variety of helpful tutorial videos on YouTube covering both introductory and moree advanced topics.\n\nIntroductory\n\nQGIS Uncovered by Steven Bernard (a design editor with the Financial Times): These videos are a few years old but they are short and lessons 4, 9, and 10 cover the information you need for the QGIS map making assignment in an accessible format.\nAn Introductory QGIS Workshop for Beginners from Michele Tobias (UC Davis DataLab): This recorded workshop is over 3 hours long but you may find it helpful to check out the section on mapping vector data (starting at 1:24:00) and the section on working with the map layout tools (starting at 2:30:00). There is also a corresponding GitHub repository with more resources.\n\n\n\nAdvanced\n\nQGIS expressions, variables, data defined settings: putting it all together! (1 hr 21 min) by Nyall Dawson from the GIS consulting firm North Road. The demonstration of how to use expressions to set labels, colors, symbology and more shows off some of the similarities between QGIS and ggplot2 (or tmap) and illustrates why a desktop GIS application is still a preferred tool for cartography for many folks (even if R excels at the data manipulation and analysis tasks).\nA Tour of QGIS Processing Toolbox Algorithms (22 min) by Ujaval Gandhi (Spatial Thoughts) offers live demonstrations of over 15 different tools within QGIS including spatial joins, importing geotagged photos, converting points to a path, and more.\nSession recordings from the FOSS4G and FOSSGIS Conferences include a number of sessions on using QGIS for research, analysis, and visualization."
  },
  {
    "objectID": "resources/r.html",
    "href": "resources/r.html",
    "title": "Using R and RStudio Desktop",
    "section": "",
    "text": "R is a statistical programming language that supports a wide range of data analysis, processing, and visualization activities.\nAdditional documentation:\n\nR Project Documentation\nThe R Manuals (Re-styled by RStudio)"
  },
  {
    "objectID": "resources/r.html#about-rstudio",
    "href": "resources/r.html#about-rstudio",
    "title": "Using R and RStudio Desktop",
    "section": "About RStudio 📋",
    "text": "About RStudio 📋\nMost people who work with R use an IDE (or integrated development environment) to write, execute, and test code. For this course, we recommend using the free, open-source RStudio Desktop application.\nAdditional documentation:\n\nRStudio IDE Cheatsheet\n\n\nCustomizing RStudio 🎨\nCustomizing the RStudio IDE\n\n\nAlternatives to RStudio 🧰\nOne popular alternatives include Visual Studio Code (using the R extension and other additions).\nIndividuals who prefer a command line interface may use GNU Emacs (using the ESS plugin) or Vim (using the Nvim-R plugin)."
  },
  {
    "objectID": "resources/r.html#introductory-resources",
    "href": "resources/r.html#introductory-resources",
    "title": "Using R and RStudio Desktop",
    "section": "Introductory resources 🐣",
    "text": "Introductory resources 🐣\nThe RStudio Cheatsheets are one-page printable quick reference sheets created by RStudio staff and contributed by volunteers. You may want to print out the following cheatsheets for a convenient reminder on keyboard shortcuts, useful functions, and typical workflows:\n\nsf\ndplyr\ntidyr\nggplot2\n\nBasics of working with R at the command line and RStudio goodies (from STAT 545 by Jenny Bryan and the STAT 545 TAs)\nR for Data Science (R4DS) by Hadley Wickham and Garrett Grolemund\n“This book will teach you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science.”\nD-Lab’s R Fundamentals Workshop: “a broad overview of the fundamentals of using R, a programming language geared toward statistical analysis and data science.”"
  },
  {
    "objectID": "resources/r.html#getting-help",
    "href": "resources/r.html#getting-help",
    "title": "Using R and RStudio Desktop",
    "section": "Getting help 🆘",
    "text": "Getting help 🆘\n\nRStudio Community"
  },
  {
    "objectID": "resources/r.html#r-packages-for-this-course",
    "href": "resources/r.html#r-packages-for-this-course",
    "title": "Using R and RStudio Desktop",
    "section": "R packages for this course 📦",
    "text": "R packages for this course 📦\nUnlike plugins for QGIS, packages are effectively required to work with spatial data in R. The following table lists the key packages we use most often during this course.\nThis list is not comprehensive. Other packages that are relevant for one specific section of the course will be noted elsewhere in the provided course materials and the packages for spatial data analysis are described in more detail on the resource page for Using R for spatial data."
  },
  {
    "objectID": "schedule.html#how-is-this-schedule-organized",
    "href": "schedule.html#how-is-this-schedule-organized",
    "title": "Schedule (Refined)",
    "section": "How is this schedule organized",
    "text": "How is this schedule organized\nEach session includes information on:\n\nAssignments due: Assignments are due by 11:59 PM on the day they’re listed unless otherwise noted.\nTopics: A listing of the topics, questions, or skills that we expect to focus on during each course session.\nReadings: Readings for this class include both material on how to work with spatial data in R and QGIS and material on broader considerations around working with data and how spatial data is collected, organized, and shared. Most readings are free and online but a few may require you to use the UMBC Library Books and Media Search or AOK Article Search to locate.\nAdditional references: Optional material (including readings and videos) that you are encouraged to use as a reference but you are not required to review before class."
  },
  {
    "objectID": "schedule.html#setup",
    "href": "schedule.html#setup",
    "title": "Schedule (Refined)",
    "section": "Setup",
    "text": "Setup\nTo participate in this class, you need to have access to a laptop with a few applications installed: R, RStudio, and QGIS. You also need to set up a GitHub account. I’ve gathered a few resources with step-by-step instructions on how to install these applications along with some introductory material for any students who has no or limited prior experience with a desktop GIS or a programming language like R.\nTo install QGIS and get an introduction to the interface (if you do not have prior experience with a desktop GIS application), please review:\n\n5. Getting started in QGIS project “QGIS Desktop User Guide/Manual (QGIS 3.22),” August 24, 2022, https://docs.qgis.org/3.22/en/docs/user_manual/index.html.\n\nCh. 1 Where Do I Start? in Andrew Cutts and Anita Graser Learn QGIS: Your Step-by-Step Guide to the Fundamental of QGIS 3.4, Fourth edition. (Birmingham: Packt, 2018). (available via UMBC Library)\n\nTo install R and RStudio and get an introduction to how R works, please review:\n\nCh. 1 Install R and RStudio and Ch. 2 R Basics and Workflows in Jenny Bryan and The STAT 545 TAs STAT 545: Data Wrangling, Exploration, and Analysis with R, 2019, https://stat545.com/.\n\n\nTo set up GitHub account and learn how to connect your account to RStudio, please review:\n\n\nInstallation and Connect Git, GitHub, RStudio from Jenny Bryan, the STAT 545 TAs, and Jim Hester Happy Git and GitHub for the useR, accessed August 28, 2022, https://happygitwithr.com/."
  },
  {
    "objectID": "schedule.html#taking-care-with-data-aug.-31",
    "href": "schedule.html#taking-care-with-data-aug.-31",
    "title": "Schedule (Refined)",
    "section": "1. Taking care with data – Aug. 31",
    "text": "1. Taking care with data – Aug. 31\nTopics\n\nCourse overview\nHow is spatial data structured\nHow can we take a critical approach to working with data\nDemonstration on using GitHub to submit weekly log\nReadings\n\nCh. 1 Local Origins in Yanni Alexander Loukissas All Data Are Local: Thinking Critically in a Data-Driven Society, 2019, doi:10.7551/mitpress/11543.001.0001.\n\nCh. 1 Introduction in Robin Lovelace, Jakub Nowosad, and Jannes Muenchow Geocomputation with R, 2nd (WIP)., 2022, https://geocompr.robinlovelace.net/.\n\nAdditional references\n\nJer Thorp “Turning Data Around,” November 18, 2016, https://medium.com/memo-random/turning-data-around-7acea1f7479c.\nDawn J. Wright, Michael F. Goodchild, and James D. Proctor “Demystifying the Persistent Ambiguity of GIS as ‘Tool’ Versus ‘Science’,” Annals of the Association of American Geographers 87, no. 2 (June 1997): 346–362, doi:10.1111/0004-5608.872057."
  },
  {
    "objectID": "schedule.html#getting-started-using-spatial-data-with-r-sept.-7",
    "href": "schedule.html#getting-started-using-spatial-data-with-r-sept.-7",
    "title": "Schedule (Refined)",
    "section": "2. Getting started using spatial data with R – Sept. 7",
    "text": "2. Getting started using spatial data with R – Sept. 7\nTopics\n\nUsing best practices for file naming and organization\nUsing RStudio to support project management\nReading and writing common spatial data file formats in R\nConverting tabular data into spatial data\nReadings\n\nCh. 2 A Place for Plant Data in Loukissas All Data Are Local.\n\nCatherine D’Ignazio and Lauren Klein “Who Collects the Data? A Tale of Three Maps,” MIT Case Studies in Social and Ethical Responsibilities of Computing no. Winter 2021 (February 5, 2021), doi:10.21428/2c646de5.fc6a97cc.\nCh. 2 Geographic data in R in Lovelace, Nowosad, and Muenchow Geocomputation with R.\n\nJennifer Bryan “How to Name Files” (May 14, 2015), https://speakerdeck.com/jennybc/how-to-name-files.\nAdditional references\n\nTom MacWright “More Than You Ever Wanted to Know about GeoJSON,” March 23, 2015, https://macwright.com/2015/03/23/geojson-second-bite.html.\nJakub Nowosad and Martijn Tennekes Elegant and Informative Maps with tmap, 2021, https://r-tmap.github.io/tmap-book/.\n\nIntroduction to R for Geospatial Data (Data Carpentry)"
  },
  {
    "objectID": "schedule.html#exploring-and-visualizing-attribute-data-with-r-sept.-14",
    "href": "schedule.html#exploring-and-visualizing-attribute-data-with-r-sept.-14",
    "title": "Schedule (Refined)",
    "section": "3. Exploring and visualizing attribute data with R – Sept. 14",
    "text": "3. Exploring and visualizing attribute data with R – Sept. 14\nAssignments due\n\nFind spatial data\nSet up a GitHub account\nTopics\n\nSubsetting or filtering based on attributes\nCreating new attributes based on existing attributes\nJoining tables by attributes with {dplyr}\n\nBasics of visualizing attribute data with {ggplot2}\n\nReadings\n\nCh. 3 Collecting Infrastructures in Loukissas All Data Are Local.\n\nCh. 3 Attribute data operations in Lovelace, Nowosad, and Muenchow Geocomputation with R.\n\nCh. 5 Data transformation in Hadley Wickham and Garrett Grolemund R for Data Science: Import, Tidy, Transform, Visualize, and Model Data, 2nd edition (WIP). (Sebastopol, CA: O’Reilly Media, 2022), https://r4ds.had.co.nz.\n\nAdditional references\n\nCh. 6 Efficient data carpentry in Colin Gillespie and Robin Lovelace Efficient R Programming (O’Reilly Media, 2021), https://csgillespie.github.io/efficientR/.\n\nCh. 9 Making maps with R in Lovelace, Nowosad, and Muenchow Geocomputation with R.\n\n\nMel Moreno and Mathieu Basille “Drawing Beautiful Maps Programmatically with R, sf and ggplot2 — Part 1: Basics,” October 25, 2018, https://r-spatial.org/r/2018/10/25/ggplot2-sf.html. (continue with Part 2: Layers and Part 3: Layouts)\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen Ggplot2: Elegant Graphics for Data Analysis, 3rd (WIP)., Use R! (Springer, 2022), https://ggplot2-book.org/index.html."
  },
  {
    "objectID": "schedule.html#exploring-and-visualizing-attribute-data-with-qgis-sept.-21",
    "href": "schedule.html#exploring-and-visualizing-attribute-data-with-qgis-sept.-21",
    "title": "Schedule (Refined)",
    "section": "4. Exploring and visualizing attribute data with QGIS – Sept. 21",
    "text": "4. Exploring and visualizing attribute data with QGIS – Sept. 21\nTopics\n\nUsing QGIS to support project management\nReading and writing common spatial data file formats with QGIS\nUsing the Style Manager and Print Layout features in QGIS\nAdditional references\n\n\nAn Introductory QGIS Workshop for Beginners from Michele Tobias (UC Davis DataLab)\n\n“An Introductory QGIS Workshop for Beginners” (July 17, 2020), https://www.youtube.com/watch?v=-7v5qfJYWxA. (workshop repository on GitHub)\n\nQGIS Uncovered by Steven Bernard (a design editor with the Financial Times)\n“QGIS Expressions, Variables, Data Defined Settings: Putting It All Together!” (June 12, 2020), https://www.youtube.com/watch?v=h-mpUkwDdOQ.\nCh. 2 Data Creation and Editing in Cutts and Graser Learn QGIS. (available via UMBC Library)"
  },
  {
    "objectID": "schedule.html#documenting-spatial-data-sept.-28",
    "href": "schedule.html#documenting-spatial-data-sept.-28",
    "title": "Schedule (Refined)",
    "section": "5. Documenting spatial data – Sept. 28",
    "text": "5. Documenting spatial data – Sept. 28\n\n\n\n\n\n\nThis session features a guest speaker, Reina Chano Murray, Geospatial Data Curator and Applications Administrator, Johns Hopkins Sheridan Libraries, sharing her perspective on documentation and archiving spatial data.\nThanks to Reina for sharing her slides and related resources in a GitHub repository for this session.\n\n\n\nAssignments due\n\nRead spatial data and make a map with QGIS\nRead spatial data and make a map with R and ggplot2\nTopics\n\nIntroduction to metadata and spatial metadata\nWriting metadata, READMEs, and other documentation\nDocumentation for data and data analysis workflows\nPlanning for reproducible spatial data analysis\nReadings\n\nCh. 7 Show Your Work in Catherine D’Ignazio and Lauren F. Klein Data Feminism (The MIT Press, 2020), doi:10.7551/mitpress/11805.001.0001.\n\nGreg Wilson et al. “Good Enough Practices in Scientific Computing,” PLOS Computational Biology 13, no. 6 (June 22, 2017): e1005510, doi:10.1371/journal.pcbi.1005510.\nAdditional references\n\nAndrew Battista et al. “GeoDatabase Data Curation Primer,” 2019, https://github.com/DataCurationNetwork/data-primers/blob/c6ec438e76fea49eaaf2806bc79ec2c8c12de7f3/Geodatabase%20Data%20Curation%20Primer/Geodata-Primer.md.\nTahu Kukutai and Maggie Walter “Indigenous Data Sovereignty: Implications for Data Journalism,” in The Data Journalism Handbook: Towards a Critical Data Practice, ed. Liliana Bounegru and Jonathan Gray, 2nd ed. (Amsterdam University Press, 2021), 65–73, doi:10.5117/9789462989511_ch09.\nNatalia Mazotte “Working Openly in Data Journalism,” in The Data Journalism Handbook: Towards a Critical Data Practice, ed. Liliana Bounegru and Jonathan Gray, 2nd ed. (Amsterdam University Press, 2021), 138–142, doi:10.5117/9789462989511_ch20."
  },
  {
    "objectID": "schedule.html#geometric-and-spatial-data-operations-with-r-oct.-5",
    "href": "schedule.html#geometric-and-spatial-data-operations-with-r-oct.-5",
    "title": "Schedule (Refined)",
    "section": "6. Geometric and spatial data operations with R – Oct. 5",
    "text": "6. Geometric and spatial data operations with R – Oct. 5\n\n\n\n\n\n\nThis session is the same day as the Jewish holiday of Yom Kippur. Class will take place remotely on October 2. A full recording for this session will be made available to students who are unable to attend the remote session.\n\n\n\nTopics\n\nUsing geometry operations for buffering or simplifying features\nUsing geometric operations like union, intersection, and difference\nUsing related functions including spatial joins and filters\nUsing spatial and geometric operations in exploratory data analysis\nReadings\n\nCh. 4 Spatial data operations in Lovelace, Nowosad, and Muenchow Geocomputation with R.\n\nCh. 5 Geometry operations in Lovelace, Nowosad, and Muenchow Geocomputation with R."
  },
  {
    "objectID": "schedule.html#tidying-data-in-r-oct.-12",
    "href": "schedule.html#tidying-data-in-r-oct.-12",
    "title": "Schedule (Refined)",
    "section": "7. Tidying data in R – Oct. 12",
    "text": "7. Tidying data in R – Oct. 12\nTopics\n\nUsing {stringr} functions to tidy messy address data\nRecoding categorical attribute data with {forcats}\n\nConverting between wide and long data formats with dplyr::pivot_longer() and dplyr::pivot_wider()\n\nWorking with date-time attributes using {lubridate}\n\nReadings\n\nCh. 5 Market, Place, Interface in Loukissas All Data Are Local.\n\nCh. 12 Tidy data in Wickham and Grolemund R for Data Science.\n\nThe Quartz Guide to Bad Data (Quartz, 2022), https://github.com/Quartz/bad-data-guide.\nAdditional references\n\n\nManipulating, analyzing and exporting data with tidyverse from Data Analysis and Visualisation in R for Ecologists (Data Carpentry)\nEmily Riederer “Column Names as Contracts,” September 6, 2020, https://emilyriederer.netlify.app/post/column-name-contracts/.\nYanqi Xu “My Data Cleaning in r Cheat Sheet,” October 24, 2019, https://www.yanqixu.com/My_R_Cheatsheet/data_cleaning_cheatsheet.html.\nKatie Rawson and Trevor Muñoz “Against Cleaning” (July 6, 2016), http://www.curatingmenus.org/articles/against-cleaning/."
  },
  {
    "objectID": "schedule.html#summarizing-and-analyzing-data-with-r-oct.-19",
    "href": "schedule.html#summarizing-and-analyzing-data-with-r-oct.-19",
    "title": "Schedule (Refined)",
    "section": "8. Summarizing and analyzing data with R – Oct. 19",
    "text": "8. Summarizing and analyzing data with R – Oct. 19\nTopics\n\nVisualizing distribution of values of attribute data\nVisualizing spatial distribution of features\nCreating crosstabs with janitor::tabyl() (see vignette on tabyls)\nSummarizing or aggregating attribute data with dplyr::summarize()\n\nCreating summary tables with {gtsummary}\n\nReadings\n\nCh. 7 Exploratory Data Analysis in Wickham and Grolemund R for Data Science.\n\nCh. 6 Models of Local Practice and Ch. 7 Local Ends in Loukissas All Data Are Local.\n\nAdditional references\n\nRoger Beecham “Exploratory Data Analysis: Using Colour and Layout for Comparison,” August 11, 2022, https://www.roger-beecham.com/class/04-class/.\nCh. 11 Scripts, algorithms and functions in Lovelace, Nowosad, and Muenchow Geocomputation with R.\n\nRoger Beecham and Robin Lovelace “A Framework for Inserting Visually Supported Inferences into Geographical Analysis Workflow: Application to Road Safety Research,” Geographical Analysis n/a, no. n/a (n.d.), doi:10.1111/gean.12338."
  },
  {
    "objectID": "schedule.html#editing-and-creating-spatial-data-with-web-based-tools-spreadsheets-qgis-or-r-oct.-26",
    "href": "schedule.html#editing-and-creating-spatial-data-with-web-based-tools-spreadsheets-qgis-or-r-oct.-26",
    "title": "Schedule (Refined)",
    "section": "9. Editing and creating spatial data with web-based tools, spreadsheets, QGIS, or R – Oct. 26",
    "text": "9. Editing and creating spatial data with web-based tools, spreadsheets, QGIS, or R – Oct. 26\nTopics\n\nCreating and editing point features using CSV or Google Sheets\nCreating and editing features using geojson.io or other web-based tools\nCreating and editing features using the {mapedit} package in R\nCreating and editing vector data in QGIS\n\nAdditional references\n\n\nData Organization in Spreadsheets for Social Scientists (Data Carpentry)"
  },
  {
    "objectID": "schedule.html#getting-data-from-public-web-services-nov.-2",
    "href": "schedule.html#getting-data-from-public-web-services-nov.-2",
    "title": "Schedule (Refined)",
    "section": "10. Getting data from public web services – Nov. 2",
    "text": "10. Getting data from public web services – Nov. 2\nTopics\n\nDownloading data from ArcGIS Feature Services with {esri2sf} (I recommend installing my fork)\nDownloading data from Socrata open data portals with {RSocrata}\n\nWorking with data dictionaries and the {labelled} package\nInterpreting administrative data and other public sources\nReadings\n\nCh. 8 Geographic data I/O in Lovelace, Nowosad, and Muenchow Geocomputation with R.\n\nDan Bouk, Kevin Ackermann, and danah boyd A Primer on Powerful Numbers: Selected Readings in the Social Study of Public Data and Official Numbers, March 23, 2022, https://datasociety.net/library/a-primer-on-powerful-numbers-selected-readings-in-the-social-study-of-public-data-and-official-numbers/."
  },
  {
    "objectID": "schedule.html#working-on-collaborative-data-projects-nov.-9",
    "href": "schedule.html#working-on-collaborative-data-projects-nov.-9",
    "title": "Schedule (Refined)",
    "section": "11. Working on collaborative data projects – Nov. 9",
    "text": "11. Working on collaborative data projects – Nov. 9\n\n\n\n\n\n\nThis session features a guest speaker, Elliott Plack, Technical Project Manager, Whitney Bailey Cox & Magnani, LLC, sharing an introduction to OpenStreetMap, his experience as an OSM admin, and perspective on spatial data as a member of the Maryland Council on Open Data.\n\n\n\nReadings\n\nEmily Talen “Bottom-up GIS,” Journal of the American Planning Association 66, no. 3 (2000): 279, doi:10.1080/01944360008976107.\nC. E. Dunn “Participatory GIS - a People’s GIS?” Progress in Human Geography 31 (January 1, 2007): 616–637.\nRob Kitchin and Tracey P. Lauriault “Small Data in the Era of Big Data,” GeoJournal 80, no. 4 (2015): 463–475, doi:10.1007/s10708-014-9601-7.\nElliot Bentley “The Web as Medium for Data Visualization,” in The Data Journalism Handbook: Towards A Critical Data Practice, ed. Liliana Bounegru and Jonathan Gray, 2nd ed. (Amsterdam University Press, 2021), 138–142, doi:10.2307/j.ctv1qr6smr_ch26.\nAssignments due\n\nFinal project proposal"
  },
  {
    "objectID": "schedule.html#contributing-to-openstreetmap-nov.-16",
    "href": "schedule.html#contributing-to-openstreetmap-nov.-16",
    "title": "Schedule (Refined)",
    "section": "12. Contributing to OpenStreetMap – Nov. 16",
    "text": "12. Contributing to OpenStreetMap – Nov. 16\nThis session (tentatively) features a workshop on editing OpenStreetMap led by Elliott Plack and a short exercise on accessing OSM data with R using the {osmdata} package.\nReadings\n\nGeoff Boeing “The Right Tools for the Job: The Case for Spatial Science Tool-Building,” Transactions in GIS 24, no. 5 (October 2020): 1299–1314, doi:10.1111/tgis.12678.\nPeter Mooney and Marco Minghini “A Review of OpenStreetMap Data,” in Mapping and the Citizen Sensor, ed. Peter Mooney et al. (Ubiquity Press, 2017), 37–60, http://www.jstor.org/stable/j.ctv3t5qzc.6."
  },
  {
    "objectID": "schedule.html#project-work-session-nov.-23-scheduled-nov.-21",
    "href": "schedule.html#project-work-session-nov.-23-scheduled-nov.-21",
    "title": "Schedule (Refined)",
    "section": "13. Project work session – Nov. 23 (scheduled Nov. 21)",
    "text": "13. Project work session – Nov. 23 (scheduled Nov. 21)\n\n\n\n\n\n\nThis session is the day before the Thanksgiving holiday and may conflict with travel for class participants. Instead of a session at the scheduled time, I plan to host a remote session at a different date and time (to be determined). This session will focus on sharing updates and support for the final project. The session will not be recorded but notes will be shared with any students who are unable to participate."
  },
  {
    "objectID": "schedule.html#special-topics-in-spatial-data-nov.-30",
    "href": "schedule.html#special-topics-in-spatial-data-nov.-30",
    "title": "Schedule (Refined)",
    "section": "14. Special topics in spatial data – Nov. 30",
    "text": "14. Special topics in spatial data – Nov. 30\n\n\n\n\n\n\nThere are a wide range of special topics that we could cover in this course but we don’t have time to cover them all. This session is a place-holder to dig deeper into a special topic based on the interests of students in the course. Possible topics could include working with spatial network data using sfnetworks, working with time series data using the QGIS temporal controller feature, visualizing elevation data using the rayshader package, or something else."
  },
  {
    "objectID": "schedule.html#final-project-review-dec.-7",
    "href": "schedule.html#final-project-review-dec.-7",
    "title": "Schedule (Refined)",
    "section": "15. Final project review – Dec. 7",
    "text": "15. Final project review – Dec. 7\n\n\n\n\n\n\nThis session will be dedicated to students sharing individual and collaborative work completed for the final project. Additional details on the final project will be shared with students prior to the end of the first section of the course in early October.\n\n\n\nAssignments due\n\nFinal project presentation\nFinal project materials (due Dec. 16)"
  },
  {
    "objectID": "slides/03_attribute-data.html#what-is-attribute-data",
    "href": "slides/03_attribute-data.html#what-is-attribute-data",
    "title": "Working with attributes in R (and QGIS)",
    "section": "What is attribute data?",
    "text": "What is attribute data?\nAttribute data is “non-spatial information associated with geographic (geometry) data.”\nMost often, when we talk about attribute data we are talking about vector attribute data.\n\nLovelace, Nowosad, and Muenchow (2022) notes:\n\n“Unlike the vector data model, the raster data model stores the coordinate of the grid cell indirectly, meaning the distinction between attribute and spatial information is less clear.”"
  },
  {
    "objectID": "slides/03_attribute-data.html#what-can-you-do-with-attribute-data",
    "href": "slides/03_attribute-data.html#what-can-you-do-with-attribute-data",
    "title": "Working with attributes in R (and QGIS)",
    "section": "What can you do with attribute data?",
    "text": "What can you do with attribute data?\nYou can…\n\nsubset or filter data\naggregate or summarize data\ncombine data sets based on shared attributes\ncreate new attributes\n\n\nYou can also do many of these same tasks using spatial operations, e.g. filtering by area instead of attribute."
  },
  {
    "objectID": "slides/03_attribute-data.html#loading-packages",
    "href": "slides/03_attribute-data.html#loading-packages",
    "title": "Working with attributes in R (and QGIS)",
    "section": "Loading packages",
    "text": "Loading packages\nFirst, you should load the {sf}, {dplyr}, and {ggplot2} packages along with data from the {spData} package:\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nlibrary(spData)\ndata(\"us_states\")"
  },
  {
    "objectID": "slides/03_attribute-data.html#comparing-approaches-to-data-frames",
    "href": "slides/03_attribute-data.html#comparing-approaches-to-data-frames",
    "title": "Working with attributes in R (and QGIS)",
    "section": "Comparing approaches to data frames",
    "text": "Comparing approaches to data frames\n\n\n\n{base}\n{dplyr}\n\n\n\n\n$\npull()\n\n\n[; subset()\nfilter(); slice(); select()\n\n\nrbind()\nbind_rows()\n\n\ncbind()\nbind_cols()\n\n\naggregate()\nsummarize()\n\n\n\n\nYou can use base R or {dplyr}.\nbase R functions are more stable and (in some cases) may be faster but dplyr and other tidyverse packages offer an intuitive interface that many people prefer."
  },
  {
    "objectID": "slides/03_attribute-data.html#using-base-r-with-attribute-data",
    "href": "slides/03_attribute-data.html#using-base-r-with-attribute-data",
    "title": "Working with attributes in R (and QGIS)",
    "section": "Using base R with attribute data",
    "text": "Using base R with attribute data"
  },
  {
    "objectID": "slides/03_attribute-data.html#using-dplyr-with-attribute-data",
    "href": "slides/03_attribute-data.html#using-dplyr-with-attribute-data",
    "title": "Working with attributes in R (and QGIS)",
    "section": "Using dplyr with attribute data",
    "text": "Using dplyr with attribute data"
  },
  {
    "objectID": "slides/03_attribute-data.html#creating-new-variables",
    "href": "slides/03_attribute-data.html#creating-new-variables",
    "title": "Working with attributes in R (and QGIS)",
    "section": "Creating new variables",
    "text": "Creating new variables"
  },
  {
    "objectID": "slides/03_attribute-data.html#chaining-functions-with-pipes",
    "href": "slides/03_attribute-data.html#chaining-functions-with-pipes",
    "title": "Working with attributes in R (and QGIS)",
    "section": "Chaining functions with pipes",
    "text": "Chaining functions with pipes\nPipes (%>% or |>) help make data transformation scripts easier to read and understand:\n\nus_states %>%\n  filter(REGION == \"Midwest\") %>%\n  mutate(\n    total_pop_change_10_15 = total_pop_change_10_15 / 1000\n  )\n\nSimple feature collection with 12 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -104.0577 ymin: 35.99568 xmax: -80.51869 ymax: 49.38436\nGeodetic CRS:  NAD83\nFirst 10 features:\n   GEOID         NAME  REGION            AREA total_pop_10 total_pop_15\n1     18      Indiana Midwest 36157.85 [mi^2]      6417398      6568645\n2     20       Kansas Midwest 82254.08 [mi^2]      2809329      2892987\n3     27    Minnesota Midwest 84388.99 [mi^2]      5241914      5419171\n4     29     Missouri Midwest 69774.95 [mi^2]      5922314      6045448\n5     38 North Dakota Midwest 70725.36 [mi^2]       659858       721640\n6     46 South Dakota Midwest 77130.41 [mi^2]       799462       843190\n7     17     Illinois Midwest 56368.25 [mi^2]     12745359     12873761\n8     19         Iowa Midwest 56271.95 [mi^2]      3016267      3093526\n9     26     Michigan Midwest 58347.38 [mi^2]      9952687      9900571\n10    31     Nebraska Midwest 77325.58 [mi^2]      1799125      1869365\n   total_pop_change_10_15                       geometry total_pop_10_scaled\n1                 151.247 MULTIPOLYGON (((-87.52404 4...            6417.398\n2                  83.658 MULTIPOLYGON (((-102.0517 4...            2809.329\n3                 177.257 MULTIPOLYGON (((-97.22904 4...            5241.914\n4                 123.134 MULTIPOLYGON (((-95.76565 4...            5922.314\n5                  61.782 MULTIPOLYGON (((-104.0487 4...             659.858\n6                  43.728 MULTIPOLYGON (((-104.0577 4...             799.462\n7                 128.402 MULTIPOLYGON (((-91.41942 4...           12745.359\n8                  77.259 MULTIPOLYGON (((-96.45326 4...            3016.267\n9                 -52.116 MULTIPOLYGON (((-85.63002 4...            9952.687\n10                 70.240 MULTIPOLYGON (((-104.0531 4...            1799.125"
  },
  {
    "objectID": "slides/03_attribute-data.html#tip-packages-to-use-with-pipes",
    "href": "slides/03_attribute-data.html#tip-packages-to-use-with-pipes",
    "title": "Working with attributes in R (and QGIS)",
    "section": "Tip: Packages to use with pipes",
    "text": "Tip: Packages to use with pipes\n\n{tidylog}\n{ViewPipeSteps}"
  },
  {
    "objectID": "slides/03_attribute-data.html#intermission-maps-and-charts-with-ggplot2",
    "href": "slides/03_attribute-data.html#intermission-maps-and-charts-with-ggplot2",
    "title": "Working with attributes in R (and QGIS)",
    "section": "Intermission: Maps and charts with {ggplot2}",
    "text": "Intermission: Maps and charts with {ggplot2}\n\n\n\nThe use of pipes is especially helpful with data visualizations where it reduces the need for intermediate placeholder objects or exports.\n\nus_states %>%\n  filter(REGION == \"Midwest\") %>%\n  mutate(\n    total_pop_change_10_15 = total_pop_change_10_15 / 1000\n  ) %>%\n  ggplot() +\n  geom_sf(aes(fill = total_pop_change_10_15)) +\n  scale_fill_distiller(type = \"div\") +\n  labs(fill = \"2010 to 2015 pop change\\n(in thousands)\")"
  },
  {
    "objectID": "slides/03_attribute-data.html#intermission-maps-and-charts-with-ggplot2-output",
    "href": "slides/03_attribute-data.html#intermission-maps-and-charts-with-ggplot2-output",
    "title": "Working with attributes in R (and QGIS)",
    "section": "Intermission: Maps and charts with {ggplot2}",
    "text": "Intermission: Maps and charts with {ggplot2}"
  },
  {
    "objectID": "slides/03_attribute-data.html#additional-topics",
    "href": "slides/03_attribute-data.html#additional-topics",
    "title": "Working with attributes in R (and QGIS)",
    "section": "Additional topics",
    "text": "Additional topics\n\nusing SQL queries to filter data\n{stringr} package and using regex\nusing case_when() for recoding variables\nusing pivot_longer() to pull variables from column names\ndate and time variables"
  },
  {
    "objectID": "slides/03_attribute-data.html#recap-on-attribute-data-with-r",
    "href": "slides/03_attribute-data.html#recap-on-attribute-data-with-r",
    "title": "Working with attributes in R (and QGIS)",
    "section": "Recap on attribute data with R",
    "text": "Recap on attribute data with R\n\nbase R and dplyr both support many of the same tasks\nuse pipes to chain together actions"
  },
  {
    "objectID": "slides/04_attribute-data-qgis.html#september-14---qgis-notes",
    "href": "slides/04_attribute-data-qgis.html#september-14---qgis-notes",
    "title": "Working with attributes in R (and QGIS)",
    "section": "September 14 - QGIS notes",
    "text": "September 14 - QGIS notes\nWhere do people encouter expressions?\n\nField calculator: Change the value of an existing or new field based on an expression\n\nInterface includes:\n\nExpression editor\nExpression search/reference\nHelp reference pane\nPreview/validation\n\nFeatures\n\nAutocompletion\nValidation in preview section\nCheck boxes to create\n\na new field (equivalent to dplyr::mutate())\nvirtual field\n\nCustom expression language modelled on SQL; most similar PostGIS\nFunctional language; Expression must be a single command of chained functions\nDifferent types of values, but it is not strict about it, e.g. ‘4’ + 6 (more forgiving than R)\n\nWatch out!\n\nTrap: character literals must be wrapped in single quotes; double-quotes are reserved for field references\nTrap: NULL values return NULL, e.g. 5 + NULL is NULL\n\nTips!\n\n|| is concatenate\nNumber fields are evaluated as expressions\n@ references variables (variable are context dependent, e.g. scale)\nContext-dependent variables have informative prefixes (e.g. map_... ) and only appear in widgets where they are available. Variable scopes include:\n\nglobal\nproject (can override global variables)\nmap\nlayout or layout item\n\n\n\n\nExpressions are first built for Field calculator but also appear as:\n\nExpression dialogue for custom labels\nSelect by expression (very common use)\nLayer feature styling\nTips on using expression builder with Layout\n\nEnclose expressions in [% and %] to evaluate\n\n\nThings to demonstrate\n\nView > New map view\nUse symbol_color variable to match label color to symbol color"
  },
  {
    "objectID": "slides/04_attribute-data-qgis.html#september-21---qgis-notes",
    "href": "slides/04_attribute-data-qgis.html#september-21---qgis-notes",
    "title": "Working with attributes in R (and QGIS)",
    "section": "September 21 - QGIS Notes",
    "text": "September 21 - QGIS Notes\nhttps://www.youtube.com/watch?v=a0cVLzbEOig"
  },
  {
    "objectID": "slides/04_attribute-data-qgis.html#stats-summary-panel",
    "href": "slides/04_attribute-data-qgis.html#stats-summary-panel",
    "title": "Working with attributes in R (and QGIS)",
    "section": "Stats Summary panel",
    "text": "Stats Summary panel\n\nStarts with layer selection\nNice for interactive exploration\nOption to limit stats to selected features\nAllows use of expressions in calculating stats\n\nKind of nice but... don’t use it\n\nOld school GIS technique\nAd hoc data analysis and not reproducible\n\nInstead...\nUse processing toolbox\n\nBasic processing algorthims\n\nBuffer\nCentroid\nBasic stats for fields\netc.\n\nAdvanced/esoteric techniques\n\nFor exploratory analysis\n\nProcessing\n\nBasic stats for fields\nStats by category\n\nNotes\n\nNot quite as interactive\nGenerates an HTML report\nEnd up with a bunch of files (HTML and spatial data) in folder w/ no way to keep track of it\n\n\nBut still... don’t do that either.\nInstead use:\n\nProcessing modeler to create a graphical analysis\n\nAllows someone else (including future you) to see analysis process\nOption to save models inside a project\nModels will appear within processing toolbox whenever you open the project file\n\nData-defined layer styling\n\nSee equivalent to mapview w/ zcol\n\n\n\n# test code\nlibrary(dplyr)"
  },
  {
    "objectID": "slides/09_creating-features.html#key-questions",
    "href": "slides/09_creating-features.html#key-questions",
    "title": "Creating feature data with R and QGIS",
    "section": "Key questions",
    "text": "Key questions\n\nWhy or why not create your own features?\nWhat tools can you use to create features?\nHow can you structure your geometry and attribute data?\nWhat tools can you use to create features?"
  },
  {
    "objectID": "slides/09_creating-features.html#why-create-your-own-features",
    "href": "slides/09_creating-features.html#why-create-your-own-features",
    "title": "Creating feature data with R and QGIS",
    "section": "Why create your own features?",
    "text": "Why create your own features?\n\nData does not exist\nData exists but…\n\nyou can’t get it\nyou aren’t allowed to use it\n\nData exists and you can get it but…\n\nThe format can’t be accessed effectively\nThe geometry is inaccurate\nThe geometry too general"
  },
  {
    "objectID": "slides/09_creating-features.html#why-not-create-your-own-features",
    "href": "slides/09_creating-features.html#why-not-create-your-own-features",
    "title": "Creating feature data with R and QGIS",
    "section": "Why not create your own features?",
    "text": "Why not create your own features?\n\nMay be time-consuming\nMay be difficult to get level of detail or accuracy required\nMay lack standards to ensure interoperability"
  },
  {
    "objectID": "slides/09_creating-features.html#what-tools-can-you-use-to-create-features",
    "href": "slides/09_creating-features.html#what-tools-can-you-use-to-create-features",
    "title": "Creating feature data with R and QGIS",
    "section": "What tools can you use to create features?",
    "text": "What tools can you use to create features?\n\nCollect non-spatial information (e.g. a set of addresses or a survey that asks respondents to provide spatial information) then add spatial data to it\nCollect spatial information using a device/sensor with a built-in GPS or using a smartphone application (e.g. Strava or StreetComplete)\nCollect spatial information by drawing features using a web application or desktop GIS\n\nDifferent situations require different approaches. Privacy, cost, accuracy, ease of use, or training requirements are all relevant factors."
  },
  {
    "objectID": "slides/09_creating-features.html#how-can-you-structure-your-geometry",
    "href": "slides/09_creating-features.html#how-can-you-structure-your-geometry",
    "title": "Creating feature data with R and QGIS",
    "section": "How can you structure your geometry?",
    "text": "How can you structure your geometry?\n\nDo you need points, lines, polygons, or something else?\nDo you need a consistent representation of the spatial characteristics of “real world” conditions or phenomena?\nWhat scale or resolution is appropriate for your data?\nShould your data be “snapped” (or related) to another existing feature?\n\nAgain, your approach depends on what you plan to do with the data in the future. Is your primary goal visualization? Analysis? Data sharing?"
  },
  {
    "objectID": "slides/09_creating-features.html#try-it-out-using-web-application-to-create-features",
    "href": "slides/09_creating-features.html#try-it-out-using-web-application-to-create-features",
    "title": "Creating feature data with R and QGIS",
    "section": "Try it out: Using web application to create features",
    "text": "Try it out: Using web application to create features\nThere are several popular web applications that support the creation of spatial data:\n\nGoogle MyMaps: Supports import of CSV, TSV, KML/KMZ, GPX, XLSX, Google Sheet, and photos with EXIF location data. Few recent updates.\ngeojson.io: Supports GeoJSON import/export along with KML, GPX, CSV, GTFS, TopoJSON, and other formats, raster tile layers. Recently updated by Mapbox.\nFelt: Supports collaborative editing and a wide range of embedded rich media.\n\nHow to decide? Look at pricing, data import and export options, collaboration needs."
  },
  {
    "objectID": "slides/09_creating-features.html#try-it-out-create-a-set-of-features-with-geojson.io",
    "href": "slides/09_creating-features.html#try-it-out-create-a-set-of-features-with-geojson.io",
    "title": "Creating feature data with R and QGIS",
    "section": "Try it out: Create a set of features with geojson.io",
    "text": "Try it out: Create a set of features with geojson.io\n\nOpen geojson.io\nOptional: Load a xyz tile layer using a rectified map from Map Warper\nUsing the basemap (or your tile layer) as a reference zoom and pan to an area of interest\nSelect the point, line, polygon, or rectangle tool from the options on the right side of the map pane\nAdd a few features to the map\nSelect a feature from the map and use the pop-up table to enter an attribute name (left side) and value (right side)\nSwitch to the table view on the right to see the attribute data\nSelect the Edit map tool from the options on the right side of the map pane. Try editing a feature (make sure to save your edits).\nSelect Save > GeoJSON from the menu to export data to a GeoJSON file"
  },
  {
    "objectID": "slides/09_creating-features.html#how-can-you-structure-your-attribute-data",
    "href": "slides/09_creating-features.html#how-can-you-structure-your-attribute-data",
    "title": "Creating feature data with R and QGIS",
    "section": "How can you structure your attribute data?",
    "text": "How can you structure your attribute data?\n\nAre there any existing data standards you can use or adapt?\nIs there an existing unique identifier you can use or adapt? Or do you need to assign an arbitrary unique identifier? Is your unique identifier stable or subject to change?\nIf you are basing your data on an existing source, do you transcribe all attributes in the original source? Or are you only transcribing some of those attributes?\nCan you use a “flat” table structure? Or do you need multiple tables or a nested format?\nCan you validate your attributes?"
  },
  {
    "objectID": "slides/09_creating-features.html#hands-on-options-for-validating-data",
    "href": "slides/09_creating-features.html#hands-on-options-for-validating-data",
    "title": "Creating feature data with R and QGIS",
    "section": "Hands-on: Options for validating data",
    "text": "Hands-on: Options for validating data\n\nGoogle Forms offers built-in validation by limiting inputs, requiring a set length, or requiring input text match a specific pattern\nGoogle Sheets and Excel both provide"
  },
  {
    "objectID": "slides/09_creating-features.html#hands-on-using-google-sheets-and-googlesheets4",
    "href": "slides/09_creating-features.html#hands-on-using-google-sheets-and-googlesheets4",
    "title": "Creating feature data with R and QGIS",
    "section": "Hands-on: Using Google Sheets (and {googlesheets4})",
    "text": "Hands-on: Using Google Sheets (and {googlesheets4})\n\nCreate a Google Sheet (I’ll provide it this time)\nSet up columns for longitude and latitude (or for a partial or full address). Note: Separate street address, city, and state columns can be combined using formulas.\nSet up additional columns as needed. Make sure to avoid merging column ranges and keep all column names in the first row.\nConsider creating a separate “data dictionary” sheet where you can use the TRANSPOSE() formula to switch column names into rows.\nConsider using the “Data validation” menu to add validation to any columns where appropriate."
  },
  {
    "objectID": "slides/09_creating-features.html#hands-on-using-mapedit",
    "href": "slides/09_creating-features.html#hands-on-using-mapedit",
    "title": "Creating feature data with R and QGIS",
    "section": "Hands-on: Using {mapedit}",
    "text": "Hands-on: Using {mapedit}\nThe mapedit package offers multiple editors:\n\n“leaflet.extras” (default)\n“leafpm”"
  },
  {
    "objectID": "slides/10_open-data.html#accessing-open-data-with-r",
    "href": "slides/10_open-data.html#accessing-open-data-with-r",
    "title": "Accessing open data with R",
    "section": "Accessing open data with R",
    "text": "Accessing open data with R\nWe are going to explore the Maryland Open Data Portal and Maryland iMap including:\n\nExploring Maryland’s Open Data Portal and using an export URL to access data\n\nUsing the {httr2} package to access an API\nWorking with the {RSocrata} to access the API\n\nExploring web services using the Maryland iMap portal\n\nUsing the {esri2sf} package to access ArcGIS Feature Services\n\nUsing the {labelled} and {dplyr} packages when working with labels and dictionaries"
  },
  {
    "objectID": "slides/10_open-data.html#interlude-working-with-powerful-numbers",
    "href": "slides/10_open-data.html#interlude-working-with-powerful-numbers",
    "title": "Accessing open data with R",
    "section": "Interlude: Working with “powerful numbers”",
    "text": "Interlude: Working with “powerful numbers”\n\nBouk, Ackermann, and boyd (2022) offer us a primer on thinking about “powerful numbers” and how the work in the world.\n\nData & Society\n\n\n\nModern Societies Are Built to Trust in Official Numbers. They Even Let Official Numbers Make Key Decisions.\nOfficial Numbers Are Made, Not Found.\nWhen Things Are Going Well, We Forget That Official Numbers Had to Be Made.\nInstitutions Make Public Data and They Make Data Public.\nOfficial Numbers Are Political.\nConsensus on Official Numbers Requires Work. (It isn’t certain that the givens will be taken.)"
  },
  {
    "objectID": "slides/10_open-data.html#exploring-maryland-imap",
    "href": "slides/10_open-data.html#exploring-maryland-imap",
    "title": "Accessing open data with R",
    "section": "Exploring Maryland iMap",
    "text": "Exploring Maryland iMap"
  },
  {
    "objectID": "slides/10_open-data.html#working-with-real-property-data-from-the-maryland-department-of-assessments-and-taxation",
    "href": "slides/10_open-data.html#working-with-real-property-data-from-the-maryland-department-of-assessments-and-taxation",
    "title": "Accessing open data with R",
    "section": "Working with real property data from the Maryland Department of Assessments and Taxation",
    "text": "Working with real property data from the Maryland Department of Assessments and Taxation\nResources:\n\nGlossary of Terms for the Real Property Data Search, Maryland Department of Assessments and Taxation\nMaryland Real Property Assessments: Fields Reference"
  },
  {
    "objectID": "slides/10_open-data.html#interludes",
    "href": "slides/10_open-data.html#interludes",
    "title": "Accessing open data with R",
    "section": "Interludes",
    "text": "Interludes\nHere is a recap of the interlude sections:\n\nUsing Tabula and the {tabulizer} R package\nUsing {officer} to access Word documents\n\n\n\n\n\n\n\nBouk, Dan, Kevin Ackermann, and danah boyd. 2022. “A Primer on Powerful Numbers: Selected Readings in the Social Study of Public Data and Official Numbers.” https://datasociety.net/library/a-primer-on-powerful-numbers-selected-readings-in-the-social-study-of-public-data-and-official-numbers/."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "GES660: Building Spatial Datasets",
    "section": "",
    "text": "Date 📅\n\nAugust 31 – December 7, 2022\n\nTime ⏰\n\nWednesdays, 6:00 pm to 8:30 pm\n\nLocation 📍\n\nJanet & Walter Sondheim Hall 001 (Cartography Lab), University of Maryland, Baltimore County, 1099 Hilltop Road, Baltimore, MD 21250"
  },
  {
    "objectID": "syllabus.html#about-this-course",
    "href": "syllabus.html#about-this-course",
    "title": "GES660: Building Spatial Datasets",
    "section": "About this course",
    "text": "About this course\nIn this course, students will learn how to find, understand, and work with spatial data for research and practice. This course leverages open-source tools, online educational resources, and real-world data from urban environments to help students build a methodological framework for academic and professional work with spatial data now and in the future.\nThe course explores the process of building and maintaining data sets about local places and prepares students to navigate critical issues including data ownership, the challenges of administrative data, and privacy consideration. Assignments and readings will introduce students to the range of uses for spatial data in planning, public policy, and advocacy around housing, health, transportation, the environment, and more.\nStudents in the course will learn to work with common file formats (e.g. GeoJSON, GeoPackage) and web services (e.g. FeatureServer, APIs) and how to read, write, document, and share data using QGIS, the R programming language, and various web services.\n\n\n\n\n\n\nThis course does not require any prior experience with desktop GIS software or R programming. Assignments will require students to use R, RStudio, and QGIS during the course. Optional resources on working with spatial data using ArcGIS Online or Python will be provided where feasible. Students will be required to use GitHub in order to share completed assignments and develop their professional portfolios."
  },
  {
    "objectID": "syllabus.html#schedule-overview",
    "href": "syllabus.html#schedule-overview",
    "title": "GES660: Building Spatial Datasets",
    "section": "Schedule overview",
    "text": "Schedule overview\nOverall, this course is divided into three sections:\n\n\n\n\n\n\n\nSection\nTopics\nSessions/Dates\n\n\n\nGetting started with spatial data\nIntroduction to using R, sf, and QGIS to work with vector, raster, and tabular data.\n1 (Aug. 31) – 6 (Oct. 5)\n\n\nBuilding and sharing spatial data\nDownloading and manipulating spatial data from Socrata, ArcGIS, OpenStreetMap, and other sources using R.\n7 (Oct. 12) – 12 (Nov. 16)\n\n\nSpecial topics and final project\nUse local data sources related to housing, transportation, community development, or related topics to work on a collaborative data project\n13 (Nov. 23) – 15 (Dec. 7)\n\n\n\nReview the course schedule for a more outline of topics, readings, and activities."
  },
  {
    "objectID": "syllabus.html#assignments",
    "href": "syllabus.html#assignments",
    "title": "GES660: Building Spatial Datasets",
    "section": "Assignments",
    "text": "Assignments\nThe work for this course includes three different activities:\n\nWriting a weekly log in response to assigned readings\nCompleting lab assignments (primarily using R and QGIS)\nCollaborating on a final project using spatial data\n\nWeekly log\nEach week students are expected to write a 1-2 paragraph reflection the assigned readings and 1-3 questions related to the technical materials. Each weekly log entry should be committed to the “log” folder as a Markdown (.md) file with a file name corresponding to the session date (e.g. “2021-08-31_log.md”). Weekly logs that are unrelated to the readings or do not include a question will be considered incomplete but may be revised and resubmitted. You are allowed to skip or miss up to four weekly log entries during the course so each entry submitted beyond 10 entries is a bonus point.\nAssignments\nFor each assignment, students will be provided instructions or reference materials to complete a set of objectives. Assignments in R will be submitted as an R script or RMarkdown file committed to the course repository. Assignments in QGIS will typically be documented with a brief written report and an exported map in a PDF format also submitted using the course repository.\nAssignments will include a clear list of objectives that must be met to be considered complete. Most assignments may also include a bonus objective that can earn additional points. Each assignment is effectively assigned points on the following scale:\n\n3: Assignment is submitted, complete, and includes a complete bonus assignment\n2: Assignment is submitted and complete\n1: Assignment is submitted but incomplete\n0: Assignment not submitted\n\nIncomplete assignments can be revised and resubmitted for full credit. Assignments submitted more than one day late cannot receive a bonus point. Assignments more than one week late will not be accepted barring exceptional circumstances.\n\n\n\n\n\n\n\n\n\nAssignment\nSession due\nDue date\n\n\n\nComplete GitHub Fundamentals\n3\n9/14\n\n\nFind spatial data\n3\n9/14\n\n\nRead spatial data and make a map with QGIS\n5\n9/28\n\n\nRead spatial data and make a map with R and ggplot2\n5\n9/28\n\n\nCompare spatial and geometry operations in QGIS and R\n6\n10/5\n\n\nDocument a public data set\n6\n10/5\n\n\nTidy tabular address data and geocode with R\n7\n10/12\n\n\nGeoreference a historic map with QGIS\n8\n10/19\n\n\nComplete an exploratory analysis of a public dataset\n9\n10/26\n\n\nMake a map using OpenStreetMap data in QGIS or R\n13\n11/16\n\n\n\n\n\nFinal project\nAll students will participate in a final project that can be completed independently or in collaboration with other students in the class. The final project must use local spatial data and/or include the creation of new spatial data about local places. Students will be encouraged to use R and Quarto for the creation of the final project but projects completed using QGIS are also acceptable.\nA draft project proposal (1-2 pages in length) must be submitted by November 9. We will review final projects as a group during the final class session on December 7 and all project materials must be submitted in a final format by December 14.\nEvaluation of the final project will be based on both self-assessment by the individual student or group and an instructor assessment. The instruction can also offer bonus points in recognition of exceptional work or, if necessary, adjust points awarded through the student self-assessment."
  },
  {
    "objectID": "syllabus.html#evaluation",
    "href": "syllabus.html#evaluation",
    "title": "GES660: Building Spatial Datasets",
    "section": "Evaluation",
    "text": "Evaluation\nThe evaluation of this course is intended to help you focus on completing assignments—not getting them perfect. You can earn up to a total of 50 points in this class through writing at least 10 weekly logs (10 points), all 10 assignments (20 points), and subsmitting a project proposal and a final project (20 points). Completing additional weekly assignments can earn bonus points and most assignments will include 1 or 2 bonus activities.\nFor the final project proposal, the project proposal may receive partial credit if revisions are required but full credit when a revised proposal is submitted. For the final project, points will be split between the self-assessment and instructor evaluation based on a rubric to be provided later in the semester.\n\n\n\n\nActivity\nPoints\nBonus points\n\n\n\nWeekly log\n10\n4\n\n\nAssignments\n20\n10\n\n\nProject proposal\n8\n0\n\n\nFinal project\n12\n4"
  },
  {
    "objectID": "syllabus.html#diversity-statement-on-respect",
    "href": "syllabus.html#diversity-statement-on-respect",
    "title": "GES660: Building Spatial Datasets",
    "section": "Diversity Statement on Respect",
    "text": "Diversity Statement on Respect\nStudents in this class are encouraged to speak up and participate during class meetings. Because the class will represent a diversity of individual beliefs, backgrounds, and experiences, every member of this class must show respect for every other member of this class.\nFrom California State University, Chico’s Office of Diversity and Inclusion."
  },
  {
    "objectID": "syllabus.html#accessibility-and-disability-accommodations-guidance-and-resources",
    "href": "syllabus.html#accessibility-and-disability-accommodations-guidance-and-resources",
    "title": "GES660: Building Spatial Datasets",
    "section": "Accessibility and Disability Accommodations, Guidance and Resources",
    "text": "Accessibility and Disability Accommodations, Guidance and Resources\nAccommodations for students with disabilities are provided for all students with a qualified disability under the Americans with Disabilities Act (ADA & ADAAA) and Section 504 of the Rehabilitation Act who request and are eligible for accommodations. The Office of Student Disability Services (SDS) is the UMBC department designated to coordinate accommodations that creates equal access for students when barriers to participation exist in University courses, programs, or activities.\nIf you have a documented disability and need to request academic accommodations in your courses, please refer to the SDS website at sds.umbc.edu for registration information and office procedures.\n\nSDS email: disAbility@umbc.edu\nSDS phone: 410-455-2459\n\nIf you will be using SDS approved accommodations in this class, please contact the instructor to discuss implementation of the accommodations. During remote instruction requirements due to COVID, communication and flexibility will be essential for success."
  },
  {
    "objectID": "syllabus.html#sexual-assault-sexual-harassment-and-gender-based-violence-and-discrimination",
    "href": "syllabus.html#sexual-assault-sexual-harassment-and-gender-based-violence-and-discrimination",
    "title": "GES660: Building Spatial Datasets",
    "section": "Sexual Assault, Sexual Harassment, and Gender Based Violence and Discrimination",
    "text": "Sexual Assault, Sexual Harassment, and Gender Based Violence and Discrimination\nUMBC Policy and Federal law (Title IX) prohibit discrimination and harassment on the basis of sex, sexual orientation, and gender identity in University programs and activities. Any student who is impacted by sexual harassment, sexual assault, domestic violence, dating violence, stalking, sexual exploitation, gender discrimination, pregnancy discrimination, gender-based harassment or retaliation should contact the University’s Title IX Coordinator to make a report and/or access support and resources:\n\nMorgan Thomas, Acting Director and Title IX Coordinator\n410-455-1354 (direct line), morganthomas@umbc.edu\n\nYou can access support and resources even if you do not want to take any further action. You will not be forced to file a formal complaint or police report. Please be aware that the University may take action on its own if essential to protect the safety of the community.\nIf you are interested in or thinking about making a report, please use the Online Reporting/Referral Form. Please note that, if you report anonymously,  the University’s ability to respond will be limited.\n\n\n\n\n\n\nFaculty are Responsible Employees with Mandatory Reporting Obligations\n\n\n\nAll faculty members are considered Responsible Employees, per UMBC’s Policy on Sexual Misconduct, Sexual Harassment, and Gender Discrimination. Faculty are therefore required to report any/ all available information regarding conduct falling under the Policy and violations of the Policy to the Title IX Coordinator, even if a student discloses an experience that occurred before attending UMBC and/or an incident that only involves people not affiliated with UMBC. Reports are required regardless of the amount of detail provided and even in instances where support has already been offered or received.\nWhile faculty members want encourage you to share information related to your life experiences through discussion and written work, students should understand that faculty are required to report past and present sexual assault, domestic and interpersonal violence, stalking, and gender discrimination that is shared with them to the Title IX Coordinator so that the University can inform students of their rights, resources and support. While you are encouraged to do so, you are not obligated to respond to outreach conducted as a result of a report to the Title IX Coordinator.\nIf you need to speak with someone in confidence, who does not have an obligation to report to the Title IX Coordinator, UMBC has a number of Confidential Resources available to support you:\n\nThe Counseling Center (Main Campus): 410-455-2472 / After-Hours 410-455-3230 [Monday – Friday; 8:30 a.m. – 5 p.m.]\nCenter for Counseling and Consultation (Shady Grove Campus): 301-738-6273 (Messages checked hourly) Online Appointment Request Form\nUniversity Health Services: 410-455-2542 [Monday – Friday 8:30 a.m. – 5 p.m.]\nPastoral Counseling via Interfaith Center: 410-455-3657; interfaith@umbc.edu [7 days a week; Fall and Spring 7 a.m. – 11 p.m.; Summer and Winter 8 a.m. – 8 p.m.]\n\nOther Resources:\n\nWomen’s Center (for students of all genders): 410-455-2714; womenscenter@umbc.edu. [Monday – Thursday 10:00am-5:30pm and Friday 10:00am-4pm]\nShady Grove Student Resources, Maryland Resources, National Resources.\n\n\n\n\n\n\n\n\n\nChild Abuse and Neglect\n\n\n\nPlease note that Maryland law and UMBC policy require that faculty report all disclosures or suspicions of child abuse or neglect to the Department of Social Services and/or the police even if the person who experienced the abuse or neglect is now over 18."
  },
  {
    "objectID": "syllabus.html#plagiarism",
    "href": "syllabus.html#plagiarism",
    "title": "GES660: Building Spatial Datasets",
    "section": "Plagiarism",
    "text": "Plagiarism\nCopying or using another’s work in written or oral form—partial or complete—without giving credit to the other person is a serious academic offense and is taken very seriously in this class, by the Department and by the University of Maryland, Baltimore County. UMBC specifically defines plagiarism as anyone who “knowingly, or by carelessness or negligence, representing as one’s own in any academic exercise the words, ideas, works of art or computer-generated information and images of someone else.”\nAny student who plagiarizes will be referred to the Department Chair and will be subject to the policies of the university. In general, the consequences of plagiarism include failing an assignment, receiving a lower course grade, and even failing a course. Examples of plagiarism include:\n\nSubmit someone else’s work as your own.\nBuy a paper from a paper-mill, website or other source.\nCopy sentences, phrases, paragraphs, or ideas from someone else’s work, published or unpublished, without giving the original author credit.\nReplace select words from a passage without giving the original author credit.\nCopy any type of graphics, tables, graphs, maps, or charts from someone else’s work without giving the original author credit.\nPiece together phrases, ideas, and sentences from a variety of sources to write an essay.\nBuild on someone else’s idea or phrase without giving the original author credit.\n\nDetails about avoiding plagiarism, examples, and disciplinary policies should be reviewed to gain a clear understanding prior to working on an assignment or exam."
  },
  {
    "objectID": "syllabus.html#covid-19-safety-protocols-and-compliance-statement",
    "href": "syllabus.html#covid-19-safety-protocols-and-compliance-statement",
    "title": "GES660: Building Spatial Datasets",
    "section": "COVID-19 Safety Protocols and Compliance Statement",
    "text": "COVID-19 Safety Protocols and Compliance Statement\nUMBC has set clear expectations for masking while on campus that include the requirement that you must wear a KN95 face mask or equivalent that covers your nose and mouth in all classrooms regardless of your vaccination status. Find more information on masks equivalent to KN95s at https://covid19.umbc.edu/masks/\nThis is to protect your health and safety as well as the health and safety of your classmates, instructor, and the university community. Anyone attending class without a KN95 mask or wearing one improperly will be asked by the instructor to put on a KN95 mask or fix their mask in the appropriate position. Any student that refuses to comply with this directive will be asked to leave the classroom immediately and failure to do so may result in the instructor requesting the assistance of the University Police. Students who refuse to wear KN95 masks may be referred to Student Conduct and Community Standards and may face disciplinary action for violations of the Code of Student Conduct, specifically, Rule 2: Behavior Which Jeopardizes the Health or Safety of Self or Others and Rule 16: Failure to Comply with the Request of a University Official. UMBC’s on-campus safety protocols, including masking requirements, are subject to change in response to the evolving situation with COVID-19."
  }
]