{
  "hash": "28a41a8845308e7cf335319e10f515a8",
  "result": {
    "markdown": "---\ntitle: \"Read tabular data and convert to spatial data with R\"\ndescription: \"Learn to read tabular data with R, convert to spatial data with the sf package, and clean data with the dplyr and tidy packages.\"\ntitle-block-categories: true\ncategory: assignment\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n## Read tabular data with coordinates and convert to sf\n\nThe `readr::read_csv()` function from the [readr](https://readr.tidyverse.org/) package is the preferred method for importing a comma separated variable (csv) data file. The package also supports less common tabular data files such as tab separated variable (tsv). You could also use the `read.csv` from the utils package (included with base R).\n\nFor example, you can read a csv file with the names and locations of Baltimore City Public School System programs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile <- here::here(\"files/data\", \"bcps_programs_SY2122.csv\")\n\nbcps_programs_df <- read_csv(file = file)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 162 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): program_name_short, type, category\ndbl (3): program_number, lon, lat\nlgl (1): swing_space\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n\nBy default, `read_csv` prints the number of rows and columns, the selected delimiter and the automatic specification of column types. In general, you should always examine the data after reading it in to make sure the column types are appropriate or if manual specification is needed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbcps_programs_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 162 × 7\n   program_name_short program_number type            categ…¹ swing…²   lon   lat\n   <chr>                       <dbl> <chr>           <chr>   <lgl>   <dbl> <dbl>\n 1 Cecil E                         7 Traditional     E       FALSE   -76.6  39.3\n 2 Steuart Hill E                  4 Traditional     E       FALSE   -76.6  39.3\n 3 Lakeland EM                    12 Traditional     EM      FALSE   -76.6  39.3\n 4 Eutaw-Marshburn E              11 Traditional     E       FALSE   -76.6  39.3\n 5 City Springs EM                 8 Charter-Conver… EM      FALSE   -76.6  39.3\n 6 James McHenry EM               10 Traditional     EM      FALSE   -76.6  39.3\n 7 Tench Tilghman EM              13 Traditional     EM      FALSE   -76.6  39.3\n 8 Stadium School M               15 Traditional     M       FALSE   -76.6  39.3\n 9 Hilton E                       21 Traditional     E       FALSE   -76.7  39.3\n10 Johnston Sq E                  16 Traditional     E       FALSE   -76.6  39.3\n# … with 152 more rows, and abbreviated variable names ¹​category, ²​swing_space\n```\n:::\n:::\n\n\nOne way to address issues with column specification is to read all columns as character types and convert columns to other types as needed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbcps_programs_df <- read_csv(file = file, col_types = \"ccccccc\")\n\nbcps_programs_df <- bcps_programs_df %>%\n  mutate(\n    lat = as.numeric(lat),\n    lon = as.numeric(lon)\n  )\n```\n:::\n\n\n::: {.callout-tip collapse=\"false\" appearance=\"default\" icon=\"true\"}\n## Validating CSV files\n\nIn some cases, you may have a CSV file that does not work the way you expect or a CSV file that you want to validate using an established schema. [CSVLint](https://csvlint.io/) or [Data Curator](https://github.com/qcif/data-curator) (from the Queensland Cyber Infrastructure Foundation) are two tools developed to support this type of validation.\n:::\n\nYou can then use `st_as_sf()` function from the [sf](https://r-spatial.github.io/sf/) package to convert the data frame read into a simple feature object. You must specify the name of the coordinate columns and the expected coordinate reference system. By default, the `st_as_sf` returns a tibble data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbcps_programs_sf <-\n  st_as_sf(\n    x = bcps_programs_df,\n    coords = c(\"lon\", \"lat\"),\n    crs = 4326\n  )\n\nclass(bcps_programs_sf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_sf(data = md_counties[3, ]) +\n  geom_sf(data = bcps_programs_sf)\n```\n\n::: {.cell-output-display}\n![](02_read-tabular-data-r_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThis function can be tricky for novice users. Missing values in coordinate columns results in an error by default (set `na.fail = FALSE` to change this).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbcps_programs_df_error <- bcps_programs_df\nbcps_programs_df_error[1, ]$lon <- NA\n\nst_as_sf(\n  x = bcps_programs_df_error,\n  coords = c(\"lon\", \"lat\"),\n  crs = 4326\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in st_as_sf.data.frame(x = bcps_programs_df_error, coords = c(\"lon\", : missing values in coordinates not allowed\n```\n:::\n:::\n\n\nIf you reverse the order of the latitude and longitude columns, the function returns an sf object---but with the points at incorrect or nonexistent locations.\n\n\n::: {.cell fig.caption='sf created with lon, lat coordinates compared to sf created with reversed lat, lon values'}\n\n```{.r .cell-code}\nbcps_programs_coord_rev <-\n  st_as_sf(\n    x = bcps_programs_df,\n    coords = c(\"lat\", \"lon\"),\n    crs = 4326\n  )\n\nggplot() +\n  geom_sf(data = bcps_programs_sf, color = \"green\") +\n  geom_sf(data = bcps_programs_coord_rev, color = \"red\")\n```\n\n::: {.cell-output-display}\n![](02_read-tabular-data-r_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-note collapse=\"true\" appearance=\"default\" icon=\"true\"}\n## Longitude and latitude or latitude and longitude?\n\nTom MacWright (creator geojson.io and the OpenStreetMap iD editor) [documented the inconsistent order](https://macwright.com/lonlat/) of latitude and longitude across a range of file formats, APIs, and service specifications. If you're wondering, which order is right, MacWright explains: \"Neither. This is an opinion with no right answer. Geographical tradition favors lat, lon. Math and software prefer lon, lat.\"\n:::\n\nWorking with other types of tabular data follows a similar work flow.\n\n-   Date from Excel files (xls or xslx) can be imported using `readxl::read_excel`. Both readr and readxl work with local file paths or with remote files if you provide a url.\n\n-   Data from Google Sheets can be accessed with `googlesheets4::read_sheet`. Google Sheets can be particularly useful when collaborating on a data project with people who can't use a GIS application or programming language.\n\n-   Nested data from a JSON file can be read with `jsonlite::read_json` and converted to a data frame by setting `simplifyVector = TRUE`.\n\n::: {.callout-warning collapse=\"false\" appearance=\"default\" icon=\"true\"}\n## Beware auto-formatting in Excel\n\nThe convenience of automatic formatting in Microsoft Excel can cause with automatic rounding of numeric coordinates and automatic formatting of non-date values as date.\n:::\n\nThe sf package also supports reading Excel and CSV files directly but it returns a tibble (not an sf object) when trying to read data where the geometry is contained in numeric coordinate columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbcps_programs_st_read <-\n  st_read(\n    dsn = file,\n    coords = c(\"lon\", \"lat\"),\n    crs = 4326\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `bcps_programs_SY2122' from data source \n  `/Users/elipousson/Documents/GitHub/bldgspatialdata/files/data/bcps_programs_SY2122.csv' \n  using driver `CSV'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n```\n:::\n:::\n\n\nHowever, `st_read` does work with a CSV file where the geometry has been encoded as [well-known text](https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry) such as the sample \"baltimore_city_wkt.csv\" file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndsn <- here::here(\"files/data\", \"baltimore_city_wkt.csv\")\n\nbaltimore_city <-\n  st_read(\n    dsn = dsn,\n    crs = 4326\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `baltimore_city_wkt' from data source \n  `/Users/elipousson/Documents/GitHub/bldgspatialdata/files/data/baltimore_city_wkt.csv' \n  using driver `CSV'\nSimple feature collection with 1 feature and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -76.71152 ymin: 39.19721 xmax: -76.52946 ymax: 39.37221\nGeodetic CRS:  WGS 84\n```\n:::\n\n```{.r .cell-code}\nggplot() +\n  geom_sf(data = baltimore_city, fill = NA) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](02_read-tabular-data-r_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Read and geocode tabular data with addresses\n\nWhat if you do not have coordinates? Geocoding address data is a common task and there are many ways to do this using R and a variety of other tools. The easiest package in most cases is the [tidygeocoder package](https://jessecambon.github.io/tidygeocoder/). This package supports [over a dozen different geocoding services](https://jessecambon.github.io/tidygeocoder/articles/geocoder_services.html) including [Nominatim](https://nominatim.org) and the [US Census Burearu](https://geocoding.geo.census.gov/) geocoder (which are both free to use without an API key).\n\nUnfortunately, accurately geocoding address data can be challenging. This toppic is addresed in more detail in the example on cleaning data.\n\n## Read tabular data and join geometry based on a named area or location\n\nOne last common way to add geometry to tabular data is to join the table to geometry based on a named area (typically an administrative boundary) or a named location.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile_url <- \"https://opendata.maryland.gov/api/views/ftsr-vapt/rows.csv\"\n\nmd_foreclosure <- file_url %>%\n  read_csv(show_col_types = FALSE) %>%\n  tidyr::pivot_longer(\n    cols = !contains(\"Zip\"),\n    names_to = \"date\",\n    values_to = \"count\"\n  )\n```\n:::\n\n\nFor data organized by zipcode can be matched to geometry for Zip Code Tabulation Areas (or ZCTAs) downloaded from the U.S. Census Bureau API with the `tigris::zcta` function and saved to a GeoPackage file \"md_zctas.gpkg\". The \"Zip\" column should be equivalent to the \"ZCTA5CE20\" column from md_zctas.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmd_zctas <- read_sf(here::here(\"files/data/md_zctas.gpkg\"))\n\nmd_foreclosure <-\n  left_join(\n    md_foreclosure,\n    md_zctas,\n    by = c(\"Zip\" = \"ZCTA5CE20\")\n  )\n\nmd_foreclosure_sf <- st_as_sf(md_foreclosure)\n\nmd_foreclosure_202207 <- filter(md_foreclosure_sf, date == \"July 2022\")\n\nggplot() +\n  geom_sf(data = md_foreclosure_202207, aes(fill = count), color = NA) +\n  geom_sf(data = md_counties, fill = NA, color = \"white\", size = 0.2) +\n  scale_fill_distiller(palette = \"PuRd\") +\n  theme_void() +\n  labs(\n    title = \"Notice of intent to foreclosure by zipcode in Maryland, July 2022\",\n    fill = \"# of notices\",\n    caption = \"Data courtesy Open Maryland\"\n  )\n```\n\n::: {.cell-output-display}\n![](02_read-tabular-data-r_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "02_read-tabular-data-r_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}